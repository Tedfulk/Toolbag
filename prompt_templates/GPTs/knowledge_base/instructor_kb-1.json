[
  {
    "title": "Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/?q=",
    "html": "Skip to content Instructor Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents The Problem with Existing LLM Frameworks The OpenAI Function Calling Game-Changer Why Pydantic? Simplifying Validation Flow with Pydantic The Modular Approach Composition of Schemas Defining Relationships Using Enums Flexible Schemas Chain of Thought Language Models as Microservices FastAPI Stub Using Instructor as a Function Response Modeling Conclusion Back to index Jason Liu Creator Metadata 2023/09/11 2 min read Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls¶ Language models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic. The Problem with Existing LLM Frameworks¶ Current frameworks for Language Learning Models (LLMs) have complex setups. Developers find it hard to control interactions with language models. Some frameworks require complex JSON Schema setups. The OpenAI Function Calling Game-Changer¶ OpenAI's Function Calling feature provides a constrained interaction model. However, it has its own complexities, mostly around JSON Schema. Why Pydantic?¶ Instructor uses Pydantic to simplify the interaction between the programmer and the language model. Widespread Adoption: Pydantic is a popular tool among Python developers. Simplicity: Pydantic allows model definition in Python. Framework Compatibility: Many Python frameworks already use Pydantic. import pydantic import instructor from openai import OpenAI # Enables the response\\\\\\_model client = instructor.patch(OpenAI()) class UserDetail(pydantic.BaseModel): name: str age: int def introduce(self): return f\"Hello I'm {self.name} and I'm {self.age} years old\" user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] ) Simplifying Validation Flow with Pydantic¶ Pydantic validators simplify features like re-asking or self-critique. This makes these tasks less complex compared to other frameworks. from typing\\\\\\_extensions import Annotated from pydantic import BaseModel, BeforeValidator from instructor import llm\\\\\\_validator, patch from openai import OpenAI class QuestionAnswerNoEvil(BaseModel): question: str answer: Annotated\\\\\\[ str, BeforeValidator( llm\\\\\\_validator(\"don't say objectionable things\") ), \\\\\\] The Modular Approach¶ Pydantic allows for modular output schemas. This leads to more organized code. Composition of Schemas¶ class UserDetails(BaseModel): name: str age: int class UserWithAddress(UserDetails): address: str Defining Relationships¶ class UserDetail(BaseModel): id: int age: int name: str friends: List\\\\\\[int\\\\\\] class UserRelationships(BaseModel): users: List\\\\\\[UserDetail\\\\\\] Using Enums¶ from enum import Enum, auto class Role(Enum): PRINCIPAL = auto() TEACHER = auto() STUDENT = auto() OTHER = auto() class UserDetail(BaseModel): age: int name: str role: Role Flexible Schemas¶ from typing import List class Property(BaseModel): key: str value: str class UserDetail(BaseModel): age: int name: str properties: List\\\\\\[Property\\\\\\] Chain of Thought¶ class TimeRange(BaseModel): chain\\\\\\_of\\\\\\_thought: str start\\\\\\_time: int end\\\\\\_time: int class UserDetail(BaseModel): id: int age: int name: str work\\\\\\_time: TimeRange leisure\\\\\\_time: TimeRange Language Models as Microservices¶ The architecture resembles FastAPI. Most code can be written as Python functions that use Pydantic objects. This eliminates the need for prompt chains. FastAPI Stub¶ app = FastAPI() @app.get(\"/user/{user\\\\\\_id}\", response\\\\\\_model=UserDetails) async def get\\\\\\_user(user\\\\\\_id: int) -> UserDetails: return UserDetails(...) Using Instructor as a Function¶ def extract\\\\\\_user(str) -> UserDetails: return client.chat.completions( response\\\\\\_model=UserDetails, messages=\\\\\\[...\\\\\\] ) Response Modeling¶ class MaybeUser(BaseModel): result: Optional\\\\\\[UserDetail\\\\\\] error: bool message: Optional\\\\\\[str\\\\\\] Conclusion¶ Instructor, with Pydantic, simplifies interaction with language models. It is usable for both experienced and new developers. If you enjoy the content or want to try out instructor please check out the github and give us a star! Was this page helpful? Back to top Next RAG is more than just embedding search Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Generators and LLM Streaming - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/11/26/python-generators-and-llm-streaming/?q=",
    "html": "Skip to content Instructor Generators and LLM Streaming Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents Python Generators: An Efficient Approach to Iterables The Basics: Yielding Values Advantages Over Traditional Collections Generator Expressions: A Shortcut Use Cases in Real-World Applications LLM Streaming E-commerce Product Ranking Scenario Stream Processing FastAPI Key Takeaways Back to index Jason Liu Creator Anmol Jawandha Contributor Metadata 2023/11/26 6 min read Generators and LLM Streaming¶ Latency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times. And what makes streaming possible? Generators! In this post, we're going to dive into the cool world of Python generators — these tools are more than just a coding syntax trick. We'll explore Python generators from the ground up and then delve into LLM streaming using the Instructor library. Python Generators: An Efficient Approach to Iterables¶ Generators in Python are a game-changer for handling large data sets and stream processing. They allow functions to yield values one at a time, pausing and resuming their state, which is a faster and more memory-efficient approach compared to traditional collections that store all elements in memory. The Basics: Yielding Values¶ A generator function in Python uses the yield keyword. It yields values one at a time, allowing the function to pause and resume its state. def count\\\\\\_to\\\\\\_3(): yield 1 yield 2 yield 3 for num in count\\\\\\_to\\\\\\_3(): print(num) 1 2 3 Advantages Over Traditional Collections¶ Lazy Evaluation & reduced latency: The time to get the first element (or time-to-first-token in LLM land) from a generator is significantly lower. Generators only produce one value at a time, whereas accessing the first element of a collection will require that the whole collection be created first. Memory Efficiency: Only one item is in memory at a time. Maintain State: Automatically maintains state between executions. Let's see how much faster generators are and where they really shine: import time def expensive\\\\\\_func(x): \"\"\"Simulate an expensive operation.\"\"\" time.sleep(1) return x \\\\\\*\\\\\\* 2 def calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_list(func\\\\\\_input, func): \"\"\"Calculate using a list comprehension and return the first result with its computation time.\"\"\" start\\\\\\_perf = time.perf\\\\\\_counter() result = \\\\\\[func(x) for x in func\\\\\\_input\\\\\\]\\\\\\[0\\\\\\] end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (list): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") return result def calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_generator(func\\\\\\_input, func): \"\"\"Calculate using a generator and return the first result with its computation time.\"\"\" start\\\\\\_perf = time.perf\\\\\\_counter() result = next(func(x) for x in func\\\\\\_input) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (generator): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") return result # Prepare inputs for the function numbers = \\\\\\[1, 2, 3, 4, 5\\\\\\] # Benchmarking first\\\\\\_result\\\\\\_list = calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_list(numbers, expensive\\\\\\_func) first\\\\\\_result\\\\\\_gen = calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_generator(numbers, expensive\\\\\\_func) Time for first result (list): 5.02 seconds Time for first result (generator): 1.01 seconds The generator computes one expensive operation and returns the first result immediately, while the list comprehension computes the expensive operation for all elements in the list before returning the first result. Generator Expressions: A Shortcut¶ Python also allows creating generators in a single line of code, known as generator expressions. They are syntactically similar to list comprehensions but use parentheses. squares = (x\\\\\\*x for x in range(10)) Use Cases in Real-World Applications¶ Generators shine in scenarios like reading large files, data streaming (eg. llm token streaming), and pipeline creation for data processing. LLM Streaming¶ If you've used ChatGPT, you'll see that the tokens are streamed out one by one, instead of the full response being shown at the end (can you imagine waiting for the full response??). This is made possible by generators. Here's how a vanilla openai generator looks: from openai import OpenAI # Set your OpenAI API key client = OpenAI( api\\\\\\_key=\"My API Key\", ) response\\\\\\_generator = client.chat.completions.create( model='gpt-3.5-turbo', messages=\\\\\\[ {'role': 'user', 'content': \"What are some good reasons to smile?\"} \\\\\\], temperature=0, stream=True ) for chunk in response\\\\\\_generator: print(chunk.choices\\\\\\[0\\\\\\].delta.content, end=\"\") This is great, but what if we want to do some structured extraction on this stream? For instance, we might want to render frontend components based on product rankings that are streamed out by an LLM. Should we wait for the entire stream to finish before extracting & validating the list of components or can we extract & validate the components in real time as they are streamed? In e-commerce, every millisecond matters so the time-to-first-render can differentiate a successful and not-so-successful e commerce store (and i know how a failing e commerce store feels :/ ). Let's see how we can use Instructor to handle extraction from this real time stream! E-commerce Product Ranking¶ Scenario¶ Imagine an e-commerce platform where we have: • a customer profile: this includes a detailed history of purchases, browsing behavior, product ratings, preferences in various categories, search history, and even responses to previous recommendations. This extensive data is crucial for generating highly personalized and relevant product suggestions. • a list of candidate products: these could be some shortlisted products we think the customer would like. Our goal is to re-rerank these candidate products for the best conversion and we'll use an LLM! Stream Processing¶ User Data: Let's assume we have the following user profile: profile\\\\\\_data = \"\"\" Customer ID: 12345 Recent Purchases: \\\\\\[Laptop, Wireless Headphones, Smart Watch\\\\\\] Frequently Browsed Categories: \\\\\\[Electronics, Books, Fitness Equipment\\\\\\] Product Ratings: {Laptop: 5 stars, Wireless Headphones: 4 stars} Recent Search History: \\\\\\[best budget laptops 2023, latest sci-fi books, yoga mats\\\\\\] Preferred Brands: \\\\\\[Apple, AllBirds, Bench\\\\\\] Responses to Previous Recommendations: {Philips: Not Interested, Adidas: Not Interested} Loyalty Program Status: Gold Member Average Monthly Spend: $500 Preferred Shopping Times: Weekend Evenings ... \"\"\" We want to rank the following products for this user: products = \\\\\\[ {\"product\\\\\\_id\": 1, \"product\\\\\\_name\": \"Apple MacBook Air (2023) - Latest model, high performance, portable\"}, {\"product\\\\\\_id\": 2, \"product\\\\\\_name\": \"Sony WH-1000XM4 Wireless Headphones - Noise-canceling, long battery life\"}, {\"product\\\\\\_id\": 3, \"product\\\\\\_name\": \"Apple Watch Series 7 - Advanced fitness tracking, seamless integration with Apple ecosystem\"}, {\"product\\\\\\_id\": 4, \"product\\\\\\_name\": \"Kindle Oasis - Premium e-reader with adjustable warm light\"}, {\"product\\\\\\_id\": 5, \"product\\\\\\_name\": \"AllBirds Wool Runners - Comfortable, eco-friendly sneakers\"}, {\"product\\\\\\_id\": 6, \"product\\\\\\_name\": \"Manduka PRO Yoga Mat - High-quality, durable, eco-friendly\"}, {\"product\\\\\\_id\": 7, \"product\\\\\\_name\": \"Bench Hooded Jacket - Stylish, durable, suitable for outdoor activities\"}, {\"product\\\\\\_id\": 8, \"product\\\\\\_name\": \"GoPro HERO9 Black - 5K video, waterproof, for action photography\"}, {\"product\\\\\\_id\": 9, \"product\\\\\\_name\": \"Nespresso Vertuo Next Coffee Machine - Quality coffee, easy to use, compact design\"}, {\"product\\\\\\_id\": 10, \"product\\\\\\_name\": \"Project Hail Mary by Andy Weir - Latest sci-fi book from a renowned author\"} \\\\\\] Let's now define our models for structured extraction. Note: instructor will conveniently let us use Iterable to model an iterable of our class. In this case, once we define our product recommendation model, we can slap on Iterable to define what we ultimately want - a (ranked) list of product recommendations. import instructor from openai import OpenAI from typing import Iterable from pydantic import BaseModel client = instructor.patch(OpenAI(), mode=instructor.function\\\\\\_calls.Mode.JSON) class ProductRecommendation(BaseModel): product\\\\\\_id: str product\\\\\\_name: str Recommendations = Iterable\\\\\\[ProductRecommendation\\\\\\] Now let's use our instructor patch. Since we don't want to wait for all the tokens to finish, will set stream to True and process each product recommendation as it comes in: prompt = f\"Based on the following user profile:\\\\\\\\n{profile\\\\\\_data}\\\\\\\\nRank the following products from most relevant to least relevant:\\\\\\\\n\" + '\\\\\\\\n'.join(f\"{product\\\\\\['product\\\\\\_id'\\\\\\]} {product\\\\\\['product\\\\\\_name'\\\\\\]}\" for product in products) start\\\\\\_perf = time.perf\\\\\\_counter() recommendations\\\\\\_stream = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Iterable\\\\\\[ProductRecommendation\\\\\\], stream=True, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"}, {\"role\": \"user\", \"content\": prompt} \\\\\\] ) for product in recommendations\\\\\\_stream: print(product) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (generator): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") break product\\\\\\_id='1' product\\\\\\_name='Apple MacBook Air (2023)' Time for first result (generator): 4.33 seconds recommendations\\\\\\_stream is a generator! It yields the extracted products as it's processing the stream in real-time. Now let's get the same response without streaming and see how they compare. start\\\\\\_perf = time.perf\\\\\\_counter() recommendations\\\\\\_list = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Iterable\\\\\\[ProductRecommendation\\\\\\], stream=False, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"}, {\"role\": \"user\", \"content\": prompt} \\\\\\] ) print(recommendations\\\\\\_list\\\\\\[0\\\\\\]) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (list): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") product\\\\\\_id='1' product\\\\\\_name='Apple MacBook Air (2023)' Time for first result (list): 8.63 seconds Our web application now displays results faster. Even a 100ms improvement can lead to a 1% increase in revenue. FastAPI¶ We can also take this and set up a streaming LLM API endpoint using FastAPI. Check out our docs on using FastAPI here! Key Takeaways¶ To summarize, we looked at: • Generators in Python: A powerful feature that allows for efficient data handling with reduced latency • LLM Streaming: LLMs provide us generators to stream tokens and Instructor can let us validate and extract data from this stream. Real-time data validation ftw! Don't forget to check our GitHub for more resources and give us a star if you find the library helpful! If you have any questions or need further clarifications, feel free to reach out or dive into the Instructor library's documentation for more detailed information. Happy coding! Was this page helpful? Back to top Previous Verifying LLM Citations with Pydantic Next Introduction to Caching in Python Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "2023 - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/archive/2023/page/2/?q=",
    "html": "Skip to content Instructor 2023 Type to start searching instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Blog Archive 2023 Table of contents Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls 2023¶ 2023/09/11 2 min read Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls Language models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic. The Problem with Existing LLM Frameworks Current frameworks for Language Learning Models (LLMs) have complex setups. Developers find it hard to control interactions with language models. Some frameworks require complex JSON Schema setups. The OpenAI Function Calling Game-Changer OpenAI's Function Calling feature provides a constrained interaction model. However, it has its own complexities, mostly around JSON Schema. Why Pydantic? Instructor uses Pydantic to simplify the interaction between the programmer and the language model. Widespread Adoption: Pydantic is a popular tool among Python developers. Simplicity: Pydantic allows model definition in Python. Framework Compatibility: Many Python frameworks already use Pydantic. import pydantic import instructor from openai import OpenAI # Enables the response\\\\\\_model client = instructor.patch(OpenAI()) class UserDetail(pydantic.BaseModel): name: str age: int def introduce(self): return f\"Hello I'm {self.name} and I'm {self.age} years old\" user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] ) Simplifying Validation Flow with Pydantic Pydantic validators simplify features like re-asking or self-critique. This makes these tasks less complex compared to other frameworks. from typing\\\\\\_extensions import Annotated from pydantic import BaseModel, BeforeValidator from instructor import llm\\\\\\_validator, patch from openai import OpenAI class QuestionAnswerNoEvil(BaseModel): question: str answer: Annotated\\\\\\[ str, BeforeValidator( llm\\\\\\_validator(\"don't say objectionable things\") ), \\\\\\] The Modular Approach Pydantic allows for modular output schemas. This leads to more organized code. Composition of Schemas class UserDetails(BaseModel): name: str age: int class UserWithAddress(UserDetails): address: str Defining Relationships class UserDetail(BaseModel): id: int age: int name: str friends: List\\\\\\[int\\\\\\] class UserRelationships(BaseModel): users: List\\\\\\[UserDetail\\\\\\] Using Enums from enum import Enum, auto class Role(Enum): PRINCIPAL = auto() TEACHER = auto() STUDENT = auto() OTHER = auto() class UserDetail(BaseModel): age: int name: str role: Role Flexible Schemas from typing import List class Property(BaseModel): key: str value: str class UserDetail(BaseModel): age: int name: str properties: List\\\\\\[Property\\\\\\] Chain of Thought class TimeRange(BaseModel): chain\\\\\\_of\\\\\\_thought: str start\\\\\\_time: int end\\\\\\_time: int class UserDetail(BaseModel): id: int age: int name: str work\\\\\\_time: TimeRange leisure\\\\\\_time: TimeRange Language Models as Microservices The architecture resembles FastAPI. Most code can be written as Python functions that use Pydantic objects. This eliminates the need for prompt chains. FastAPI Stub app = FastAPI() @app.get(\"/user/{user\\\\\\_id}\", response\\\\\\_model=UserDetails) async def get\\\\\\_user(user\\\\\\_id: int) -> UserDetails: return UserDetails(...) Using Instructor as a Function def extract\\\\\\_user(str) -> UserDetails: return client.chat.completions( response\\\\\\_model=UserDetails, messages=\\\\\\[...\\\\\\] ) Response Modeling class MaybeUser(BaseModel): result: Optional\\\\\\[UserDetail\\\\\\] error: bool message: Optional\\\\\\[str\\\\\\] Conclusion Instructor, with Pydantic, simplifies interaction with language models. It is usable for both experienced and new developers. If you enjoy the content or want to try out instructor please check out the github and give us a star! Continue reading 1 2 Back to top Previous Welcome to the Instructor Blog Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Welcome to the Instructor Blog - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/page/2/?q=",
    "html": "Skip to content Instructor Welcome to the Instructor Blog Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Blog Archive 2023 Table of contents Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls Welcome to the Instructor Blog¶ 2023/09/11 2 min read Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls Language models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic. The Problem with Existing LLM Frameworks Current frameworks for Language Learning Models (LLMs) have complex setups. Developers find it hard to control interactions with language models. Some frameworks require complex JSON Schema setups. The OpenAI Function Calling Game-Changer OpenAI's Function Calling feature provides a constrained interaction model. However, it has its own complexities, mostly around JSON Schema. Why Pydantic? Instructor uses Pydantic to simplify the interaction between the programmer and the language model. Widespread Adoption: Pydantic is a popular tool among Python developers. Simplicity: Pydantic allows model definition in Python. Framework Compatibility: Many Python frameworks already use Pydantic. import pydantic import instructor from openai import OpenAI # Enables the response\\\\\\_model client = instructor.patch(OpenAI()) class UserDetail(pydantic.BaseModel): name: str age: int def introduce(self): return f\"Hello I'm {self.name} and I'm {self.age} years old\" user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] ) Simplifying Validation Flow with Pydantic Pydantic validators simplify features like re-asking or self-critique. This makes these tasks less complex compared to other frameworks. from typing\\\\\\_extensions import Annotated from pydantic import BaseModel, BeforeValidator from instructor import llm\\\\\\_validator, patch from openai import OpenAI class QuestionAnswerNoEvil(BaseModel): question: str answer: Annotated\\\\\\[ str, BeforeValidator( llm\\\\\\_validator(\"don't say objectionable things\") ), \\\\\\] The Modular Approach Pydantic allows for modular output schemas. This leads to more organized code. Composition of Schemas class UserDetails(BaseModel): name: str age: int class UserWithAddress(UserDetails): address: str Defining Relationships class UserDetail(BaseModel): id: int age: int name: str friends: List\\\\\\[int\\\\\\] class UserRelationships(BaseModel): users: List\\\\\\[UserDetail\\\\\\] Using Enums from enum import Enum, auto class Role(Enum): PRINCIPAL = auto() TEACHER = auto() STUDENT = auto() OTHER = auto() class UserDetail(BaseModel): age: int name: str role: Role Flexible Schemas from typing import List class Property(BaseModel): key: str value: str class UserDetail(BaseModel): age: int name: str properties: List\\\\\\[Property\\\\\\] Chain of Thought class TimeRange(BaseModel): chain\\\\\\_of\\\\\\_thought: str start\\\\\\_time: int end\\\\\\_time: int class UserDetail(BaseModel): id: int age: int name: str work\\\\\\_time: TimeRange leisure\\\\\\_time: TimeRange Language Models as Microservices The architecture resembles FastAPI. Most code can be written as Python functions that use Pydantic objects. This eliminates the need for prompt chains. FastAPI Stub app = FastAPI() @app.get(\"/user/{user\\\\\\_id}\", response\\\\\\_model=UserDetails) async def get\\\\\\_user(user\\\\\\_id: int) -> UserDetails: return UserDetails(...) Using Instructor as a Function def extract\\\\\\_user(str) -> UserDetails: return client.chat.completions( response\\\\\\_model=UserDetails, messages=\\\\\\[...\\\\\\] ) Response Modeling class MaybeUser(BaseModel): result: Optional\\\\\\[UserDetail\\\\\\] error: bool message: Optional\\\\\\[str\\\\\\] Conclusion Instructor, with Pydantic, simplifies interaction with language models. It is usable for both experienced and new developers. If you enjoy the content or want to try out instructor please check out the github and give us a star! Continue reading 1 2 Back to top Previous Core Library Next 2023 Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "AI Engineer Keynote: Pydantic is all you need - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/11/02/ai-engineer-keynote-pydantic-is-all-you-need/?q=",
    "html": "Skip to content Instructor AI Engineer Keynote: Pydantic is all you need Type to start searching instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Back to index Jason Liu Creator Metadata 2023/11/02 1 min read AI Engineer Keynote: Pydantic is all you need¶ Click here to watch the full talk Last month, I ventured back onto the speaking circuit at the inaugural AI Engineer Summit, sharing insights on leveraging Pydantic for effective prompt engineering. I dove deep into what is covered in our documentation and standard blog posts, I'd genuinely appreciate any feedback on the talk – every bit helps in refining the art. So, take a moment to check out the full talk here, and let's continue pushing the boundaries of what's possible. Was this page helpful? Back to top Previous Good LLM Validation is Just Good Validation Next Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Introduction to Caching in Python - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/11/26/python-caching/?q=",
    "html": "Skip to content Instructor Introduction to Caching in Python Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents 1. functools.cache for Simple In-Memory Caching 2. diskcache for Persistent, Large Data Caching 2. Redis Caching Decorator for Distributed Systems Conclusion Back to index Jason Liu Creator Metadata 2023/11/26 7 min read Introduction to Caching in Python¶ Instructor makes working with language models easy, but they are still computationally expensive. Today, we're diving into optimizing instructor code while maintaining the excellent DX offered by Pydantic models. We'll tackle the challenges of caching Pydantic models, typically incompatible with pickle, and explore solutions that use decorators like functools.cache. Then, we'll craft custom decorators with diskcache and redis to support persistent caching and distributed systems. Lets first consider our canonical example, using the OpenAI Python client to extract user details. import instructor from openai import OpenAI from pydantic import BaseModel # Enables \\\\\\`response\\\\\\_model\\\\\\` client = instructor.patch(OpenAI()) class UserDetail(BaseModel): name: str age: int def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Now imagine batch processing data, running tests or experiments, or simply calling extract multiple times over a workflow. We'll quickly run into performance issues, as the function may be called repeatedly, and the same data will be processed over and over again, costing us time and money. 1. functools.cache for Simple In-Memory Caching¶ When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session or in an application where we don't need to persist the cache between sessions. import functools @functools.cache def extract(data): return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Changing the Model does not Invalidate the Cache Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result. Now we can call extract multiple times with the same argument, and the result will be cached in memory for faster access. import time start = time.perf\\\\\\_counter() # Using time.perf\\\\\\_counter() to measure the time taken to run the function is better than using time.time() because it's more accurate and less susceptible to system clock changes. model = extract(\"Extract jason is 25 years old\") print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") start = time.perf\\\\\\_counter() model = extract(\"Extract jason is 25 years old\") # The second time we call extract, the result is returned from the cache, and the function is not called. print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") >>> Time taken: 0.9267581660533324 >>> Time taken: 1.2080417945981026e-06 # The second call to extract is much faster because the result is returned from the cache! Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries. What is a decorator? 2. diskcache for Persistent, Large Data Caching¶ Copy Caching Code When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data! import functools import inspect import instructor import diskcache from openai import OpenAI from pydantic import BaseModel client = instructor.patch(OpenAI()) cache = diskcache.Cache('./my\\\\\\_cache\\\\\\_directory') def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation # We use inspect.signature to get the function's return type annotation, which we use to validate the cached result. if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type We use Pydantic's model\\\\\\_validate\\\\\\_json to deserialize the cached result into a Pydantic model. return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence. 2. Redis Caching Decorator for Distributed Systems¶ Copy Caching Code When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures. import redis import functools import inspect import json import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) cache = redis.Redis(\"localhost\") def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: # Assuming client.chat.completions.create returns a UserDetail instance return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types. Looking carefully If you look carefully at the code above you'll notice that we're using the same instructor\\\\\\_cache decorator as before. The implementatino is the same, but we're using a different caching backend! Conclusion¶ Choosing the right caching strategy depends on your application's specific needs, such as the size and type of data, the need for persistence, and the system's architecture. Whether it's optimizing a function's performance in a small application or managing large datasets in a distributed environment, Python offers robust solutions to improve efficiency and reduce computational overhead. If you'd like to use this code, try to send it over to ChatGPT to understand it more, and to add additional features that might matter for you, for example, the cache isn't invalidated when your BaseModel changes, so you might want to encode the Model.model\\\\\\_json\\\\\\_schema() as part of the key. If you like the content check out our GitHub as give us a star and checkout the library. Was this page helpful? Back to top Previous Generators and LLM Streaming Next Structured Outputs with Anyscale Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Introduction to Batch Processing using asyncio and Instructor - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/11/13/learn-async/?q=",
    "html": "Skip to content Instructor Introduction to Batch Processing using asyncio and Instructor Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents Understanding asyncio Understanding async and await Understanding gather vs as\\\\\\_completed Example: Batch Processing for loop: Running tasks sequentially. asyncio.gather: Running tasks concurrently. asyncio.as\\\\\\_completed: Handling tasks as they complete. Rate-Limited Gather: Using semaphores to limit concurrency. Rate-Limited As Completed: Using semaphores to limit concurrency. Results Practical implications of batch processing Back to index Jason Liu Creator Metadata 2023/11/13 6 min read Introduction to Batch Processing using asyncio and Instructor¶ Today, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using instructor and learn how to use asyncio.gather and asyncio.as\\\\\\_completed for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using asyncio.Semaphore. Github Example If you want to run the code examples in this article, you can find them on jxnl/instructor We will start by defining an async function that calls openai to extract data, and then examine four different ways to execute it. We will discuss the pros and cons of each approach and analyze the results of running them on a small batch. Understanding asyncio¶ asyncio is a Python library that enables writing concurrent code using the async/await syntax. It is particularly useful for IO-bound and structured network code. If you are familiar with OpenAI's SDK, you might have encountered two classes: OpenAI() and AsyncOpenAI(). Today, we will be using the AsyncOpenAI() class, which processes data asynchronously. By utilizing these tools in web applications or batch processing, we can significantly improve performance by handling multiple requests concurrently instead of sequentially. Understanding async and await¶ We will be using the async and await keywords to define asynchronous functions. The async keyword is used to define a function that returns a coroutine object. The await keyword is used to wait for the result of a coroutine object. If you want to understand the deeper details of asyncio, I recommend reading this article by Real Python. Understanding gather vs as\\\\\\_completed¶ In this post we'll show two ways to run tasks concurrently: asyncio.gather and asyncio.as\\\\\\_completed. The gather method is used to run multiple tasks concurrently and return the results as a list. The as\\\\\\_completed returns a iterable is used to run multiple tasks concurrently and return the results as they complete. Another great resource on the differences between the two can be found here. Example: Batch Processing¶ In this example, we will demonstrate how to use asyncio for batch processing tasks, specifically for extracting and processing data concurrently. The script will extract data from a list of texts and process it concurrently using asyncio. import instructor from pydantic import BaseModel from openai import AsyncOpenAI # Enables \\\\\\`response\\\\\\_model\\\\\\` in \\\\\\`create\\\\\\` method client = instructor.apatch(AsyncOpenAI()) We use instructor.apatch to patch the create method of AsyncOpenAI to accept a response\\\\\\_model argument. This is because the create method of AsyncOpenAI does not accept a response\\\\\\_model argument without this patch. class Person(BaseModel): name: str age: int async def extract\\\\\\_person(text: str) -> Person: return await client.chat.completions.create( We use await here to wait for the response from the server before we return the result. This is because create returns a coroutine object, not the result of the coroutine. model=\"gpt-3.5-turbo\", messages=\\\\\\[ {\"role\": \"user\", \"content\": text}, \\\\\\], response\\\\\\_model=Person, ) Notice that now there are async and await keywords in the function definition. This is because we're using the asyncio library to run the function concurrently. Now lets define a batch of texts to process. dataset = \\\\\\[ \"My name is John and I am 20 years old\", \"My name is Mary and I am 21 years old\", \"My name is Bob and I am 22 years old\", \"My name is Alice and I am 23 years old\", \"My name is Jane and I am 24 years old\", \"My name is Joe and I am 25 years old\", \"My name is Jill and I am 26 years old\", \\\\\\] for loop: Running tasks sequentially.¶ persons = \\\\\\[\\\\\\] for text in dataset: person = await extract\\\\\\_person(text) persons.append(person) Even though there is an await keyword, we still have to wait for each task to finish before starting the next one. This is because we're using a for loop to iterate over the dataset. This method, which uses a for loop, will be the slowest among the four methods discussed today. asyncio.gather: Running tasks concurrently.¶ async def gather(): tasks\\\\\\_get\\\\\\_persons = \\\\\\[extract\\\\\\_person(text) for text in dataset\\\\\\] all\\\\\\_persons = await asyncio.gather(\\\\\\*tasks\\\\\\_get\\\\\\_persons) We use await here to wait for all the tasks to finish before assigning the result to all\\\\\\_persons. This is because asyncio.gather returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.as\\\\\\_completed to achieve the same result. Using asyncio.gather allows us to return all the results at once. It is an effective way to speed up our code, but it's not the only way. Particularly, if we have a large dataset, we might not want to wait for everything to finish before starting to process the results. This is where asyncio.as\\\\\\_completed comes into play. asyncio.as\\\\\\_completed: Handling tasks as they complete.¶ async def as\\\\\\_completed(): all\\\\\\_persons = \\\\\\[\\\\\\] tasks\\\\\\_get\\\\\\_persons = \\\\\\[extract\\\\\\_person(text) for text in dataset\\\\\\] for person in asyncio.as\\\\\\_completed(tasks\\\\\\_get\\\\\\_persons): all\\\\\\_persons.append(await person) We use await here to wait for each task to complete before appending it to the list. This is because as\\\\\\_completed returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.gather to achieve the same result. This method is a great way to handle large datasets. We can start processing the results as they come in, especially if we are streaming data back to a client. However, these methods aim to complete as many tasks as possible as quickly as possible. This can be problematic if we want to be considerate to the server we're making requests to. This is where rate limiting comes into play. While there are libraries available to assist with rate limiting, for our initial defense, we will use a semaphore to limit the number of concurrent requests we make. Ordering of results Its important to note that the order of the results will not be the same as the order of the dataset. This is because the tasks are completed in the order they finish, not the order they were started. If you need to preserve the order of the results, you can use asyncio.gather instead. Rate-Limited Gather: Using semaphores to limit concurrency.¶ sem = asyncio.Semaphore(2) async def rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text: str, sem: Semaphore) -> Person: async with sem: We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to. return await extract\\\\\\_person(text) async def rate\\\\\\_limited\\\\\\_gather(sem: Semaphore): tasks\\\\\\_get\\\\\\_persons = \\\\\\[rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text, sem) for text in dataset\\\\\\] resp = await asyncio.gather(\\\\\\*tasks\\\\\\_get\\\\\\_persons) Rate-Limited As Completed: Using semaphores to limit concurrency.¶ sem = asyncio.Semaphore(2) async def rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text: str, sem: Semaphore) -> Person: async with sem: We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to. return await extract\\\\\\_person(text) async def rate\\\\\\_limited\\\\\\_as\\\\\\_completed(sem: Semaphore): all\\\\\\_persons = \\\\\\[\\\\\\] tasks\\\\\\_get\\\\\\_persons = \\\\\\[rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text, sem) for text in dataset\\\\\\] for person in asyncio.as\\\\\\_completed(tasks\\\\\\_get\\\\\\_persons): all\\\\\\_persons.append(await person) We use await here to wait for each task to complete before appending it to the list. This is because as\\\\\\_completed returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.gather to achieve the same result. Now that we have seen the code, let's examine the results of processing 7 texts. As the prompts become longer or if we use GPT-4, the differences between these methods will become more pronounced. Other Options Its important to also note that here we are using a semaphore to limit the number of concurrent requests. However, there are other ways to limit concurrency esp since we have rate limit information from the openai request. You can imagine using a library like ratelimit to limit the number of requests per second. OR catching rate limit exceptions and using tenacity to retry the request after a certain amount of time. tenacity aiolimiter Results¶ As you can see, the for loop is the slowest, while asyncio.as\\\\\\_completed and asyncio.gather are the fastest without any rate limiting. Method Execution Time Rate Limited (Semaphore) For Loop 6.17 seconds Asyncio.gather 0.85 seconds Asyncio.as\\\\\\_completed 0.95 seconds Asyncio.gather 3.04 seconds 2 Asyncio.as\\\\\\_completed 3.26 seconds 2 Practical implications of batch processing¶ The choice of approach depends on the task's nature and the desired balance between speed and resource utilization. Here are some guidelines to consider: Use asyncio.gather for handling multiple independent tasks quickly. Apply asyncio.as\\\\\\_completed for large datasets to process tasks as they complete. Implement rate-limiting to avoid overwhelming servers or API endpoints. If you find the content helpful or want to try out Instructor, please visit our GitHub page and give us a star! Was this page helpful? Back to top Previous Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density Next Verifying LLM Citations with Pydantic Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Structured Outputs with Anyscale - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/12/15/patching/?q=",
    "html": "Skip to content Instructor Structured Outputs with Anyscale Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents Patching Anyscale Back to index Anmol Jawandha Contributor Jason Liu Creator Metadata 2023/12/15 2 min read Structured Outputs with Anyscale¶ Open-source LLMS are gaining popularity, and the release of Anyscale's Mistral model has made it possible to obtain structured outputs using JSON schema at any scale. Instead of relying on a model's default output mode, you can utilize JSON schema to obtain structured outputs. This approach is a time-saving alternative to extensive prompt engineering. By the end of this blog post, you will learn how to effectively utilize the instructor at any scale. But before we proceed, let's first explore the concept of patching. Patching¶ Instructor's patch enhances a openai api it with the following features: response\\\\\\_model in create calls that returns a pydantic model max\\\\\\_retries in create calls that retries the call if it fails by using a backoff strategy Learn More To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page. Anyscale¶ The good news is that Anyscale employs the same OpenAI client, and its models support some of these output modes too! Getting access If you want to try this out for yourself check out the Anyscale website. You can get started here. Let's explore one of the models available in Anyscale's extensive collection! from openai import OpenAI from pydantic import BaseModel import instructor class UserDetails(BaseModel): name: str age: int # enables \\\\\\`response\\\\\\_model\\\\\\` in create call client = instructor.patch( OpenAI( base\\\\\\_url=\"https://api.endpoints.anyscale.com/v1\", api\\\\\\_key=\"\" ), # This uses Anyscale's json schema output mode mode=instructor.Mode.JSON\\\\\\_SCHEMA ) resp = client.chat.completions.create( model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class extractor\" }, { \"role\": \"user\", \"content\": 'Extract the following entities: \"Jason is 20\"' }, \\\\\\], response\\\\\\_model=UserDetails, ) print(resp) # >>> name='Jason' age=20 You can find more information about Anyscale's output mode support here. Was this page helpful? Back to top Previous Introduction to Caching in Python Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/11/05/chain-of-density/?q=",
    "html": "Skip to content Instructor Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents Part 1) Chain of Density Original Prompt Data Modelling Initial Summary Rewritten Summary Putting it all Together Part 2) Fine-Tuning Creating a Training Set Creating Fine-Tuning Jobs Results and Benchmarks Conclusions Back to index Ivan Leo Contributor Jason Liu Creator Metadata 2023/11/05 15 min read Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density¶ Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor In this article, we'll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4's iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density. By the end you'll end up with a GPT 3.5 model, (fine-tuned using Instructor's great tooling), capable of producing summaries that rival the effectiveness of Chain of Density \\\\\\[Adams et al. (2023)\\\\\\]. As always, all code is readily available in our examples/chain-of-density folder in our repo for your reference. Datasets and Colab Notebook Part 1) Chain of Density¶ Summarizing extensive texts with AI can be challenging, often relying on inconsistent techniques. Their novel method, Chain Of Density prompting, enhances AI-based text summarization, outperforming human-generated summaries. Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density. First introduced in the paper - From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. The team has found that this method is able to consistently beats similar summaries written by human annotators. Implementation Details Original Prompt¶ We can break down the original process into smaller api calls. This allows us to introduce validation at each step to ensure that we're getting the results that we want. Original Chain of Density Prompt Improved process with Instructor Data Modelling¶ Before we begin modelling the data, let's make sure we install all of our dependencies pip install instructor aiohttp rich Initial Summary¶ Let's start by walking through some of the data models that we'll be using as the response\\\\\\_model for our open ai function calls Firstly, we'll need a data model for the initial summary that we will be generating. We'll take the description of this class straight from the original prompt. It's important to note that these docstrings serve a purpose, they are directly used by the LLM when generating the outputs. A quick note on Docstrings class InitialSummary(BaseModel): \"\"\" This is an initial summary which should be long ( 4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words. \"\"\" summary: str = Field( ..., description=\"This is a summary of the article provided which is overly verbose and uses fillers. It should be roughly 80 words in length\", ) Rewritten Summary¶ We'll also need one additional class to help model the rewritten schema class RewrittenSummary(BaseModel): \"\"\" This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. Guidelines - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" - Missing entities can appear anywhere in the new summary An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. \"\"\" summary: str = Field( ..., description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\", ) absent: List\\\\\\[str\\\\\\] = Field( ..., default\\\\\\_factory=list, description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\", ) missing: List\\\\\\[str\\\\\\] = Field( default\\\\\\_factory=list, description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\", ) Using Pydantic Validators with Instructor For a more in-depth walkthrough on how to use Pydantic validators with the Instructor library, we recommend checking out our previous article on LLM validation - Good LLM Validation is just Good Validation Ideally, we'd like for Missing to have a length between 1 and 3, Absent to be an empty list and for our rewritten summaries to keep a minimum entity density. With Instructor, we can implement this logic using native Pydantic validators that are simply declared as part of the class itself. import nltk import spacy nlp = spacy.load(\"en\\\\\\_core\\\\\\_web\\\\\\_sm\") @field\\\\\\_validator(\"summary\") def min\\\\\\_length(cls, v: str): tokens = nltk.word\\\\\\_tokenize(v) Similar to the original paper, we utilize the NLTK word tokenizer to count the number of tokens within our generated sentences. We aim for at least 60 tokens in our generated summary so that we don't lose information. num\\\\\\_tokens = len(tokens) if num\\\\\\_tokens < 60: raise ValueError( \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\" ) return v @field\\\\\\_validator(\"missing\") def has\\\\\\_missing\\\\\\_entities(cls, missing\\\\\\_entities: List\\\\\\[str\\\\\\]): if len(missing\\\\\\_entities) == 0: raise ValueError( \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\" ) return missing\\\\\\_entities @field\\\\\\_validator(\"absent\") def has\\\\\\_no\\\\\\_absent\\\\\\_entities(cls, absent\\\\\\_entities: List\\\\\\[str\\\\\\]): absent\\\\\\_entity\\\\\\_string = \",\".join(absent\\\\\\_entities) if len(absent\\\\\\_entities) > 0: print(f\"Detected absent entities of {absent\\\\\\_entity\\\\\\_string}\") raise ValueError( f\"Do not omit the following Entities {absent\\\\\\_entity\\\\\\_string} from the new summary\" ) return absent\\\\\\_entities @field\\\\\\_validator(\"summary\") def min\\\\\\_entity\\\\\\_density(cls, v: str): tokens = nltk.word\\\\\\_tokenize(v) num\\\\\\_tokens = len(tokens) # Extract Entities doc = nlp(v) We also use the spaCy library to calculate the entity density of the generated summary. num\\\\\\_entities = len(doc.ents) density = num\\\\\\_entities / num\\\\\\_tokens if density < 0.08: We also implement a minimum entity density so that we stay within a given range. 0.08 is arbitrarily chosen in this case raise ValueError( f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\" ) return v Putting it all Together¶ Now that we have our models and the rough flow figured out, let's implement a function to summarize a piece of text using Chain Of Density summarization. from openai import OpenAI import instructor client = instructor.patch(OpenAI()) We need to apply a patch function on the OpenAI client for us to get all of the benefits that Instructor provides. With a simple patch, we can get automatic type coercion of our outputs and automatic retries for invalid outputs out of the box! def summarize\\\\\\_article(article: str, summary\\\\\\_steps: int = 3): summary\\\\\\_chain = \\\\\\[\\\\\\] # We first generate an initial summary summary: InitialSummary = client.chat.completions.create( We first generate an initial summary. Note here that we explictly ask for a summary that has 80 words and is lengthy with overly verbose fillers in the system prompt model=\"gpt-4-0613\", response\\\\\\_model=InitialSummary, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": \"The generated summary should be about 80 words.\", }, \\\\\\], max\\\\\\_retries=2, ) prev\\\\\\_summary = None summary\\\\\\_chain.append(summary.summary) for i in range(summary\\\\\\_steps): missing\\\\\\_entity\\\\\\_message = ( \\\\\\[\\\\\\] if prev\\\\\\_summary is None else \\\\\\[ { \"role\": \"user\", \"content\": f\"Please include these Missing Entities: {','.join(prev\\\\\\_summary.missing)}\", }, \\\\\\] ) new\\\\\\_summary: RewrittenSummary = client.chat.completions.create( We slightly modify the original system prompt used in the original paper to perform a rewrite of the summary. Using Instructor, we also get validation of the generated output with our field\\\\\\_validators that we defined above model=\"gpt-4-0613\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"\"\" You are going to generate an increasingly concise,entity-dense summary of the following article. Perform the following two tasks - Identify 1-3 informative entities from the following article which is missing from the previous summary - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities Guidelines - Make every word count: re-write the previous summary to improve flow and make space for additional entities - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\". - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article. - Missing entities can appear anywhere in the new summary - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \"\"\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": f\"Here is the previous summary: {summary\\\\\\_chain\\\\\\[-1\\\\\\]}\", }, \\\\\\*missing\\\\\\_entity\\\\\\_message, \\\\\\], max\\\\\\_retries=3, If you've chosen a value that is larger than 0.08, make sure to increase this value in case you need to do multiple rewrites max\\\\\\_tokens=1000, response\\\\\\_model=RewrittenSummary, ) summary\\\\\\_chain.append(new\\\\\\_summary.summary) prev\\\\\\_summary = new\\\\\\_summary return summary\\\\\\_chain This summarization function yields a result which triples the number of entities while maintaining the same number of tokens. We can also see that stylistically, the summary is a lot more natural. First Iteration This article discusses the highly-anticipated boxing match between Manny Pacquiao and Floyd Mayweather. The article revolves around Manny Pacquiao's statements about his upcoming fight and his preparations for the same. A portion of the article provides details about the financial stipulations of the match and its significance in the sporting arena. Quotes from Pacquiao illustrating his determination and his battle strategy are highlighted. The tone of the article is largely centered around creating a build-up to the upcoming mega event. Final Iteration Manny Pacquiao, the Filipino boxer, anticipates the forthcoming May 2 showdown at the MGM Grand as the fight of his life, against the undefeated American Floyd Mayweather, in a $300m bout. Despite being seen as the underdog in this high-stakes Las Vegas match, Pacquiao is confident, promising a warrior's spirit and assuring the fans who have been awaiting this encounter for a decade, that it will indeed be the biggest sporting spectacle in history worthy of their anticipation Part 2) Fine-Tuning¶ In this section, we'll look into how to fine-tune a GPT 3.5 model so that it is able to perform at an equivalent level as a GPT-4 model. We'll then compare the performance of our model against that of GPT-4 to see how it stacks up. Creating a Training Set¶ In order to prevent any contamination of data during testing, we randomly sampled 120 articles from the griffin/chain-of-density dataset and split these articles into a train.csv and a test.csv file which we uploaded to Hugging Face. Now, we just neeed to import the Instructions module from the Instructor package which allows you to generate a nicely formatted .jsonl file to be used for fine-tuning from typing import List from chain\\\\\\_of\\\\\\_density import summarize\\\\\\_article In this example, we're using the summarize\\\\\\_article that we defined up above. We saved it in a local file called chain\\\\\\_of\\\\\\_density.py, hence the import import csv import logging import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) We patch the default OpenAI client so that we can use the Instructor library with it logging.basicConfig(level=logging.INFO) We also need to configure logging at the INFO level. This is very important, if this is not configured, your output will not be generated. instructions = instructor.Instructions( name=\"Chain Of Density\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"generated.jsonl\")\\\\\\], openai\\\\\\_client=client, ) class GeneratedSummary(BaseModel): \"\"\" This represents a highly concise summary that includes as many entities as possible from the original source article. An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. Guidelines - Make every word count - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" \"\"\" summary: str = Field( ..., description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \", ) @instructions.distil We instantiate a Instruction object which will help us handle the conversion of our function calls into a valid .jsonl file. We also define the name of the .jsonl file in the log\\\\\\_handlers parameter def distil\\\\\\_summarization(text: str) -> GeneratedSummary: summary\\\\\\_chain: List\\\\\\[str\\\\\\] = summarize\\\\\\_article(text) return GeneratedSummary(summary=summary\\\\\\_chain\\\\\\[-1\\\\\\]) We add in an instructions.distil annotation so that we automatically capture the input and output of the function we'd like to fine-tune our model to output with open(\"train.csv\", \"r\") as file: reader = csv.reader(file) next(reader) # Skip the header for article, summary in reader: # Run Distillisation to generate the values distil\\\\\\_summarization(article) Rate Limiting We recommend running this script on a small subset of the dataset first to test you've got everything configured nicely. Don't forget to add in rate limiting error handling with tenacity and set the OPENAI\\\\\\_API\\\\\\_KEY shell environment variable before running any subsequent commands Creating Fine-Tuning Jobs¶ Once we run this script, we'll have a new file called generated.jsonl in our local repository. Now all that's left is to run the command below to start fine-tuning your first model! instructor jobs create-from-file generated.jsonl Finetuning Reference Once the job is complete, all we need to do is to then change the annotation in the function call to distil\\\\\\_summarization in our original file above to start using our new model. @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard def distil\\\\\\_summarization(text: str) -> GeneratedSummary: summary\\\\\\_chain: List\\\\\\[str\\\\\\] = summarize\\\\\\_article(text) return GeneratedSummary(summary=summary\\\\\\_chain\\\\\\[-1\\\\\\]) With that, you've now got your own fine-tuned model ready to go and serve data in production. We've seen how Instructor can make your life easier, from fine-tuning to distillation. Results and Benchmarks¶ We'll be comparing the following models in 3 ways using 20 articles that were not used for fine-tuning. Entity Density : This is entities per token, the higher the better for density. Latency : Time to last token generated in seconds Costs : Total cost to generate outputs - we break down the cost into training and inference costs for easy reference 3.5 Finetuned (n) This is a GPT 3.5 model that we fine-tuned on n examples. Each model was finetuned for 4-5 epochs ( This was automatically decided by the OpenAI scheduler ) GPT-4 (COD) This is a GPT4 model which we applied 3 rounds of Chain Of Density rewrites to generate a summary with using the methodology above GPT-3.5 (Vanilla) This is a GPT 3.5 model that we asked to generate entity-dense summaries which were concise. Summaries were generated in a single pass targetting about 80-90 tokens. Model Mean Latency (s) Mean Entity Density 3.5 Finetuned (20) 2.1 0.15 3.5 Finetuned (50) 2.1 0.14 3.5 Finetuned (76) 2.1 0.14 GPT-3.5 (Vanilla) 16.8 0.12 GPT-4 (COD) 49.5 0.15 Finetuning Datasets Using the OpenAI Usage Dashboard, we can calculate the cost of generating 20 summaries as seen below. Model Training Cost ($) Inference Cost ($) Tokens Used Total Cost ($) GPT-3.5 (Vanilla) - 0.20 51,162 0.2 3.5 Finetuned (20) 0.7 0.20 56,573 0.8 3.5 Finetuned (50) 1.4 0.17 49,057 1.3 3.5 Finetuned (76) 1.8 0.17 51,583 2.5 GPT-4 (COD) - 12.9 409,062 12.9 Here, we can see that GPT-4 has an approximate inference cost of 0.65 per summary while our finetuned models have an inference cost of 0.0091 per summary which is ~ 72x cheaper. Interestingly, the model finetuned with the least examples seems to outperform the others. While the reason for this is unknown, a few potential reasons could be that either we didn't train for sufficient epochs ( We chose the default 5 epochs ) or that the models started learning to imitate other behaviour such as more abstract writing styles from the larger variety of samples, resulting in a decrease in entity density. Conclusions¶ Finetuning this iterative method was 20-40x faster while improving overall performance, resulting in massive efficiency gains by finetuning and distilling capabilities into specialized models. We've seen how Instructor can make your life easier, from data modeling to distillation and finetuning. If you enjoy the content or want to try out instructor check out the github and don't forget to give us a star! Was this page helpful? Back to top Previous AI Engineer Keynote: Pydantic is all you need Next Introduction to Batch Processing using asyncio and Instructor Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/?q=",
    "html": "Skip to content Instructor Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents Introduction Why use Instructor? Quick Start: How to Use Instructor's Distillation Feature The Intricacies of Fine-tuning Language Models Why Instructor and Distillation are Game Changers Role of Instructor in Simplifying Fine-Tuning Logging Output and Running a Finetune Next Steps and Future Plans Conclusion Back to index Jason Liu Creator Metadata 2023/10/17 4 min read Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation¶ Introduction¶ Get ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the instructor.instructions streamlines this process, making the task you want to distil more efficient and powerful while preserving its original functionality and backwards compatibility. If you want to see the full example checkout examples/distillation Why use Instructor?¶ Imagine you're developing a backend service that uses a mix old and new school ML practises, it may involve pipelines with multiple function calls, validations, and data processing. Sounds cumbersome, right? That's where Instructor comes in. It simplifies complex procedures, making them more efficient and easier to manage by adding a decorator to your function that will automatically generate a dataset for fine-tuning and help you swap out the function implementation. Quick Start: How to Use Instructor's Distillation Feature¶ Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file. import logging import random from pydantic import BaseModel from instructor import Instructions # pip install instructor # Logging setup logging.basicConfig(level=logging.INFO) instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"math\\\\\\_finetunes.jsonl\")\\\\\\] ) class Multiply(BaseModel): a: int b: int result: int # Define a function with distillation # The decorator will automatically generate a dataset for fine-tuning # They must return a pydantic model to leverage function calling @instructions.distil def fn(a: int, b: int) -> Multiply: resp = a \\\\\\* b return Multiply(a=a, b=b, result=resp) # Generate some data for \\\\\\_ in range(10): a = random.randint(100, 999) b = random.randint(100, 999) print(fn(a, b)) The Intricacies of Fine-tuning Language Models¶ Fine-tuning isn't just about writing a function like def f(a, b): return a \\\\\\* b. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this. Why Instructor and Distillation are Game Changers¶ The library offers two main benefits: Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code. Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions. Role of Instructor in Simplifying Fine-Tuning¶ The from instructor import Instructions feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior. Logging Output and Running a Finetune¶ Here's how the logging output would look: { \"messages\": \\\\\\[ {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'}, {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'}, {\"role\": \"assistant\", \"function\\\\\\_call\": { \"name\": \"Multiply\", \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}' } } \\\\\\], \"functions\": \\\\\\[ {\"name\": \"Multiply\", \"description\": \"Correctly extracted \\\\\\`Multiply\\\\\\`...\"} \\\\\\] } Run a finetune like this: Don't forget to set your OpenAI Key as an environment variable All of the instructor jobs commands assume you've set an environment variable of OPENAI\\\\\\_API\\\\\\_KEY in your shell. You can set this by running the command export OPENAI\\\\\\_API\\\\\\_KEY= in your shell instructor jobs create-from-file math\\\\\\_finetunes.jsonl Next Steps and Future Plans¶ Here's a sneak peek of what I'm planning: from instructor import Instructions, patch patch() Don't forget to run the patch() command that we provide with the Instructor package. This helps automatically serialize the content back into the \\\\\\`Pydantic\\\\\\`\\\\\\` model that we're looking for. class Multiply(BaseModel): a: int b: int result: int instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", ) @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard def fn(a: int, b: int) -> Multiply: resp = a + b return Multiply(a=a, b=b, result=resp) With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation. Conclusion¶ We've seen how Instructor can make your life easier, from fine-tuning to distillation. Now if you're thinking wow, I'd love a backend service to do this for continously, you're in luck! Please check out the survey at useinstructor.com and let us know who you are. If you enjoy the content or want to try out instructor please check out the github and give us a star! Was this page helpful? Back to top Previous RAG is more than just embedding search Next Good LLM Validation is Just Good Validation Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Verifying LLM Citations with Pydantic - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/11/18/validate-citations/?q=",
    "html": "Skip to content Instructor Verifying LLM Citations with Pydantic Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents Example 1: Simple Substring Check Code Example: Error Message Example: Example 2: Using LLM for Verification Code Example: Result: Error Message Example: Example 3: Aligning Citations and Answers Code Example: Error Message Example: Conclusion Back to index Jason Liu Creator Metadata 2023/11/18 4 min read Verifying LLM Citations with Pydantic¶ Ensuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification. We'll start with using a simple substring check to verify citations. Then we'll use instructor itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses. Example 1: Simple Substring Check¶ In this example, we use the Statements class to verify if a given substring quote exists within a text chunk. If the substring is not found, an error is raised. Code Example:¶ from typing import List, Optional from openai import OpenAI from pydantic import BaseModel, Field, ValidationError, ValidationInfo, field\\\\\\_validator, model\\\\\\_validator import instructor client = instructor.patch(OpenAI()) class Statements(BaseModel): body: str substring\\\\\\_quote: str @field\\\\\\_validator(\"substring\\\\\\_quote\") @classmethod def substring\\\\\\_quote\\\\\\_exists(cls, v: str, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) for text\\\\\\_chunk in context.values(): if v in text\\\\\\_chunk: # While we use a simple substring check in this example, we can use more complex techniques like regex or Levenshtein distance. return v raise ValueError(\"Could not find substring\\\\\\_quote \\\\\\`{v}\\\\\\` in contexts\") class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] Once the class is defined, we can use it to validate the context and raise an error if the substring is not found. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is not the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example:¶ answer.0.substring\\\\\\_quote Value error, Could not find substring\\\\\\_quote \\\\\\`Paris is the capital of France\\\\\\` in contexts \\\\\\[type=value\\\\\\_error, input\\\\\\_value='Paris is the capital of France', input\\\\\\_type=str\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Pydantic raises a validation error when the substring\\\\\\_quote attribute does not exist in the context. This approach can be used to validate more complex data using techniques like regex or Levenshtein distance. Example 2: Using LLM for Verification¶ This approach leverages OpenAI's LLM to validate citations. If the citation does not exist in the context, the LLM returns an error message. Code Example:¶ class Validation(BaseModel): is\\\\\\_valid: bool error\\\\\\_messages: Optional\\\\\\[str\\\\\\] = Field(None, description=\"Error messages if any\") class Statements(BaseModel): body: str substring\\\\\\_quote: str @model\\\\\\_validator(mode=\"after\") def substring\\\\\\_quote\\\\\\_exists(self, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) resp: Validation = client.chat.completions.create( response\\\\\\_model=Validation, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Does the following citation exist in the following context?\\\\\\\\n\\\\\\\\nCitation: {self.substring\\\\\\_quote}\\\\\\\\n\\\\\\\\nContext: {context}\", } \\\\\\], model=\"gpt-3.5-turbo\", ) if resp.is\\\\\\_valid: return self raise ValueError(resp.error\\\\\\_messages) class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] Now when we use a correct citation, the LLM returns a valid response. resp = AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is the capital of France\", 3: \"Irrelevant data\", } }, ) print(resp.model\\\\\\_dump\\\\\\_json(indent=2)) Result:¶ { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ { \"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\" } \\\\\\] } When we have citations that don't exist in the context, the LLM returns an error message. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is not the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example:¶ 1 validation error for AnswerWithCitaton answer.0 Value error, Citation not found in context \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'body': 'Paris', 'substr... the capital of France'}, input\\\\\\_type=dict\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Example 3: Aligning Citations and Answers¶ In this example, we ensure that the provided answers are aligned with the given citations and context. The LLM is used to verify the alignment. We use the same Statements model as above, but we add a new model for the answer that also verifies the alignment of citations. Code Example:¶ class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] @model\\\\\\_validator(mode=\"after\") def validate\\\\\\_answer(self, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) resp: Validation = client.chat.completions.create( response\\\\\\_model=Validation, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Does the following answers match the question and the context?\\\\\\\\n\\\\\\\\nQuestion: {self.question}\\\\\\\\n\\\\\\\\nAnswer: {self.answer}\\\\\\\\n\\\\\\\\nContext: {context}\", } \\\\\\], model=\"gpt-3.5-turbo\", ) if resp.is\\\\\\_valid: return self raise ValueError(resp.error\\\\\\_messages) When we have a mismatch between the answer and the citation, the LLM returns an error message. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Texas\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example:¶ 1 validation error for AnswerWithCitaton Value error, The answer does not match the question and context \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'question': 'What is the...he capital of France'}\\\\\\]}, input\\\\\\_type=dict\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Conclusion¶ These examples demonstrate the potential of using Pydantic and OpenAI to enhance data accuracy through citation verification. While the LLM-based approach may not be efficient for runtime operations, it has exciting implications for generating a dataset of accurate responses. By leveraging this method during data generation, we can fine-tune a model that excels in citation accuracy. Similar to our last post on finetuning a better summarizer. If you like the content check out our GitHub as give us a star and checkout the library. Was this page helpful? Back to top Previous Introduction to Batch Processing using asyncio and Instructor Next Generators and LLM Streaming Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/",
    "html": "Skip to content Instructor Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls Type to start searching instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents The Problem with Existing LLM Frameworks The OpenAI Function Calling Game-Changer Why Pydantic? Simplifying Validation Flow with Pydantic The Modular Approach Composition of Schemas Defining Relationships Using Enums Flexible Schemas Chain of Thought Language Models as Microservices FastAPI Stub Using Instructor as a Function Response Modeling Conclusion Back to index Jason Liu Creator Metadata 2023/09/11 2 min read Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls¶ Language models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic. The Problem with Existing LLM Frameworks¶ Current frameworks for Language Learning Models (LLMs) have complex setups. Developers find it hard to control interactions with language models. Some frameworks require complex JSON Schema setups. The OpenAI Function Calling Game-Changer¶ OpenAI's Function Calling feature provides a constrained interaction model. However, it has its own complexities, mostly around JSON Schema. Why Pydantic?¶ Instructor uses Pydantic to simplify the interaction between the programmer and the language model. Widespread Adoption: Pydantic is a popular tool among Python developers. Simplicity: Pydantic allows model definition in Python. Framework Compatibility: Many Python frameworks already use Pydantic. import pydantic import instructor from openai import OpenAI # Enables the response\\\\\\_model client = instructor.patch(OpenAI()) class UserDetail(pydantic.BaseModel): name: str age: int def introduce(self): return f\"Hello I'm {self.name} and I'm {self.age} years old\" user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] ) Simplifying Validation Flow with Pydantic¶ Pydantic validators simplify features like re-asking or self-critique. This makes these tasks less complex compared to other frameworks. from typing\\\\\\_extensions import Annotated from pydantic import BaseModel, BeforeValidator from instructor import llm\\\\\\_validator, patch from openai import OpenAI class QuestionAnswerNoEvil(BaseModel): question: str answer: Annotated\\\\\\[ str, BeforeValidator( llm\\\\\\_validator(\"don't say objectionable things\") ), \\\\\\] The Modular Approach¶ Pydantic allows for modular output schemas. This leads to more organized code. Composition of Schemas¶ class UserDetails(BaseModel): name: str age: int class UserWithAddress(UserDetails): address: str Defining Relationships¶ class UserDetail(BaseModel): id: int age: int name: str friends: List\\\\\\[int\\\\\\] class UserRelationships(BaseModel): users: List\\\\\\[UserDetail\\\\\\] Using Enums¶ from enum import Enum, auto class Role(Enum): PRINCIPAL = auto() TEACHER = auto() STUDENT = auto() OTHER = auto() class UserDetail(BaseModel): age: int name: str role: Role Flexible Schemas¶ from typing import List class Property(BaseModel): key: str value: str class UserDetail(BaseModel): age: int name: str properties: List\\\\\\[Property\\\\\\] Chain of Thought¶ class TimeRange(BaseModel): chain\\\\\\_of\\\\\\_thought: str start\\\\\\_time: int end\\\\\\_time: int class UserDetail(BaseModel): id: int age: int name: str work\\\\\\_time: TimeRange leisure\\\\\\_time: TimeRange Language Models as Microservices¶ The architecture resembles FastAPI. Most code can be written as Python functions that use Pydantic objects. This eliminates the need for prompt chains. FastAPI Stub¶ app = FastAPI() @app.get(\"/user/{user\\\\\\_id}\", response\\\\\\_model=UserDetails) async def get\\\\\\_user(user\\\\\\_id: int) -> UserDetails: return UserDetails(...) Using Instructor as a Function¶ def extract\\\\\\_user(str) -> UserDetails: return client.chat.completions( response\\\\\\_model=UserDetails, messages=\\\\\\[...\\\\\\] ) Response Modeling¶ class MaybeUser(BaseModel): result: Optional\\\\\\[UserDetail\\\\\\] error: bool message: Optional\\\\\\[str\\\\\\] Conclusion¶ Instructor, with Pydantic, simplifies interaction with language models. It is usable for both experienced and new developers. If you enjoy the content or want to try out instructor please check out the github and give us a star! Was this page helpful? Back to top Next RAG is more than just embedding search Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "2023 - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/archive/2023/?q=",
    "html": "Skip to content Instructor 2023 Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Blog Archive 2023 Table of contents Structured Outputs with Anyscale Introduction to Caching in Python Generators and LLM Streaming Verifying LLM Citations with Pydantic Introduction to Batch Processing using asyncio and Instructor Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density AI Engineer Keynote: Pydantic is all you need Good LLM Validation is Just Good Validation Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation RAG is more than just embedding search 2023¶ 2023/12/15 2 min read Structured Outputs with Anyscale Open-source LLMS are gaining popularity, and the release of Anyscale's Mistral model has made it possible to obtain structured outputs using JSON schema at any scale. Instead of relying on a model's default output mode, you can utilize JSON schema to obtain structured outputs. This approach is a time-saving alternative to extensive prompt engineering. By the end of this blog post, you will learn how to effectively utilize the instructor at any scale. But before we proceed, let's first explore the concept of patching. Patching Instructor's patch enhances a openai api it with the following features: response\\\\\\_model in create calls that returns a pydantic model max\\\\\\_retries in create calls that retries the call if it fails by using a backoff strategy Learn More To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page. Anyscale The good news is that Anyscale employs the same OpenAI client, and its models support some of these output modes too! Getting access If you want to try this out for yourself check out the Anyscale website. You can get started here. Let's explore one of the models available in Anyscale's extensive collection! from openai import OpenAI from pydantic import BaseModel import instructor class UserDetails(BaseModel): name: str age: int # enables \\\\\\`response\\\\\\_model\\\\\\` in create call client = instructor.patch( OpenAI( base\\\\\\_url=\"https://api.endpoints.anyscale.com/v1\", api\\\\\\_key=\"\" ), # This uses Anyscale's json schema output mode mode=instructor.Mode.JSON\\\\\\_SCHEMA ) resp = client.chat.completions.create( model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class extractor\" }, { \"role\": \"user\", \"content\": 'Extract the following entities: \"Jason is 20\"' }, \\\\\\], response\\\\\\_model=UserDetails, ) print(resp) # >>> name='Jason' age=20 You can find more information about Anyscale's output mode support here. Continue reading 2023/11/26 7 min read Introduction to Caching in Python Instructor makes working with language models easy, but they are still computationally expensive. Today, we're diving into optimizing instructor code while maintaining the excellent DX offered by Pydantic models. We'll tackle the challenges of caching Pydantic models, typically incompatible with pickle, and explore solutions that use decorators like functools.cache. Then, we'll craft custom decorators with diskcache and redis to support persistent caching and distributed systems. Lets first consider our canonical example, using the OpenAI Python client to extract user details. import instructor from openai import OpenAI from pydantic import BaseModel # Enables \\\\\\`response\\\\\\_model\\\\\\` client = instructor.patch(OpenAI()) class UserDetail(BaseModel): name: str age: int def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Now imagine batch processing data, running tests or experiments, or simply calling extract multiple times over a workflow. We'll quickly run into performance issues, as the function may be called repeatedly, and the same data will be processed over and over again, costing us time and money. 1. functools.cache for Simple In-Memory Caching When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session or in an application where we don't need to persist the cache between sessions. import functools @functools.cache def extract(data): return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Changing the Model does not Invalidate the Cache Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result. Now we can call extract multiple times with the same argument, and the result will be cached in memory for faster access. import time start = time.perf\\\\\\_counter() # Using time.perf\\\\\\_counter() to measure the time taken to run the function is better than using time.time() because it's more accurate and less susceptible to system clock changes. model = extract(\"Extract jason is 25 years old\") print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") start = time.perf\\\\\\_counter() model = extract(\"Extract jason is 25 years old\") # The second time we call extract, the result is returned from the cache, and the function is not called. print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") >>> Time taken: 0.9267581660533324 >>> Time taken: 1.2080417945981026e-06 # The second call to extract is much faster because the result is returned from the cache! Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries. What is a decorator? 2. diskcache for Persistent, Large Data Caching Copy Caching Code When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data! import functools import inspect import instructor import diskcache from openai import OpenAI from pydantic import BaseModel client = instructor.patch(OpenAI()) cache = diskcache.Cache('./my\\\\\\_cache\\\\\\_directory') def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation # We use inspect.signature to get the function's return type annotation, which we use to validate the cached result. if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type We use Pydantic's model\\\\\\_validate\\\\\\_json to deserialize the cached result into a Pydantic model. return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence. 2. Redis Caching Decorator for Distributed Systems Copy Caching Code When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures. import redis import functools import inspect import json import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) cache = redis.Redis(\"localhost\") def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: # Assuming client.chat.completions.create returns a UserDetail instance return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types. Looking carefully If you look carefully at the code above you'll notice that we're using the same instructor\\\\\\_cache decorator as before. The implementatino is the same, but we're using a different caching backend! Conclusion Choosing the right caching strategy depends on your application's specific needs, such as the size and type of data, the need for persistence, and the system's architecture. Whether it's optimizing a function's performance in a small application or managing large datasets in a distributed environment, Python offers robust solutions to improve efficiency and reduce computational overhead. If you'd like to use this code, try to send it over to ChatGPT to understand it more, and to add additional features that might matter for you, for example, the cache isn't invalidated when your BaseModel changes, so you might want to encode the Model.model\\\\\\_json\\\\\\_schema() as part of the key. If you like the content check out our GitHub as give us a star and checkout the library. Continue reading 2023/11/26 6 min read Generators and LLM Streaming Latency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times. And what makes streaming possible? Generators! In this post, we're going to dive into the cool world of Python generators — these tools are more than just a coding syntax trick. We'll explore Python generators from the ground up and then delve into LLM streaming using the Instructor library. Python Generators: An Efficient Approach to Iterables Generators in Python are a game-changer for handling large data sets and stream processing. They allow functions to yield values one at a time, pausing and resuming their state, which is a faster and more memory-efficient approach compared to traditional collections that store all elements in memory. The Basics: Yielding Values A generator function in Python uses the yield keyword. It yields values one at a time, allowing the function to pause and resume its state. def count\\\\\\_to\\\\\\_3(): yield 1 yield 2 yield 3 for num in count\\\\\\_to\\\\\\_3(): print(num) 1 2 3 Advantages Over Traditional Collections Lazy Evaluation & reduced latency: The time to get the first element (or time-to-first-token in LLM land) from a generator is significantly lower. Generators only produce one value at a time, whereas accessing the first element of a collection will require that the whole collection be created first. Memory Efficiency: Only one item is in memory at a time. Maintain State: Automatically maintains state between executions. Let's see how much faster generators are and where they really shine: import time def expensive\\\\\\_func(x): \"\"\"Simulate an expensive operation.\"\"\" time.sleep(1) return x \\\\\\*\\\\\\* 2 def calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_list(func\\\\\\_input, func): \"\"\"Calculate using a list comprehension and return the first result with its computation time.\"\"\" start\\\\\\_perf = time.perf\\\\\\_counter() result = \\\\\\[func(x) for x in func\\\\\\_input\\\\\\]\\\\\\[0\\\\\\] end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (list): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") return result def calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_generator(func\\\\\\_input, func): \"\"\"Calculate using a generator and return the first result with its computation time.\"\"\" start\\\\\\_perf = time.perf\\\\\\_counter() result = next(func(x) for x in func\\\\\\_input) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (generator): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") return result # Prepare inputs for the function numbers = \\\\\\[1, 2, 3, 4, 5\\\\\\] # Benchmarking first\\\\\\_result\\\\\\_list = calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_list(numbers, expensive\\\\\\_func) first\\\\\\_result\\\\\\_gen = calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_generator(numbers, expensive\\\\\\_func) Time for first result (list): 5.02 seconds Time for first result (generator): 1.01 seconds The generator computes one expensive operation and returns the first result immediately, while the list comprehension computes the expensive operation for all elements in the list before returning the first result. Generator Expressions: A Shortcut Python also allows creating generators in a single line of code, known as generator expressions. They are syntactically similar to list comprehensions but use parentheses. squares = (x\\\\\\*x for x in range(10)) Use Cases in Real-World Applications Generators shine in scenarios like reading large files, data streaming (eg. llm token streaming), and pipeline creation for data processing. LLM Streaming If you've used ChatGPT, you'll see that the tokens are streamed out one by one, instead of the full response being shown at the end (can you imagine waiting for the full response??). This is made possible by generators. Here's how a vanilla openai generator looks: from openai import OpenAI # Set your OpenAI API key client = OpenAI( api\\\\\\_key=\"My API Key\", ) response\\\\\\_generator = client.chat.completions.create( model='gpt-3.5-turbo', messages=\\\\\\[ {'role': 'user', 'content': \"What are some good reasons to smile?\"} \\\\\\], temperature=0, stream=True ) for chunk in response\\\\\\_generator: print(chunk.choices\\\\\\[0\\\\\\].delta.content, end=\"\") This is great, but what if we want to do some structured extraction on this stream? For instance, we might want to render frontend components based on product rankings that are streamed out by an LLM. Should we wait for the entire stream to finish before extracting & validating the list of components or can we extract & validate the components in real time as they are streamed? In e-commerce, every millisecond matters so the time-to-first-render can differentiate a successful and not-so-successful e commerce store (and i know how a failing e commerce store feels :/ ). Let's see how we can use Instructor to handle extraction from this real time stream! E-commerce Product Ranking SCENARIO Imagine an e-commerce platform where we have: • a customer profile: this includes a detailed history of purchases, browsing behavior, product ratings, preferences in various categories, search history, and even responses to previous recommendations. This extensive data is crucial for generating highly personalized and relevant product suggestions. • a list of candidate products: these could be some shortlisted products we think the customer would like. Our goal is to re-rerank these candidate products for the best conversion and we'll use an LLM! STREAM PROCESSING User Data: Let's assume we have the following user profile: profile\\\\\\_data = \"\"\" Customer ID: 12345 Recent Purchases: \\\\\\[Laptop, Wireless Headphones, Smart Watch\\\\\\] Frequently Browsed Categories: \\\\\\[Electronics, Books, Fitness Equipment\\\\\\] Product Ratings: {Laptop: 5 stars, Wireless Headphones: 4 stars} Recent Search History: \\\\\\[best budget laptops 2023, latest sci-fi books, yoga mats\\\\\\] Preferred Brands: \\\\\\[Apple, AllBirds, Bench\\\\\\] Responses to Previous Recommendations: {Philips: Not Interested, Adidas: Not Interested} Loyalty Program Status: Gold Member Average Monthly Spend: $500 Preferred Shopping Times: Weekend Evenings ... \"\"\" We want to rank the following products for this user: products = \\\\\\[ {\"product\\\\\\_id\": 1, \"product\\\\\\_name\": \"Apple MacBook Air (2023) - Latest model, high performance, portable\"}, {\"product\\\\\\_id\": 2, \"product\\\\\\_name\": \"Sony WH-1000XM4 Wireless Headphones - Noise-canceling, long battery life\"}, {\"product\\\\\\_id\": 3, \"product\\\\\\_name\": \"Apple Watch Series 7 - Advanced fitness tracking, seamless integration with Apple ecosystem\"}, {\"product\\\\\\_id\": 4, \"product\\\\\\_name\": \"Kindle Oasis - Premium e-reader with adjustable warm light\"}, {\"product\\\\\\_id\": 5, \"product\\\\\\_name\": \"AllBirds Wool Runners - Comfortable, eco-friendly sneakers\"}, {\"product\\\\\\_id\": 6, \"product\\\\\\_name\": \"Manduka PRO Yoga Mat - High-quality, durable, eco-friendly\"}, {\"product\\\\\\_id\": 7, \"product\\\\\\_name\": \"Bench Hooded Jacket - Stylish, durable, suitable for outdoor activities\"}, {\"product\\\\\\_id\": 8, \"product\\\\\\_name\": \"GoPro HERO9 Black - 5K video, waterproof, for action photography\"}, {\"product\\\\\\_id\": 9, \"product\\\\\\_name\": \"Nespresso Vertuo Next Coffee Machine - Quality coffee, easy to use, compact design\"}, {\"product\\\\\\_id\": 10, \"product\\\\\\_name\": \"Project Hail Mary by Andy Weir - Latest sci-fi book from a renowned author\"} \\\\\\] Let's now define our models for structured extraction. Note: instructor will conveniently let us use Iterable to model an iterable of our class. In this case, once we define our product recommendation model, we can slap on Iterable to define what we ultimately want - a (ranked) list of product recommendations. import instructor from openai import OpenAI from typing import Iterable from pydantic import BaseModel client = instructor.patch(OpenAI(), mode=instructor.function\\\\\\_calls.Mode.JSON) class ProductRecommendation(BaseModel): product\\\\\\_id: str product\\\\\\_name: str Recommendations = Iterable\\\\\\[ProductRecommendation\\\\\\] Now let's use our instructor patch. Since we don't want to wait for all the tokens to finish, will set stream to True and process each product recommendation as it comes in: prompt = f\"Based on the following user profile:\\\\\\\\n{profile\\\\\\_data}\\\\\\\\nRank the following products from most relevant to least relevant:\\\\\\\\n\" + '\\\\\\\\n'.join(f\"{product\\\\\\['product\\\\\\_id'\\\\\\]} {product\\\\\\['product\\\\\\_name'\\\\\\]}\" for product in products) start\\\\\\_perf = time.perf\\\\\\_counter() recommendations\\\\\\_stream = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Iterable\\\\\\[ProductRecommendation\\\\\\], stream=True, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"}, {\"role\": \"user\", \"content\": prompt} \\\\\\] ) for product in recommendations\\\\\\_stream: print(product) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (generator): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") break product\\\\\\_id='1' product\\\\\\_name='Apple MacBook Air (2023)' Time for first result (generator): 4.33 seconds recommendations\\\\\\_stream is a generator! It yields the extracted products as it's processing the stream in real-time. Now let's get the same response without streaming and see how they compare. start\\\\\\_perf = time.perf\\\\\\_counter() recommendations\\\\\\_list = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Iterable\\\\\\[ProductRecommendation\\\\\\], stream=False, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"}, {\"role\": \"user\", \"content\": prompt} \\\\\\] ) print(recommendations\\\\\\_list\\\\\\[0\\\\\\]) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (list): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") product\\\\\\_id='1' product\\\\\\_name='Apple MacBook Air (2023)' Time for first result (list): 8.63 seconds Our web application now displays results faster. Even a 100ms improvement can lead to a 1% increase in revenue. FastAPI We can also take this and set up a streaming LLM API endpoint using FastAPI. Check out our docs on using FastAPI here! Key Takeaways To summarize, we looked at: • Generators in Python: A powerful feature that allows for efficient data handling with reduced latency • LLM Streaming: LLMs provide us generators to stream tokens and Instructor can let us validate and extract data from this stream. Real-time data validation ftw! Don't forget to check our GitHub for more resources and give us a star if you find the library helpful! If you have any questions or need further clarifications, feel free to reach out or dive into the Instructor library's documentation for more detailed information. Happy coding! Continue reading 2023/11/18 4 min read Verifying LLM Citations with Pydantic Ensuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification. We'll start with using a simple substring check to verify citations. Then we'll use instructor itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses. Example 1: Simple Substring Check In this example, we use the Statements class to verify if a given substring quote exists within a text chunk. If the substring is not found, an error is raised. Code Example: from typing import List, Optional from openai import OpenAI from pydantic import BaseModel, Field, ValidationError, ValidationInfo, field\\\\\\_validator, model\\\\\\_validator import instructor client = instructor.patch(OpenAI()) class Statements(BaseModel): body: str substring\\\\\\_quote: str @field\\\\\\_validator(\"substring\\\\\\_quote\") @classmethod def substring\\\\\\_quote\\\\\\_exists(cls, v: str, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) for text\\\\\\_chunk in context.values(): if v in text\\\\\\_chunk: # While we use a simple substring check in this example, we can use more complex techniques like regex or Levenshtein distance. return v raise ValueError(\"Could not find substring\\\\\\_quote \\\\\\`{v}\\\\\\` in contexts\") class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] Once the class is defined, we can use it to validate the context and raise an error if the substring is not found. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is not the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example: answer.0.substring\\\\\\_quote Value error, Could not find substring\\\\\\_quote \\\\\\`Paris is the capital of France\\\\\\` in contexts \\\\\\[type=value\\\\\\_error, input\\\\\\_value='Paris is the capital of France', input\\\\\\_type=str\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Pydantic raises a validation error when the substring\\\\\\_quote attribute does not exist in the context. This approach can be used to validate more complex data using techniques like regex or Levenshtein distance. Example 2: Using LLM for Verification This approach leverages OpenAI's LLM to validate citations. If the citation does not exist in the context, the LLM returns an error message. Code Example: class Validation(BaseModel): is\\\\\\_valid: bool error\\\\\\_messages: Optional\\\\\\[str\\\\\\] = Field(None, description=\"Error messages if any\") class Statements(BaseModel): body: str substring\\\\\\_quote: str @model\\\\\\_validator(mode=\"after\") def substring\\\\\\_quote\\\\\\_exists(self, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) resp: Validation = client.chat.completions.create( response\\\\\\_model=Validation, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Does the following citation exist in the following context?\\\\\\\\n\\\\\\\\nCitation: {self.substring\\\\\\_quote}\\\\\\\\n\\\\\\\\nContext: {context}\", } \\\\\\], model=\"gpt-3.5-turbo\", ) if resp.is\\\\\\_valid: return self raise ValueError(resp.error\\\\\\_messages) class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] Now when we use a correct citation, the LLM returns a valid response. resp = AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is the capital of France\", 3: \"Irrelevant data\", } }, ) print(resp.model\\\\\\_dump\\\\\\_json(indent=2)) Result: { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ { \"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\" } \\\\\\] } When we have citations that don't exist in the context, the LLM returns an error message. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is not the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example: 1 validation error for AnswerWithCitaton answer.0 Value error, Citation not found in context \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'body': 'Paris', 'substr... the capital of France'}, input\\\\\\_type=dict\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Example 3: Aligning Citations and Answers In this example, we ensure that the provided answers are aligned with the given citations and context. The LLM is used to verify the alignment. We use the same Statements model as above, but we add a new model for the answer that also verifies the alignment of citations. Code Example: class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] @model\\\\\\_validator(mode=\"after\") def validate\\\\\\_answer(self, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) resp: Validation = client.chat.completions.create( response\\\\\\_model=Validation, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Does the following answers match the question and the context?\\\\\\\\n\\\\\\\\nQuestion: {self.question}\\\\\\\\n\\\\\\\\nAnswer: {self.answer}\\\\\\\\n\\\\\\\\nContext: {context}\", } \\\\\\], model=\"gpt-3.5-turbo\", ) if resp.is\\\\\\_valid: return self raise ValueError(resp.error\\\\\\_messages) When we have a mismatch between the answer and the citation, the LLM returns an error message. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Texas\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example: 1 validation error for AnswerWithCitaton Value error, The answer does not match the question and context \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'question': 'What is the...he capital of France'}\\\\\\]}, input\\\\\\_type=dict\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Conclusion These examples demonstrate the potential of using Pydantic and OpenAI to enhance data accuracy through citation verification. While the LLM-based approach may not be efficient for runtime operations, it has exciting implications for generating a dataset of accurate responses. By leveraging this method during data generation, we can fine-tune a model that excels in citation accuracy. Similar to our last post on finetuning a better summarizer. If you like the content check out our GitHub as give us a star and checkout the library. Continue reading 2023/11/13 6 min read Introduction to Batch Processing using asyncio and Instructor Today, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using instructor and learn how to use asyncio.gather and asyncio.as\\\\\\_completed for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using asyncio.Semaphore. Github Example If you want to run the code examples in this article, you can find them on jxnl/instructor We will start by defining an async function that calls openai to extract data, and then examine four different ways to execute it. We will discuss the pros and cons of each approach and analyze the results of running them on a small batch. Understanding asyncio asyncio is a Python library that enables writing concurrent code using the async/await syntax. It is particularly useful for IO-bound and structured network code. If you are familiar with OpenAI's SDK, you might have encountered two classes: OpenAI() and AsyncOpenAI(). Today, we will be using the AsyncOpenAI() class, which processes data asynchronously. By utilizing these tools in web applications or batch processing, we can significantly improve performance by handling multiple requests concurrently instead of sequentially. Understanding async and await We will be using the async and await keywords to define asynchronous functions. The async keyword is used to define a function that returns a coroutine object. The await keyword is used to wait for the result of a coroutine object. If you want to understand the deeper details of asyncio, I recommend reading this article by Real Python. Understanding gather vs as\\\\\\_completed In this post we'll show two ways to run tasks concurrently: asyncio.gather and asyncio.as\\\\\\_completed. The gather method is used to run multiple tasks concurrently and return the results as a list. The as\\\\\\_completed returns a iterable is used to run multiple tasks concurrently and return the results as they complete. Another great resource on the differences between the two can be found here. Example: Batch Processing In this example, we will demonstrate how to use asyncio for batch processing tasks, specifically for extracting and processing data concurrently. The script will extract data from a list of texts and process it concurrently using asyncio. import instructor from pydantic import BaseModel from openai import AsyncOpenAI # Enables \\\\\\`response\\\\\\_model\\\\\\` in \\\\\\`create\\\\\\` method client = instructor.apatch(AsyncOpenAI()) We use instructor.apatch to patch the create method of AsyncOpenAI to accept a response\\\\\\_model argument. This is because the create method of AsyncOpenAI does not accept a response\\\\\\_model argument without this patch. class Person(BaseModel): name: str age: int async def extract\\\\\\_person(text: str) -> Person: return await client.chat.completions.create( We use await here to wait for the response from the server before we return the result. This is because create returns a coroutine object, not the result of the coroutine. model=\"gpt-3.5-turbo\", messages=\\\\\\[ {\"role\": \"user\", \"content\": text}, \\\\\\], response\\\\\\_model=Person, ) Notice that now there are async and await keywords in the function definition. This is because we're using the asyncio library to run the function concurrently. Now lets define a batch of texts to process. dataset = \\\\\\[ \"My name is John and I am 20 years old\", \"My name is Mary and I am 21 years old\", \"My name is Bob and I am 22 years old\", \"My name is Alice and I am 23 years old\", \"My name is Jane and I am 24 years old\", \"My name is Joe and I am 25 years old\", \"My name is Jill and I am 26 years old\", \\\\\\] for loop: Running tasks sequentially. persons = \\\\\\[\\\\\\] for text in dataset: person = await extract\\\\\\_person(text) persons.append(person) Even though there is an await keyword, we still have to wait for each task to finish before starting the next one. This is because we're using a for loop to iterate over the dataset. This method, which uses a for loop, will be the slowest among the four methods discussed today. asyncio.gather: Running tasks concurrently. async def gather(): tasks\\\\\\_get\\\\\\_persons = \\\\\\[extract\\\\\\_person(text) for text in dataset\\\\\\] all\\\\\\_persons = await asyncio.gather(\\\\\\*tasks\\\\\\_get\\\\\\_persons) We use await here to wait for all the tasks to finish before assigning the result to all\\\\\\_persons. This is because asyncio.gather returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.as\\\\\\_completed to achieve the same result. Using asyncio.gather allows us to return all the results at once. It is an effective way to speed up our code, but it's not the only way. Particularly, if we have a large dataset, we might not want to wait for everything to finish before starting to process the results. This is where asyncio.as\\\\\\_completed comes into play. asyncio.as\\\\\\_completed: Handling tasks as they complete. async def as\\\\\\_completed(): all\\\\\\_persons = \\\\\\[\\\\\\] tasks\\\\\\_get\\\\\\_persons = \\\\\\[extract\\\\\\_person(text) for text in dataset\\\\\\] for person in asyncio.as\\\\\\_completed(tasks\\\\\\_get\\\\\\_persons): all\\\\\\_persons.append(await person) We use await here to wait for each task to complete before appending it to the list. This is because as\\\\\\_completed returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.gather to achieve the same result. This method is a great way to handle large datasets. We can start processing the results as they come in, especially if we are streaming data back to a client. However, these methods aim to complete as many tasks as possible as quickly as possible. This can be problematic if we want to be considerate to the server we're making requests to. This is where rate limiting comes into play. While there are libraries available to assist with rate limiting, for our initial defense, we will use a semaphore to limit the number of concurrent requests we make. Ordering of results Its important to note that the order of the results will not be the same as the order of the dataset. This is because the tasks are completed in the order they finish, not the order they were started. If you need to preserve the order of the results, you can use asyncio.gather instead. Rate-Limited Gather: Using semaphores to limit concurrency. sem = asyncio.Semaphore(2) async def rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text: str, sem: Semaphore) -> Person: async with sem: We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to. return await extract\\\\\\_person(text) async def rate\\\\\\_limited\\\\\\_gather(sem: Semaphore): tasks\\\\\\_get\\\\\\_persons = \\\\\\[rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text, sem) for text in dataset\\\\\\] resp = await asyncio.gather(\\\\\\*tasks\\\\\\_get\\\\\\_persons) Rate-Limited As Completed: Using semaphores to limit concurrency. sem = asyncio.Semaphore(2) async def rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text: str, sem: Semaphore) -> Person: async with sem: We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to. return await extract\\\\\\_person(text) async def rate\\\\\\_limited\\\\\\_as\\\\\\_completed(sem: Semaphore): all\\\\\\_persons = \\\\\\[\\\\\\] tasks\\\\\\_get\\\\\\_persons = \\\\\\[rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text, sem) for text in dataset\\\\\\] for person in asyncio.as\\\\\\_completed(tasks\\\\\\_get\\\\\\_persons): all\\\\\\_persons.append(await person) We use await here to wait for each task to complete before appending it to the list. This is because as\\\\\\_completed returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.gather to achieve the same result. Now that we have seen the code, let's examine the results of processing 7 texts. As the prompts become longer or if we use GPT-4, the differences between these methods will become more pronounced. Other Options Its important to also note that here we are using a semaphore to limit the number of concurrent requests. However, there are other ways to limit concurrency esp since we have rate limit information from the openai request. You can imagine using a library like ratelimit to limit the number of requests per second. OR catching rate limit exceptions and using tenacity to retry the request after a certain amount of time. tenacity aiolimiter Results As you can see, the for loop is the slowest, while asyncio.as\\\\\\_completed and asyncio.gather are the fastest without any rate limiting. Method Execution Time Rate Limited (Semaphore) For Loop 6.17 seconds Asyncio.gather 0.85 seconds Asyncio.as\\\\\\_completed 0.95 seconds Asyncio.gather 3.04 seconds 2 Asyncio.as\\\\\\_completed 3.26 seconds 2 Practical implications of batch processing The choice of approach depends on the task's nature and the desired balance between speed and resource utilization. Here are some guidelines to consider: Use asyncio.gather for handling multiple independent tasks quickly. Apply asyncio.as\\\\\\_completed for large datasets to process tasks as they complete. Implement rate-limiting to avoid overwhelming servers or API endpoints. If you find the content helpful or want to try out Instructor, please visit our GitHub page and give us a star! Continue reading 2023/11/05 15 min read Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor In this article, we'll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4's iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density. By the end you'll end up with a GPT 3.5 model, (fine-tuned using Instructor's great tooling), capable of producing summaries that rival the effectiveness of Chain of Density \\\\\\[Adams et al. (2023)\\\\\\]. As always, all code is readily available in our examples/chain-of-density folder in our repo for your reference. Datasets and Colab Notebook Part 1) Chain of Density Summarizing extensive texts with AI can be challenging, often relying on inconsistent techniques. Their novel method, Chain Of Density prompting, enhances AI-based text summarization, outperforming human-generated summaries. Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density. First introduced in the paper - From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. The team has found that this method is able to consistently beats similar summaries written by human annotators. Implementation Details Original Prompt We can break down the original process into smaller api calls. This allows us to introduce validation at each step to ensure that we're getting the results that we want. Original Chain of Density Prompt Improved process with Instructor Data Modelling Before we begin modelling the data, let's make sure we install all of our dependencies pip install instructor aiohttp rich INITIAL SUMMARY Let's start by walking through some of the data models that we'll be using as the response\\\\\\_model for our open ai function calls Firstly, we'll need a data model for the initial summary that we will be generating. We'll take the description of this class straight from the original prompt. It's important to note that these docstrings serve a purpose, they are directly used by the LLM when generating the outputs. A quick note on Docstrings class InitialSummary(BaseModel): \"\"\" This is an initial summary which should be long ( 4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words. \"\"\" summary: str = Field( ..., description=\"This is a summary of the article provided which is overly verbose and uses fillers. It should be roughly 80 words in length\", ) REWRITTEN SUMMARY We'll also need one additional class to help model the rewritten schema class RewrittenSummary(BaseModel): \"\"\" This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. Guidelines - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" - Missing entities can appear anywhere in the new summary An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. \"\"\" summary: str = Field( ..., description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\", ) absent: List\\\\\\[str\\\\\\] = Field( ..., default\\\\\\_factory=list, description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\", ) missing: List\\\\\\[str\\\\\\] = Field( default\\\\\\_factory=list, description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\", ) Using Pydantic Validators with Instructor For a more in-depth walkthrough on how to use Pydantic validators with the Instructor library, we recommend checking out our previous article on LLM validation - Good LLM Validation is just Good Validation Ideally, we'd like for Missing to have a length between 1 and 3, Absent to be an empty list and for our rewritten summaries to keep a minimum entity density. With Instructor, we can implement this logic using native Pydantic validators that are simply declared as part of the class itself. import nltk import spacy nlp = spacy.load(\"en\\\\\\_core\\\\\\_web\\\\\\_sm\") @field\\\\\\_validator(\"summary\") def min\\\\\\_length(cls, v: str): tokens = nltk.word\\\\\\_tokenize(v) Similar to the original paper, we utilize the NLTK word tokenizer to count the number of tokens within our generated sentences. We aim for at least 60 tokens in our generated summary so that we don't lose information. num\\\\\\_tokens = len(tokens) if num\\\\\\_tokens < 60: raise ValueError( \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\" ) return v @field\\\\\\_validator(\"missing\") def has\\\\\\_missing\\\\\\_entities(cls, missing\\\\\\_entities: List\\\\\\[str\\\\\\]): if len(missing\\\\\\_entities) == 0: raise ValueError( \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\" ) return missing\\\\\\_entities @field\\\\\\_validator(\"absent\") def has\\\\\\_no\\\\\\_absent\\\\\\_entities(cls, absent\\\\\\_entities: List\\\\\\[str\\\\\\]): absent\\\\\\_entity\\\\\\_string = \",\".join(absent\\\\\\_entities) if len(absent\\\\\\_entities) > 0: print(f\"Detected absent entities of {absent\\\\\\_entity\\\\\\_string}\") raise ValueError( f\"Do not omit the following Entities {absent\\\\\\_entity\\\\\\_string} from the new summary\" ) return absent\\\\\\_entities @field\\\\\\_validator(\"summary\") def min\\\\\\_entity\\\\\\_density(cls, v: str): tokens = nltk.word\\\\\\_tokenize(v) num\\\\\\_tokens = len(tokens) # Extract Entities doc = nlp(v) We also use the spaCy library to calculate the entity density of the generated summary. num\\\\\\_entities = len(doc.ents) density = num\\\\\\_entities / num\\\\\\_tokens if density < 0.08: We also implement a minimum entity density so that we stay within a given range. 0.08 is arbitrarily chosen in this case raise ValueError( f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\" ) return v Putting it all Together Now that we have our models and the rough flow figured out, let's implement a function to summarize a piece of text using Chain Of Density summarization. from openai import OpenAI import instructor client = instructor.patch(OpenAI()) We need to apply a patch function on the OpenAI client for us to get all of the benefits that Instructor provides. With a simple patch, we can get automatic type coercion of our outputs and automatic retries for invalid outputs out of the box! def summarize\\\\\\_article(article: str, summary\\\\\\_steps: int = 3): summary\\\\\\_chain = \\\\\\[\\\\\\] # We first generate an initial summary summary: InitialSummary = client.chat.completions.create( We first generate an initial summary. Note here that we explictly ask for a summary that has 80 words and is lengthy with overly verbose fillers in the system prompt model=\"gpt-4-0613\", response\\\\\\_model=InitialSummary, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": \"The generated summary should be about 80 words.\", }, \\\\\\], max\\\\\\_retries=2, ) prev\\\\\\_summary = None summary\\\\\\_chain.append(summary.summary) for i in range(summary\\\\\\_steps): missing\\\\\\_entity\\\\\\_message = ( \\\\\\[\\\\\\] if prev\\\\\\_summary is None else \\\\\\[ { \"role\": \"user\", \"content\": f\"Please include these Missing Entities: {','.join(prev\\\\\\_summary.missing)}\", }, \\\\\\] ) new\\\\\\_summary: RewrittenSummary = client.chat.completions.create( We slightly modify the original system prompt used in the original paper to perform a rewrite of the summary. Using Instructor, we also get validation of the generated output with our field\\\\\\_validators that we defined above model=\"gpt-4-0613\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"\"\" You are going to generate an increasingly concise,entity-dense summary of the following article. Perform the following two tasks - Identify 1-3 informative entities from the following article which is missing from the previous summary - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities Guidelines - Make every word count: re-write the previous summary to improve flow and make space for additional entities - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\". - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article. - Missing entities can appear anywhere in the new summary - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \"\"\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": f\"Here is the previous summary: {summary\\\\\\_chain\\\\\\[-1\\\\\\]}\", }, \\\\\\*missing\\\\\\_entity\\\\\\_message, \\\\\\], max\\\\\\_retries=3, If you've chosen a value that is larger than 0.08, make sure to increase this value in case you need to do multiple rewrites max\\\\\\_tokens=1000, response\\\\\\_model=RewrittenSummary, ) summary\\\\\\_chain.append(new\\\\\\_summary.summary) prev\\\\\\_summary = new\\\\\\_summary return summary\\\\\\_chain This summarization function yields a result which triples the number of entities while maintaining the same number of tokens. We can also see that stylistically, the summary is a lot more natural. First Iteration This article discusses the highly-anticipated boxing match between Manny Pacquiao and Floyd Mayweather. The article revolves around Manny Pacquiao's statements about his upcoming fight and his preparations for the same. A portion of the article provides details about the financial stipulations of the match and its significance in the sporting arena. Quotes from Pacquiao illustrating his determination and his battle strategy are highlighted. The tone of the article is largely centered around creating a build-up to the upcoming mega event. Final Iteration Manny Pacquiao, the Filipino boxer, anticipates the forthcoming May 2 showdown at the MGM Grand as the fight of his life, against the undefeated American Floyd Mayweather, in a $300m bout. Despite being seen as the underdog in this high-stakes Las Vegas match, Pacquiao is confident, promising a warrior's spirit and assuring the fans who have been awaiting this encounter for a decade, that it will indeed be the biggest sporting spectacle in history worthy of their anticipation Part 2) Fine-Tuning In this section, we'll look into how to fine-tune a GPT 3.5 model so that it is able to perform at an equivalent level as a GPT-4 model. We'll then compare the performance of our model against that of GPT-4 to see how it stacks up. Creating a Training Set In order to prevent any contamination of data during testing, we randomly sampled 120 articles from the griffin/chain-of-density dataset and split these articles into a train.csv and a test.csv file which we uploaded to Hugging Face. Now, we just neeed to import the Instructions module from the Instructor package which allows you to generate a nicely formatted .jsonl file to be used for fine-tuning from typing import List from chain\\\\\\_of\\\\\\_density import summarize\\\\\\_article In this example, we're using the summarize\\\\\\_article that we defined up above. We saved it in a local file called chain\\\\\\_of\\\\\\_density.py, hence the import import csv import logging import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) We patch the default OpenAI client so that we can use the Instructor library with it logging.basicConfig(level=logging.INFO) We also need to configure logging at the INFO level. This is very important, if this is not configured, your output will not be generated. instructions = instructor.Instructions( name=\"Chain Of Density\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"generated.jsonl\")\\\\\\], openai\\\\\\_client=client, ) class GeneratedSummary(BaseModel): \"\"\" This represents a highly concise summary that includes as many entities as possible from the original source article. An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. Guidelines - Make every word count - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" \"\"\" summary: str = Field( ..., description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \", ) @instructions.distil We instantiate a Instruction object which will help us handle the conversion of our function calls into a valid .jsonl file. We also define the name of the .jsonl file in the log\\\\\\_handlers parameter def distil\\\\\\_summarization(text: str) -> GeneratedSummary: summary\\\\\\_chain: List\\\\\\[str\\\\\\] = summarize\\\\\\_article(text) return GeneratedSummary(summary=summary\\\\\\_chain\\\\\\[-1\\\\\\]) We add in an instructions.distil annotation so that we automatically capture the input and output of the function we'd like to fine-tune our model to output with open(\"train.csv\", \"r\") as file: reader = csv.reader(file) next(reader) # Skip the header for article, summary in reader: # Run Distillisation to generate the values distil\\\\\\_summarization(article) Rate Limiting We recommend running this script on a small subset of the dataset first to test you've got everything configured nicely. Don't forget to add in rate limiting error handling with tenacity and set the OPENAI\\\\\\_API\\\\\\_KEY shell environment variable before running any subsequent commands Creating Fine-Tuning Jobs Once we run this script, we'll have a new file called generated.jsonl in our local repository. Now all that's left is to run the command below to start fine-tuning your first model! instructor jobs create-from-file generated.jsonl Finetuning Reference Once the job is complete, all we need to do is to then change the annotation in the function call to distil\\\\\\_summarization in our original file above to start using our new model. @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard def distil\\\\\\_summarization(text: str) -> GeneratedSummary: summary\\\\\\_chain: List\\\\\\[str\\\\\\] = summarize\\\\\\_article(text) return GeneratedSummary(summary=summary\\\\\\_chain\\\\\\[-1\\\\\\]) With that, you've now got your own fine-tuned model ready to go and serve data in production. We've seen how Instructor can make your life easier, from fine-tuning to distillation. Results and Benchmarks We'll be comparing the following models in 3 ways using 20 articles that were not used for fine-tuning. Entity Density : This is entities per token, the higher the better for density. Latency : Time to last token generated in seconds Costs : Total cost to generate outputs - we break down the cost into training and inference costs for easy reference 3.5 Finetuned (n) This is a GPT 3.5 model that we fine-tuned on n examples. Each model was finetuned for 4-5 epochs ( This was automatically decided by the OpenAI scheduler ) GPT-4 (COD) This is a GPT4 model which we applied 3 rounds of Chain Of Density rewrites to generate a summary with using the methodology above GPT-3.5 (Vanilla) This is a GPT 3.5 model that we asked to generate entity-dense summaries which were concise. Summaries were generated in a single pass targetting about 80-90 tokens. Model Mean Latency (s) Mean Entity Density 3.5 Finetuned (20) 2.1 0.15 3.5 Finetuned (50) 2.1 0.14 3.5 Finetuned (76) 2.1 0.14 GPT-3.5 (Vanilla) 16.8 0.12 GPT-4 (COD) 49.5 0.15 Finetuning Datasets Using the OpenAI Usage Dashboard, we can calculate the cost of generating 20 summaries as seen below. Model Training Cost ($) Inference Cost ($) Tokens Used Total Cost ($) GPT-3.5 (Vanilla) - 0.20 51,162 0.2 3.5 Finetuned (20) 0.7 0.20 56,573 0.8 3.5 Finetuned (50) 1.4 0.17 49,057 1.3 3.5 Finetuned (76) 1.8 0.17 51,583 2.5 GPT-4 (COD) - 12.9 409,062 12.9 Here, we can see that GPT-4 has an approximate inference cost of 0.65 per summary while our finetuned models have an inference cost of 0.0091 per summary which is ~ 72x cheaper. Interestingly, the model finetuned with the least examples seems to outperform the others. While the reason for this is unknown, a few potential reasons could be that either we didn't train for sufficient epochs ( We chose the default 5 epochs ) or that the models started learning to imitate other behaviour such as more abstract writing styles from the larger variety of samples, resulting in a decrease in entity density. Conclusions Finetuning this iterative method was 20-40x faster while improving overall performance, resulting in massive efficiency gains by finetuning and distilling capabilities into specialized models. We've seen how Instructor can make your life easier, from data modeling to distillation and finetuning. If you enjoy the content or want to try out instructor check out the github and don't forget to give us a star! Continue reading 2023/11/02 1 min read AI Engineer Keynote: Pydantic is all you need Click here to watch the full talk Last month, I ventured back onto the speaking circuit at the inaugural AI Engineer Summit, sharing insights on leveraging Pydantic for effective prompt engineering. I dove deep into what is covered in our documentation and standard blog posts, I'd genuinely appreciate any feedback on the talk – every bit helps in refining the art. So, take a moment to check out the full talk here, and let's continue pushing the boundaries of what's possible. Continue reading 2023/10/23 10 min read Good LLM Validation is Just Good Validation What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here. Validation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like Pydantic and Instructor. We validate these outputs using a validation function which conforms to the structure seen below. def validation\\\\\\_function(value): if condition(value): raise ValueError(\"Value is not valid\") return mutation(value) What is Instructor? Instructor helps to ensure you get the exact response type you're looking for when using openai's function call api. Once you've defined the Pydantic model for your desired response, Instructor handles all the complicated logic in-between - from the parsing/validation of the response to the automatic retries for invalid responses. This means that we can build in validators 'for free' and have a clear separation of concerns between the prompt and the code that calls openai. from openai import OpenAI import instructor # pip install instructor from pydantic import BaseModel # This enables response\\\\\\_model keyword # from client.chat.completions.create client = instructor.patch(OpenAI()) To simplify your work with OpenAI models and streamline the extraction of Pydantic objects from prompts, we offer a patching mechanism for the ChatCompletion class. class UserDetail(BaseModel): name: str age: int user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] max\\\\\\_retries=3 Invalid responses that fail to be validated succesfully will trigger up to as many reattempts as you define. ) assert user.name == \"Jason\" As long as you pass in a response\\\\\\_model parameter to the ChatCompletion api call, the returned object will always be a validated Pydantic object. assert user.age == 25 In this post, we'll explore how to evolve from static, rule-based validation methods to dynamic, machine learning-driven ones. You'll learn to use Pydantic and Instructor to leverage language models and dive into advanced topics like content moderation, validating chain of thought reasoning, and contextual validation. Let's examine how these approaches with a example. Imagine that you run a software company who wants to ensure you never serve hateful and racist content. This isn't an easy job since the language around these topics change very quickly and frequently. Software 1.0: Introduction to Validations in Pydantic A simple method could be to compile a list of different words that are often associated with hate speech. For simplicity, let's assume that we've found that the words Steal and Rob are good predictors of hateful speech from our database. We can modify our validation structure above to accomodate this. This will throw an error if we pass in a string like Let's rob the bank! or We should steal from the supermarkets. Pydantic offers two approaches for this validation: using the field\\\\\\_validator decorator or the Annotated hints. Using field\\\\\\_validator decorator We can use the field\\\\\\_validator decorator to define a validator for a field in Pydantic. Here's a quick example of how we might be able to do so. from pydantic import BaseModel, ValidationError, field\\\\\\_validator from pydantic.fields import Field class UserMessage(BaseModel): message: str @field\\\\\\_validator('message') def message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words(cls, v: str) -> str: for word in v.split(): We split the sentence into its individual words and iterate through each of the words. We then try to see if any of these words are in our blacklist which in this case is just rob and steal if word.lower() in {'rob','steal'}: raise ValueError(f\"\\\\\\`{word}\\\\\\` was found in the message \\\\\\`{v}\\\\\\`\") return v try: UserMessage(message=\"This is a lovely day\") UserMessage(message=\"We should go and rob a bank\") except ValidationError as e: print(e) Since the message This is a lovely day does not have any blacklisted words, no errors are thrown. However, in the given example above, the validation fails for the message We should go and rob a bank due to the presence of the word rob and the corresponding error message is displayed. 1 validation error for UserMessage message Value error, \\\\\\`rob\\\\\\` was found in the message \\\\\\`We should go and rob a bank\\\\\\` \\\\\\[type=value\\\\\\_error, input\\\\\\_value='We should go and rob a bank', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Using Annotated Alternatively, you can use the Annotated function to perform the same validation. Here's an example where we utilise the same function we started with. from pydantic import BaseModel, ValidationError from typing import Annotated from pydantic.functional\\\\\\_validators import AfterValidator def message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words(value:str): for word in value.split(): if word.lower() in {'rob','steal'}: raise ValueError(f\"\\\\\\`{word}\\\\\\` was found in the message \\\\\\`{value}\\\\\\`\") return value class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words)\\\\\\] try: UserMessage(message=\"This is a lovely day\") UserMessage(message=\"We should go and rob a bank\") except ValidationError as e: print(e) This code snippet achieves the same validation result. If the user message contains any of the words in the blacklist, a ValueError is raised and the corresponding error message is displayed. 1 validation error for UserMessage message Value error, \\\\\\`rob\\\\\\` was found in the message \\\\\\`We should go and rob a bank\\\\\\` \\\\\\[type=value\\\\\\_error, input\\\\\\_value='We should go and rob a bank', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Validation is a fundamental concept in software development and remains the same when applied to AI systems. Existing programming concepts should be leveraged when possible instead of introducing new terms and standards. The underlying principles of validation remain unchanged. Suppose now that we've gotten a new message - Violence is always acceptable, as long as we silence the witness. Our original validator wouldn't throw any errors when passed this new message since it uses neither the words rob or steal. However, it's clear that it is not a message which should be published. How can we ensure that our validation logic can adapt to new challenges? Software 3.0: Validation for LLMs or powered by LLMs Building upon the understanding of simple field validators, let's delve into probabilistic validation in software 3.0, (prompt engineering). We'll introduce an LLM-powered validator called llm\\\\\\_validator that uses a statement to verify the value. We can get around this by using the inbuilt llm\\\\\\_validator class from Instructor. from instructor import llm\\\\\\_validator from pydantic import BaseModel, ValidationError from typing import Annotated from pydantic.functional\\\\\\_validators import AfterValidator class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(llm\\\\\\_validator(\"don't say objectionable things\"))\\\\\\] try: UserMessage(message=\"Violence is always acceptable, as long as we silence the witness\") except ValidationError as e: print(e) This produces the following error message as seen below 1 validation error for UserMessage message Assertion failed, The statement promotes violence, which is objectionable. \\\\\\[type=assertion\\\\\\_error, input\\\\\\_value='Violence is always accep... we silence the witness', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/assertion\\\\\\_error The error message is generated by the language model (LLM) rather than the code itself, making it helpful for re-asking the model in a later section. To better understand this approach, let's see how to build an llm\\\\\\_validator from scratch. Creating Your Own Field Level llm\\\\\\_validator Building your own llm\\\\\\_validator can be a valuable exercise to get started with Instructor and create custom validators. Before we continue, let's review the anatomy of a validator: def validation\\\\\\_function(value): if condition(value): raise ValueError(\"Value is not valid\") return value As we can see, a validator is simply a function that takes in a value and returns a value. If the value is not valid, it raises a ValueError. We can represent this using the following structure: class Validation(BaseModel): is\\\\\\_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\") error\\\\\\_message: Optional\\\\\\[str\\\\\\] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\") Using this structure, we can implement the same logic as before and utilize Instructor to generate the validation. import instructor from openai import OpenAI # Enables \\\\\\`response\\\\\\_model\\\\\\` and \\\\\\`max\\\\\\_retries\\\\\\` parameters client = instructor.patch(OpenAI()) def validator(v): statement = \"don't say objectionable things\" resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Does \\\\\\`{v}\\\\\\` follow the rules: {statement}\", }, \\\\\\], # this comes from client = instructor.patch(OpenAI()) response\\\\\\_model=Validation, The new parameter of response\\\\\\_model comes from client = instructor.patch(OpenAI()) and does not exist in the original OpenAI SDK. This allows us to pass in the Pydantic model that we want as a response. ) if not resp.is\\\\\\_valid: raise ValueError(resp.error\\\\\\_message) return v Now we can use this validator in the same way we used the llm\\\\\\_validator from Instructor. class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(validator)\\\\\\] Writing more complex validations Validating Chain of Thought A popular way of prompting large language models nowadays is known as chain of thought. This involves getting a model to generate reasons and explanations for an answer to a prompt. We can utilise Pydantic and Instructor to perform a validation to check of the reasoning is reasonable, given both the answer and the chain of thought. To do this we can't build a field validator since we need to access multiple fields in the model. Instead we can use a model validator. def validate\\\\\\_chain\\\\\\_of\\\\\\_thought(values): chain\\\\\\_of\\\\\\_thought = values\\\\\\[\"chain\\\\\\_of\\\\\\_thought\"\\\\\\] answer = values\\\\\\[\"answer\"\\\\\\] resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Verify that \\\\\\`{answer}\\\\\\` follows the chain of thought: {chain\\\\\\_of\\\\\\_thought}\", }, \\\\\\], # this comes from client = instructor.patch(OpenAI()) response\\\\\\_model=Validation, ) if not resp.is\\\\\\_valid: raise ValueError(resp.error\\\\\\_message) return values We can then take advantage of the model\\\\\\_validator decorator to perform a validation on a subset of the model's data. We're defining a model validator here which runs before Pydantic parses the input into its respective fields. That's why we have a before keyword used in the model\\\\\\_validator class. from pydantic import BaseModel, model\\\\\\_validator class AIResponse(BaseModel): chain\\\\\\_of\\\\\\_thought: str answer: str @model\\\\\\_validator(mode='before') @classmethod def chain\\\\\\_of\\\\\\_thought\\\\\\_makes\\\\\\_sense(cls, data: Any) -> Any: # here we assume data is the dict representation of the model # since we use 'before' mode. return validate\\\\\\_chain\\\\\\_of\\\\\\_thought(data) Now, when you create a AIResponse instance, the chain\\\\\\_of\\\\\\_thought\\\\\\_makes\\\\\\_sense validator will be invoked. Here's an example: try: resp = AIResponse( chain\\\\\\_of\\\\\\_thought=\"1 + 1 = 2\", answer=\"The meaning of life is 42\" ) except ValidationError as e: print(e) If we create a AIResponse instance with an answer that does not follow the chain of thought, we will get an error. 1 validation error for AIResponse Value error, The statement 'The meaning of life is 42' does not follow the chain of thought: 1 + 1 = 2. \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'chain\\\\\\_of\\\\\\_thought': '1 +... meaning of life is 42'}, input\\\\\\_type=dict\\\\\\] Validating Citations From Original Text Let's see a more concrete example. Let's say that we've asked our model a question about some text source and we want to validate that the generated answer is supported by the source. This would allow us to minimize hallucinations and prevent statements that are not backed by the original text. While we could verify this by looking up the original source manually, a more scalable approach is to use a validator to do this automatically. We can pass in additional context to our validation functions using the model\\\\\\_validate function in Pydantic so that our models have more information to work with when performing validation. This context is a normal python dictionary and can be accessed inside the info argument in our validator functions. from pydantic import ValidationInfo,BaseModel,field\\\\\\_validator class AnswerWithCitation(BaseModel): answer: str citation: str @field\\\\\\_validator('citation') @classmethod def citation\\\\\\_exists(cls, v: str, info: ValidationInfo): This info object corresponds to the value of context that we pass into the model\\\\\\_validate function as seen below. context = info.context if context: context = context.get('text\\\\\\_chunk') if v not in context: raise ValueError(f\"Citation \\\\\\`{v}\\\\\\` not found in text chunks\") return v We can then take our original example and test it against our new model try: AnswerWithCitation.model\\\\\\_validate( {\"answer\": \"Jason is a cool guy\", \"citation\": \"Jason is cool\"}, context={\"text\\\\\\_chunk\": \"Jason is just a guy\"}, This context object is just a normal python dictionary and can take in and store any arbitrary values ) except ValidationError as e: print(e) This in turn generates the following error since Jason is cool does not exist in the text Jason is just a guy. 1 validation error for AnswerWithCitation citation Value error, Citation \\\\\\`Jason is cool\\\\\\` not found in text chunks \\\\\\[type=value\\\\\\_error, input\\\\\\_value='Jason is cool', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Putting it all together with client = instructor.patch(OpenAI()) To pass this context from the client.chat.completions.create call, client = instructor.patch(OpenAI()) also passes the validation\\\\\\_context, which will be accessible from the info argument in the decorated validator functions. from openai import OpenAI import instructor # Enables \\\\\\`response\\\\\\_model\\\\\\` and \\\\\\`max\\\\\\_retries\\\\\\` parameters client = instructor.patch(OpenAI()) def answer\\\\\\_question(question:str, text\\\\\\_chunk: str) -> AnswerWithCitation: return client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Answer the question: {question} with the text chunk: {text\\\\\\_chunk}\", }, \\\\\\], response\\\\\\_model=AnswerWithCitation, validation\\\\\\_context={\"text\\\\\\_chunk\": text\\\\\\_chunk}, ) Error Handling and Re-Asking Validators can ensure certain properties of the outputs by throwing errors, in an AI system we can use the errors and allow language model to self correct. The by running client = instructor.patch(OpenAI()) not only do we add response\\\\\\_model and validation\\\\\\_context it also allows you to use the max\\\\\\_retries parameter to specify the number of times to try and self correct. This approach provides a layer of defense against two types of bad outputs: Pydantic Validation Errors (code or LLM-based) JSON Decoding Errors (when the model returns an incorrect response) Define the Response Model with Validators To keep things simple lets assume we have a model that returns a UserModel object. We can define the response model using Pydantic and add a field validator to ensure that the name is in uppercase. from pydantic import BaseModel, field\\\\\\_validator class UserModel(BaseModel): name: str age: int @field\\\\\\_validator(\"name\") @classmethod def validate\\\\\\_name(cls, v): if v.upper() != v: raise ValueError(\"Name must be in uppercase.\") return v This is where the max\\\\\\_retries parameter comes in. It allows the model to self correct and retry the prompt using the error message rather than the prompt. model = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"}, \\\\\\], # Powered by client = instructor.patch(OpenAI()) response\\\\\\_model=UserModel, max\\\\\\_retries=2, ) assert model.name == \"JASON\" In this example, even though there is no code explicitly transforming the name to uppercase, the model is able to correct the output. Conclusion From the simplicity of Pydantic and Instructor to the dynamic validation capabilities of LLMs, the landscape of validation is changing but without needing to introduce new contepts. It's clear that the future of validation is not just about preventing bad data but about allowing llms to understand the data and correcting it. If you enjoy the content or want to try out Instructor please check out the github and give us a star! Continue reading 2023/10/17 4 min read Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation Introduction Get ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the instructor.instructions streamlines this process, making the task you want to distil more efficient and powerful while preserving its original functionality and backwards compatibility. If you want to see the full example checkout examples/distillation Why use Instructor? Imagine you're developing a backend service that uses a mix old and new school ML practises, it may involve pipelines with multiple function calls, validations, and data processing. Sounds cumbersome, right? That's where Instructor comes in. It simplifies complex procedures, making them more efficient and easier to manage by adding a decorator to your function that will automatically generate a dataset for fine-tuning and help you swap out the function implementation. Quick Start: How to Use Instructor's Distillation Feature Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file. import logging import random from pydantic import BaseModel from instructor import Instructions # pip install instructor # Logging setup logging.basicConfig(level=logging.INFO) instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"math\\\\\\_finetunes.jsonl\")\\\\\\] ) class Multiply(BaseModel): a: int b: int result: int # Define a function with distillation # The decorator will automatically generate a dataset for fine-tuning # They must return a pydantic model to leverage function calling @instructions.distil def fn(a: int, b: int) -> Multiply: resp = a \\\\\\* b return Multiply(a=a, b=b, result=resp) # Generate some data for \\\\\\_ in range(10): a = random.randint(100, 999) b = random.randint(100, 999) print(fn(a, b)) The Intricacies of Fine-tuning Language Models Fine-tuning isn't just about writing a function like def f(a, b): return a \\\\\\* b. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this. Why Instructor and Distillation are Game Changers The library offers two main benefits: Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code. Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions. Role of Instructor in Simplifying Fine-Tuning The from instructor import Instructions feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior. Logging Output and Running a Finetune Here's how the logging output would look: { \"messages\": \\\\\\[ {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'}, {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'}, {\"role\": \"assistant\", \"function\\\\\\_call\": { \"name\": \"Multiply\", \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}' } } \\\\\\], \"functions\": \\\\\\[ {\"name\": \"Multiply\", \"description\": \"Correctly extracted \\\\\\`Multiply\\\\\\`...\"} \\\\\\] } Run a finetune like this: Don't forget to set your OpenAI Key as an environment variable All of the instructor jobs commands assume you've set an environment variable of OPENAI\\\\\\_API\\\\\\_KEY in your shell. You can set this by running the command export OPENAI\\\\\\_API\\\\\\_KEY= in your shell instructor jobs create-from-file math\\\\\\_finetunes.jsonl Next Steps and Future Plans Here's a sneak peek of what I'm planning: from instructor import Instructions, patch patch() Don't forget to run the patch() command that we provide with the Instructor package. This helps automatically serialize the content back into the \\\\\\`Pydantic\\\\\\`\\\\\\` model that we're looking for. class Multiply(BaseModel): a: int b: int result: int instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", ) @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard def fn(a: int, b: int) -> Multiply: resp = a + b return Multiply(a=a, b=b, result=resp) With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation. Conclusion We've seen how Instructor can make your life easier, from fine-tuning to distillation. Now if you're thinking wow, I'd love a backend service to do this for continously, you're in luck! Please check out the survey at useinstructor.com and let us know who you are. If you enjoy the content or want to try out instructor please check out the github and give us a star! Continue reading 2023/09/17 7 min read RAG is more than just embedding search With the advent of large language models (LLM), retrival augmented generation (RAG) has become a hot topic. However throught the past year of helping startups integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware. What is RAG? Retrival augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized. Simple RAG that embedded the user query and makes a search. So let's kick things off by examining what I like to call the 'Dumb' RAG Model—a basic setup that's more common than you'd think. The 'Dumb' RAG Model When you ask a question like, \"what is the capital of France?\" The RAG 'dumb' model embeds the query and searches in some unopinonated search endpoint. Limited to a single method API like search(query: str) -> List\\\\\\[str\\\\\\]. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings. Why is this a problem? Query-Document Mismatch: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation! Monolithic Search Backend: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more. Limitation of text search: Restricts complex queries to a single string ({query: str}), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, asking what problems did we fix last week cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week. Limited ability to plan: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context its able to plan a suite of queries to execute to return the best results. Now let's dive into how we can make it smarter with query understanding. This is where things get interesting. Improving the RAG Model with Query Understanding Shoutouts Much of this work has been inspired by / done in collab with a few of my clients at new.computer, Metaphor Systems, and Naro, go check them out! Ultimately what you want to deploy is a system that understands how to take the query and rewrite it to improve precision and recall. Query Understanding system routes to multiple search backends. Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems. Whats instructor? Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling API. Widespread Adoption: Pydantic is a popular tool among Python developers. Simplicity: Pydantic allows model definition in Python. Framework Compatibility: Many Python frameworks already use Pydantic. Case Study 1: Metaphor Systems Take Metaphor Systems, which turns natural language queries into their custom search-optimized query. If you take a look web UI you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using a language model, and turns it into a fully specified metaphor systems query. Metaphor Systems UI If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. It's actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library class DateRange(BaseModel): start: datetime.date end: datetime.date class MetaphorQuery(BaseModel): rewritten\\\\\\_query: str published\\\\\\_daterange: DateRange domains\\\\\\_allow\\\\\\_list: List\\\\\\[str\\\\\\] async def execute(): return await metaphor.search(...) Note how we model a rewritten query, range of published dates, and a list of domains to search in. This powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works. import instructor from openai import OpenAI # Enables response\\\\\\_model in the openai client client = instructor.patch(OpenAI()) query = client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=MetaphorQuery, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\" }, { \"role\": \"user\", \"content\": \"What are some recent developments in AI?\" } \\\\\\], ) Example Output { \"rewritten\\\\\\_query\": \"novel developments advancements ai artificial intelligence machine learning\", \"published\\\\\\_daterange\": { \"start\": \"2023-09-17\", \"end\": \"2021-06-17\" }, \"domains\\\\\\_allow\\\\\\_list\": \\\\\\[\"arxiv.org\"\\\\\\] } This isn't just about adding some date ranges. It's about nuanced, tailored searches, that are deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features. class DateRange(BaseModel): start: datetime.date end: datetime.date chain\\\\\\_of\\\\\\_thought: str = Field( None, description=\"Think step by step to plan what is the best time range to search in\" ) Now, let's see how this approach can help model an agent like personal assistant. Case Study 2: Personal Assistant Another great example of this multiple dispatch pattern is a personal assistant. You might ask, \"What do I have today?\", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts. class ClientSource(enum.Enum): GMAIL = \"gmail\" CALENDAR = \"calendar\" class SearchClient(BaseModel): query: str keywords: List\\\\\\[str\\\\\\] email: str source: ClientSource start\\\\\\_date: datetime.date end\\\\\\_date: datetime.date async def execute(self) -> str: if self.source == ClientSource.GMAIL: ... elif self.source == ClientSource.CALENDAR: ... class Retrival(BaseModel): queries: List\\\\\\[SearchClient\\\\\\] async def execute(self) -> str: return await asyncio.gather(\\\\\\*\\\\\\[query.execute() for query in self.queries\\\\\\]) Now we can call this with a simple query like \"What do I have today?\" and it will try to async dispatch to the correct backend. It's still important to prompt the language model well, but we'll leave that for another day. import instructor from openai import OpenAI # Enables response\\\\\\_model in the openai client client = instructor.patch(OpenAI()) retrival = client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=Retrival, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"}, {\"role\": \"user\", \"content\": \"What do I have today?\"} \\\\\\], ) Example Output { \"queries\": \\\\\\[ { \"query\": None, \"keywords\": None, \"email\": \"jason@example.com\", \"source\": \"gmail\", \"start\\\\\\_date\": \"2023-09-17\", \"end\\\\\\_date\": None }, { \"query\": None, \"keywords\": \\\\\\[\"meeting\", \"call\", \"zoom\"\\\\\\]\\\\\\]\\\\\\], \"email\": \"jason@example.com\", \"source\": \"calendar\", \"start\\\\\\_date\": \"2023-09-17\", \"end\\\\\\_date\": None } \\\\\\] } Notice that we have a list of queries that route to different search backends (email and calendar). We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well. Perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app. Can I used framework X? I get this question a lot, but it's just code. Within these dispatchs you can do whatever you want. You can use input() to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information. The sky is the limit. Both of these examples showcase how both search providors and consumers can use instructor to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend. Conclusion This isnt about fancy embedding tricks, it's just plain old information retrival and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place. What's Next? Here I want to show that \\\\\\`instructor\\\\\\`\\\\\\` isn’t just about data extraction. It’s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning — the untapped goldmine is skilled use of tools and APIs. If you enjoy the content or want to try out instructor please check out the github and give us a star! Continue reading 1 2 Back to top Previous Welcome to the Instructor Blog Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "RAG is more than just embedding search - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/?q=",
    "html": "Skip to content Instructor RAG is more than just embedding search Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents The 'Dumb' RAG Model Why is this a problem? Improving the RAG Model with Query Understanding Whats instructor? Case Study 1: Metaphor Systems Case Study 2: Personal Assistant Conclusion What's Next? Back to index Jason Liu Creator Metadata 2023/09/17 7 min read RAG is more than just embedding search¶ With the advent of large language models (LLM), retrival augmented generation (RAG) has become a hot topic. However throught the past year of helping startups integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware. What is RAG? Retrival augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized. Simple RAG that embedded the user query and makes a search. So let's kick things off by examining what I like to call the 'Dumb' RAG Model—a basic setup that's more common than you'd think. The 'Dumb' RAG Model¶ When you ask a question like, \"what is the capital of France?\" The RAG 'dumb' model embeds the query and searches in some unopinonated search endpoint. Limited to a single method API like search(query: str) -> List\\\\\\[str\\\\\\]. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings. Why is this a problem?¶ Query-Document Mismatch: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation! Monolithic Search Backend: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more. Limitation of text search: Restricts complex queries to a single string ({query: str}), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, asking what problems did we fix last week cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week. Limited ability to plan: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context its able to plan a suite of queries to execute to return the best results. Now let's dive into how we can make it smarter with query understanding. This is where things get interesting. Improving the RAG Model with Query Understanding¶ Shoutouts Much of this work has been inspired by / done in collab with a few of my clients at new.computer, Metaphor Systems, and Naro, go check them out! Ultimately what you want to deploy is a system that understands how to take the query and rewrite it to improve precision and recall. Query Understanding system routes to multiple search backends. Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems. Whats instructor?¶ Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling API. Widespread Adoption: Pydantic is a popular tool among Python developers. Simplicity: Pydantic allows model definition in Python. Framework Compatibility: Many Python frameworks already use Pydantic. Case Study 1: Metaphor Systems¶ Take Metaphor Systems, which turns natural language queries into their custom search-optimized query. If you take a look web UI you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using a language model, and turns it into a fully specified metaphor systems query. Metaphor Systems UI If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. It's actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library class DateRange(BaseModel): start: datetime.date end: datetime.date class MetaphorQuery(BaseModel): rewritten\\\\\\_query: str published\\\\\\_daterange: DateRange domains\\\\\\_allow\\\\\\_list: List\\\\\\[str\\\\\\] async def execute(): return await metaphor.search(...) Note how we model a rewritten query, range of published dates, and a list of domains to search in. This powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works. import instructor from openai import OpenAI # Enables response\\\\\\_model in the openai client client = instructor.patch(OpenAI()) query = client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=MetaphorQuery, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\" }, { \"role\": \"user\", \"content\": \"What are some recent developments in AI?\" } \\\\\\], ) Example Output { \"rewritten\\\\\\_query\": \"novel developments advancements ai artificial intelligence machine learning\", \"published\\\\\\_daterange\": { \"start\": \"2023-09-17\", \"end\": \"2021-06-17\" }, \"domains\\\\\\_allow\\\\\\_list\": \\\\\\[\"arxiv.org\"\\\\\\] } This isn't just about adding some date ranges. It's about nuanced, tailored searches, that are deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features. class DateRange(BaseModel): start: datetime.date end: datetime.date chain\\\\\\_of\\\\\\_thought: str = Field( None, description=\"Think step by step to plan what is the best time range to search in\" ) Now, let's see how this approach can help model an agent like personal assistant. Case Study 2: Personal Assistant¶ Another great example of this multiple dispatch pattern is a personal assistant. You might ask, \"What do I have today?\", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts. class ClientSource(enum.Enum): GMAIL = \"gmail\" CALENDAR = \"calendar\" class SearchClient(BaseModel): query: str keywords: List\\\\\\[str\\\\\\] email: str source: ClientSource start\\\\\\_date: datetime.date end\\\\\\_date: datetime.date async def execute(self) -> str: if self.source == ClientSource.GMAIL: ... elif self.source == ClientSource.CALENDAR: ... class Retrival(BaseModel): queries: List\\\\\\[SearchClient\\\\\\] async def execute(self) -> str: return await asyncio.gather(\\\\\\*\\\\\\[query.execute() for query in self.queries\\\\\\]) Now we can call this with a simple query like \"What do I have today?\" and it will try to async dispatch to the correct backend. It's still important to prompt the language model well, but we'll leave that for another day. import instructor from openai import OpenAI # Enables response\\\\\\_model in the openai client client = instructor.patch(OpenAI()) retrival = client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=Retrival, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"}, {\"role\": \"user\", \"content\": \"What do I have today?\"} \\\\\\], ) Example Output { \"queries\": \\\\\\[ { \"query\": None, \"keywords\": None, \"email\": \"jason@example.com\", \"source\": \"gmail\", \"start\\\\\\_date\": \"2023-09-17\", \"end\\\\\\_date\": None }, { \"query\": None, \"keywords\": \\\\\\[\"meeting\", \"call\", \"zoom\"\\\\\\]\\\\\\]\\\\\\], \"email\": \"jason@example.com\", \"source\": \"calendar\", \"start\\\\\\_date\": \"2023-09-17\", \"end\\\\\\_date\": None } \\\\\\] } Notice that we have a list of queries that route to different search backends (email and calendar). We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well. Perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app. Can I used framework X? I get this question a lot, but it's just code. Within these dispatchs you can do whatever you want. You can use input() to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information. The sky is the limit. Both of these examples showcase how both search providors and consumers can use instructor to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend. Conclusion¶ This isnt about fancy embedding tricks, it's just plain old information retrival and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place. What's Next?¶ Here I want to show that \\\\\\`instructor\\\\\\`\\\\\\` isn’t just about data extraction. It’s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning — the untapped goldmine is skilled use of tools and APIs. If you enjoy the content or want to try out instructor please check out the github and give us a star! Was this page helpful? Back to top Previous Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls Next Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Good LLM Validation is Just Good Validation - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/10/23/good-llm-validation-is-just-good-validation/?q=",
    "html": "Skip to content Instructor Good LLM Validation is Just Good Validation Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents What is Instructor? Software 1.0: Introduction to Validations in Pydantic Using field\\\\\\_validator decorator Using Annotated Software 3.0: Validation for LLMs or powered by LLMs Creating Your Own Field Level llm\\\\\\_validator Writing more complex validations Validating Chain of Thought Validating Citations From Original Text Putting it all together with client = instructor.patch(OpenAI()) Error Handling and Re-Asking Define the Response Model with Validators Conclusion Back to index Jason Liu Creator Ivan Leo Contributor Metadata 2023/10/23 10 min read Good LLM Validation is Just Good Validation¶ What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here. Validation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like Pydantic and Instructor. We validate these outputs using a validation function which conforms to the structure seen below. def validation\\\\\\_function(value): if condition(value): raise ValueError(\"Value is not valid\") return mutation(value) What is Instructor?¶ Instructor helps to ensure you get the exact response type you're looking for when using openai's function call api. Once you've defined the Pydantic model for your desired response, Instructor handles all the complicated logic in-between - from the parsing/validation of the response to the automatic retries for invalid responses. This means that we can build in validators 'for free' and have a clear separation of concerns between the prompt and the code that calls openai. from openai import OpenAI import instructor # pip install instructor from pydantic import BaseModel # This enables response\\\\\\_model keyword # from client.chat.completions.create client = instructor.patch(OpenAI()) To simplify your work with OpenAI models and streamline the extraction of Pydantic objects from prompts, we offer a patching mechanism for the ChatCompletion class. class UserDetail(BaseModel): name: str age: int user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] max\\\\\\_retries=3 Invalid responses that fail to be validated succesfully will trigger up to as many reattempts as you define. ) assert user.name == \"Jason\" As long as you pass in a response\\\\\\_model parameter to the ChatCompletion api call, the returned object will always be a validated Pydantic object. assert user.age == 25 In this post, we'll explore how to evolve from static, rule-based validation methods to dynamic, machine learning-driven ones. You'll learn to use Pydantic and Instructor to leverage language models and dive into advanced topics like content moderation, validating chain of thought reasoning, and contextual validation. Let's examine how these approaches with a example. Imagine that you run a software company who wants to ensure you never serve hateful and racist content. This isn't an easy job since the language around these topics change very quickly and frequently. Software 1.0: Introduction to Validations in Pydantic¶ A simple method could be to compile a list of different words that are often associated with hate speech. For simplicity, let's assume that we've found that the words Steal and Rob are good predictors of hateful speech from our database. We can modify our validation structure above to accomodate this. This will throw an error if we pass in a string like Let's rob the bank! or We should steal from the supermarkets. Pydantic offers two approaches for this validation: using the field\\\\\\_validator decorator or the Annotated hints. Using field\\\\\\_validator decorator¶ We can use the field\\\\\\_validator decorator to define a validator for a field in Pydantic. Here's a quick example of how we might be able to do so. from pydantic import BaseModel, ValidationError, field\\\\\\_validator from pydantic.fields import Field class UserMessage(BaseModel): message: str @field\\\\\\_validator('message') def message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words(cls, v: str) -> str: for word in v.split(): We split the sentence into its individual words and iterate through each of the words. We then try to see if any of these words are in our blacklist which in this case is just rob and steal if word.lower() in {'rob','steal'}: raise ValueError(f\"\\\\\\`{word}\\\\\\` was found in the message \\\\\\`{v}\\\\\\`\") return v try: UserMessage(message=\"This is a lovely day\") UserMessage(message=\"We should go and rob a bank\") except ValidationError as e: print(e) Since the message This is a lovely day does not have any blacklisted words, no errors are thrown. However, in the given example above, the validation fails for the message We should go and rob a bank due to the presence of the word rob and the corresponding error message is displayed. 1 validation error for UserMessage message Value error, \\\\\\`rob\\\\\\` was found in the message \\\\\\`We should go and rob a bank\\\\\\` \\\\\\[type=value\\\\\\_error, input\\\\\\_value='We should go and rob a bank', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Using Annotated¶ Alternatively, you can use the Annotated function to perform the same validation. Here's an example where we utilise the same function we started with. from pydantic import BaseModel, ValidationError from typing import Annotated from pydantic.functional\\\\\\_validators import AfterValidator def message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words(value:str): for word in value.split(): if word.lower() in {'rob','steal'}: raise ValueError(f\"\\\\\\`{word}\\\\\\` was found in the message \\\\\\`{value}\\\\\\`\") return value class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words)\\\\\\] try: UserMessage(message=\"This is a lovely day\") UserMessage(message=\"We should go and rob a bank\") except ValidationError as e: print(e) This code snippet achieves the same validation result. If the user message contains any of the words in the blacklist, a ValueError is raised and the corresponding error message is displayed. 1 validation error for UserMessage message Value error, \\\\\\`rob\\\\\\` was found in the message \\\\\\`We should go and rob a bank\\\\\\` \\\\\\[type=value\\\\\\_error, input\\\\\\_value='We should go and rob a bank', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Validation is a fundamental concept in software development and remains the same when applied to AI systems. Existing programming concepts should be leveraged when possible instead of introducing new terms and standards. The underlying principles of validation remain unchanged. Suppose now that we've gotten a new message - Violence is always acceptable, as long as we silence the witness. Our original validator wouldn't throw any errors when passed this new message since it uses neither the words rob or steal. However, it's clear that it is not a message which should be published. How can we ensure that our validation logic can adapt to new challenges? Software 3.0: Validation for LLMs or powered by LLMs¶ Building upon the understanding of simple field validators, let's delve into probabilistic validation in software 3.0, (prompt engineering). We'll introduce an LLM-powered validator called llm\\\\\\_validator that uses a statement to verify the value. We can get around this by using the inbuilt llm\\\\\\_validator class from Instructor. from instructor import llm\\\\\\_validator from pydantic import BaseModel, ValidationError from typing import Annotated from pydantic.functional\\\\\\_validators import AfterValidator class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(llm\\\\\\_validator(\"don't say objectionable things\"))\\\\\\] try: UserMessage(message=\"Violence is always acceptable, as long as we silence the witness\") except ValidationError as e: print(e) This produces the following error message as seen below 1 validation error for UserMessage message Assertion failed, The statement promotes violence, which is objectionable. \\\\\\[type=assertion\\\\\\_error, input\\\\\\_value='Violence is always accep... we silence the witness', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/assertion\\\\\\_error The error message is generated by the language model (LLM) rather than the code itself, making it helpful for re-asking the model in a later section. To better understand this approach, let's see how to build an llm\\\\\\_validator from scratch. Creating Your Own Field Level llm\\\\\\_validator¶ Building your own llm\\\\\\_validator can be a valuable exercise to get started with Instructor and create custom validators. Before we continue, let's review the anatomy of a validator: def validation\\\\\\_function(value): if condition(value): raise ValueError(\"Value is not valid\") return value As we can see, a validator is simply a function that takes in a value and returns a value. If the value is not valid, it raises a ValueError. We can represent this using the following structure: class Validation(BaseModel): is\\\\\\_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\") error\\\\\\_message: Optional\\\\\\[str\\\\\\] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\") Using this structure, we can implement the same logic as before and utilize Instructor to generate the validation. import instructor from openai import OpenAI # Enables \\\\\\`response\\\\\\_model\\\\\\` and \\\\\\`max\\\\\\_retries\\\\\\` parameters client = instructor.patch(OpenAI()) def validator(v): statement = \"don't say objectionable things\" resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Does \\\\\\`{v}\\\\\\` follow the rules: {statement}\", }, \\\\\\], # this comes from client = instructor.patch(OpenAI()) response\\\\\\_model=Validation, The new parameter of response\\\\\\_model comes from client = instructor.patch(OpenAI()) and does not exist in the original OpenAI SDK. This allows us to pass in the Pydantic model that we want as a response. ) if not resp.is\\\\\\_valid: raise ValueError(resp.error\\\\\\_message) return v Now we can use this validator in the same way we used the llm\\\\\\_validator from Instructor. class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(validator)\\\\\\] Writing more complex validations¶ Validating Chain of Thought¶ A popular way of prompting large language models nowadays is known as chain of thought. This involves getting a model to generate reasons and explanations for an answer to a prompt. We can utilise Pydantic and Instructor to perform a validation to check of the reasoning is reasonable, given both the answer and the chain of thought. To do this we can't build a field validator since we need to access multiple fields in the model. Instead we can use a model validator. def validate\\\\\\_chain\\\\\\_of\\\\\\_thought(values): chain\\\\\\_of\\\\\\_thought = values\\\\\\[\"chain\\\\\\_of\\\\\\_thought\"\\\\\\] answer = values\\\\\\[\"answer\"\\\\\\] resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Verify that \\\\\\`{answer}\\\\\\` follows the chain of thought: {chain\\\\\\_of\\\\\\_thought}\", }, \\\\\\], # this comes from client = instructor.patch(OpenAI()) response\\\\\\_model=Validation, ) if not resp.is\\\\\\_valid: raise ValueError(resp.error\\\\\\_message) return values We can then take advantage of the model\\\\\\_validator decorator to perform a validation on a subset of the model's data. We're defining a model validator here which runs before Pydantic parses the input into its respective fields. That's why we have a before keyword used in the model\\\\\\_validator class. from pydantic import BaseModel, model\\\\\\_validator class AIResponse(BaseModel): chain\\\\\\_of\\\\\\_thought: str answer: str @model\\\\\\_validator(mode='before') @classmethod def chain\\\\\\_of\\\\\\_thought\\\\\\_makes\\\\\\_sense(cls, data: Any) -> Any: # here we assume data is the dict representation of the model # since we use 'before' mode. return validate\\\\\\_chain\\\\\\_of\\\\\\_thought(data) Now, when you create a AIResponse instance, the chain\\\\\\_of\\\\\\_thought\\\\\\_makes\\\\\\_sense validator will be invoked. Here's an example: try: resp = AIResponse( chain\\\\\\_of\\\\\\_thought=\"1 + 1 = 2\", answer=\"The meaning of life is 42\" ) except ValidationError as e: print(e) If we create a AIResponse instance with an answer that does not follow the chain of thought, we will get an error. 1 validation error for AIResponse Value error, The statement 'The meaning of life is 42' does not follow the chain of thought: 1 + 1 = 2. \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'chain\\\\\\_of\\\\\\_thought': '1 +... meaning of life is 42'}, input\\\\\\_type=dict\\\\\\] Validating Citations From Original Text¶ Let's see a more concrete example. Let's say that we've asked our model a question about some text source and we want to validate that the generated answer is supported by the source. This would allow us to minimize hallucinations and prevent statements that are not backed by the original text. While we could verify this by looking up the original source manually, a more scalable approach is to use a validator to do this automatically. We can pass in additional context to our validation functions using the model\\\\\\_validate function in Pydantic so that our models have more information to work with when performing validation. This context is a normal python dictionary and can be accessed inside the info argument in our validator functions. from pydantic import ValidationInfo,BaseModel,field\\\\\\_validator class AnswerWithCitation(BaseModel): answer: str citation: str @field\\\\\\_validator('citation') @classmethod def citation\\\\\\_exists(cls, v: str, info: ValidationInfo): This info object corresponds to the value of context that we pass into the model\\\\\\_validate function as seen below. context = info.context if context: context = context.get('text\\\\\\_chunk') if v not in context: raise ValueError(f\"Citation \\\\\\`{v}\\\\\\` not found in text chunks\") return v We can then take our original example and test it against our new model try: AnswerWithCitation.model\\\\\\_validate( {\"answer\": \"Jason is a cool guy\", \"citation\": \"Jason is cool\"}, context={\"text\\\\\\_chunk\": \"Jason is just a guy\"}, This context object is just a normal python dictionary and can take in and store any arbitrary values ) except ValidationError as e: print(e) This in turn generates the following error since Jason is cool does not exist in the text Jason is just a guy. 1 validation error for AnswerWithCitation citation Value error, Citation \\\\\\`Jason is cool\\\\\\` not found in text chunks \\\\\\[type=value\\\\\\_error, input\\\\\\_value='Jason is cool', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Putting it all together with client = instructor.patch(OpenAI())¶ To pass this context from the client.chat.completions.create call, client = instructor.patch(OpenAI()) also passes the validation\\\\\\_context, which will be accessible from the info argument in the decorated validator functions. from openai import OpenAI import instructor # Enables \\\\\\`response\\\\\\_model\\\\\\` and \\\\\\`max\\\\\\_retries\\\\\\` parameters client = instructor.patch(OpenAI()) def answer\\\\\\_question(question:str, text\\\\\\_chunk: str) -> AnswerWithCitation: return client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Answer the question: {question} with the text chunk: {text\\\\\\_chunk}\", }, \\\\\\], response\\\\\\_model=AnswerWithCitation, validation\\\\\\_context={\"text\\\\\\_chunk\": text\\\\\\_chunk}, ) Error Handling and Re-Asking¶ Validators can ensure certain properties of the outputs by throwing errors, in an AI system we can use the errors and allow language model to self correct. The by running client = instructor.patch(OpenAI()) not only do we add response\\\\\\_model and validation\\\\\\_context it also allows you to use the max\\\\\\_retries parameter to specify the number of times to try and self correct. This approach provides a layer of defense against two types of bad outputs: Pydantic Validation Errors (code or LLM-based) JSON Decoding Errors (when the model returns an incorrect response) Define the Response Model with Validators¶ To keep things simple lets assume we have a model that returns a UserModel object. We can define the response model using Pydantic and add a field validator to ensure that the name is in uppercase. from pydantic import BaseModel, field\\\\\\_validator class UserModel(BaseModel): name: str age: int @field\\\\\\_validator(\"name\") @classmethod def validate\\\\\\_name(cls, v): if v.upper() != v: raise ValueError(\"Name must be in uppercase.\") return v This is where the max\\\\\\_retries parameter comes in. It allows the model to self correct and retry the prompt using the error message rather than the prompt. model = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"}, \\\\\\], # Powered by client = instructor.patch(OpenAI()) response\\\\\\_model=UserModel, max\\\\\\_retries=2, ) assert model.name == \"JASON\" In this example, even though there is no code explicitly transforming the name to uppercase, the model is able to correct the output. Conclusion¶ From the simplicity of Pydantic and Instructor to the dynamic validation capabilities of LLMs, the landscape of validation is changing but without needing to introduce new contepts. It's clear that the future of validation is not just about preventing bad data but about allowing llms to understand the data and correcting it. If you enjoy the content or want to try out Instructor please check out the github and give us a star! Was this page helpful? Back to top Previous Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation Next AI Engineer Keynote: Pydantic is all you need Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "2023 - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/archive/2023/page/2/",
    "html": "Skip to content Instructor 2023 Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Blog Archive 2023 Table of contents Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls 2023¶ 2023/09/11 2 min read Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls Language models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic. The Problem with Existing LLM Frameworks Current frameworks for Language Learning Models (LLMs) have complex setups. Developers find it hard to control interactions with language models. Some frameworks require complex JSON Schema setups. The OpenAI Function Calling Game-Changer OpenAI's Function Calling feature provides a constrained interaction model. However, it has its own complexities, mostly around JSON Schema. Why Pydantic? Instructor uses Pydantic to simplify the interaction between the programmer and the language model. Widespread Adoption: Pydantic is a popular tool among Python developers. Simplicity: Pydantic allows model definition in Python. Framework Compatibility: Many Python frameworks already use Pydantic. import pydantic import instructor from openai import OpenAI # Enables the response\\\\\\_model client = instructor.patch(OpenAI()) class UserDetail(pydantic.BaseModel): name: str age: int def introduce(self): return f\"Hello I'm {self.name} and I'm {self.age} years old\" user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] ) Simplifying Validation Flow with Pydantic Pydantic validators simplify features like re-asking or self-critique. This makes these tasks less complex compared to other frameworks. from typing\\\\\\_extensions import Annotated from pydantic import BaseModel, BeforeValidator from instructor import llm\\\\\\_validator, patch from openai import OpenAI class QuestionAnswerNoEvil(BaseModel): question: str answer: Annotated\\\\\\[ str, BeforeValidator( llm\\\\\\_validator(\"don't say objectionable things\") ), \\\\\\] The Modular Approach Pydantic allows for modular output schemas. This leads to more organized code. Composition of Schemas class UserDetails(BaseModel): name: str age: int class UserWithAddress(UserDetails): address: str Defining Relationships class UserDetail(BaseModel): id: int age: int name: str friends: List\\\\\\[int\\\\\\] class UserRelationships(BaseModel): users: List\\\\\\[UserDetail\\\\\\] Using Enums from enum import Enum, auto class Role(Enum): PRINCIPAL = auto() TEACHER = auto() STUDENT = auto() OTHER = auto() class UserDetail(BaseModel): age: int name: str role: Role Flexible Schemas from typing import List class Property(BaseModel): key: str value: str class UserDetail(BaseModel): age: int name: str properties: List\\\\\\[Property\\\\\\] Chain of Thought class TimeRange(BaseModel): chain\\\\\\_of\\\\\\_thought: str start\\\\\\_time: int end\\\\\\_time: int class UserDetail(BaseModel): id: int age: int name: str work\\\\\\_time: TimeRange leisure\\\\\\_time: TimeRange Language Models as Microservices The architecture resembles FastAPI. Most code can be written as Python functions that use Pydantic objects. This eliminates the need for prompt chains. FastAPI Stub app = FastAPI() @app.get(\"/user/{user\\\\\\_id}\", response\\\\\\_model=UserDetails) async def get\\\\\\_user(user\\\\\\_id: int) -> UserDetails: return UserDetails(...) Using Instructor as a Function def extract\\\\\\_user(str) -> UserDetails: return client.chat.completions( response\\\\\\_model=UserDetails, messages=\\\\\\[...\\\\\\] ) Response Modeling class MaybeUser(BaseModel): result: Optional\\\\\\[UserDetail\\\\\\] error: bool message: Optional\\\\\\[str\\\\\\] Conclusion Instructor, with Pydantic, simplifies interaction with language models. It is usable for both experienced and new developers. If you enjoy the content or want to try out instructor please check out the github and give us a star! Continue reading 1 2 Back to top Previous Welcome to the Instructor Blog Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Finetuning GPT-3.5 - Instructor",
    "url": "https://jxnl.github.io/instructor/cli/finetune/?q=",
    "html": "Skip to content Instructor Finetuning GPT-3.5 Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog CLI Reference Finetuning GPT-3.5 Usage Tracking Table of contents Creating a Fine-Tuning Job View Jobs Options Create from File Usage Create from ID Usage Viewing Files and Jobs Viewing Jobs Viewing Files Using the Command Line Interface¶ The instructor CLI provides functionalities for managing fine-tuning jobs on OpenAI. Incomplete API The CLI is still under development and does not yet support all features of the API. If you would like to use a feature that is not yet supported, please consider using the contributing to our library jxnl/instructor instead. !!! note \"Low hanging fruit\" If you want to contribute we're looking for a few things: 1. Adding filenames on upload Creating a Fine-Tuning Job¶ View Jobs Options¶ $ instructor jobs --help Usage: instructor jobs \\\\\\[OPTIONS\\\\\\] COMMAND \\\\\\[ARGS\\\\\\]... Monitor and create fine tuning jobs ╭─ Options ───────────────────────────────────────────────────────────────────────────────╮ │ --help Display the help message. │ ╰─────────────────────────────────────────────────────────────────────────────────────────╯ ╭─ Commands ──────────────────────────────────────────────────────────────────────────────────────────────────╮ │ cancel Cancel a fine-tuning job. │ │ create-from-file Create a fine-tuning job from a file. │ │ create-from-id Create a fine-tuning job from an existing ID. │ │ list Monitor the status of the most recent fine-tuning jobs. │ ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ Create from File¶ The create-from-file command uploads and trains a model in a single step. ❯ instructor jobs create-from-file --help Usage: instructor jobs create-from-file \\\\\\[OPTIONS\\\\\\] FILE Create a fine-tuning job from a file. ╭─ Arguments ───────────────────────────────────────────────────────────────────────────────────────╮ │ \\\\\\* file TEXT Path to the file for fine-tuning \\\\\\[default: None\\\\\\] \\\\\\[required\\\\\\] │ ╰───────────────────────────────────────────────────────────────────────────────────────────────────╯ ╭─ Options ─────────────────────────────────────────────────────────────────────────────────────────╮ │ --model TEXT Model to use for fine-tuning \\\\\\[default: gpt-3.5-turbo\\\\\\] │ │ --poll INTEGER Polling interval in seconds \\\\\\[default: 2\\\\\\] │ │ --n-epochs INTEGER Number of epochs for fine-tuning │ │ --batch-size TEXT Batch size for fine-tuning │ │ --learning-rate-multiplier TEXT Learning rate multiplier for fine-tuning │ │ --validation-file TEXT Path to the validation file \\\\\\[default: None\\\\\\] │ │ --model-suffix TEXT Suffix to identify the model \\\\\\[default: None\\\\\\] │ │ --help Show this message and exit. │ ╰──────────────────────────────────────────────────────────────────────────────── Usage¶ $ instructor jobs create-from-file transformed\\\\\\_data.jsonl --validation\\\\\\_file validation\\\\\\_data.jsonl --n\\\\\\_epochs 3 --batch\\\\\\_size 16 --learning\\\\\\_rate\\\\\\_multiplier 0.5 Create from ID¶ The create-from-id command uses an uploaded file and trains a model ❯ instructor jobs create-from-id --help Usage: instructor jobs create-from-id \\\\\\[OPTIONS\\\\\\] ID Create a fine-tuning job from an existing ID. ╭─ Arguments ───────────────────────────────────────────────────────────────────────────╮ │ \\\\\\* id TEXT ID of the existing fine-tuning job \\\\\\[default: None\\\\\\] \\\\\\[required\\\\\\] │ ╰───────────────────────────────────────────────────────────────────────────────────────╯ ╭─ Options ─────────────────────────────────────────────────────────────────────────────╮ │ --model TEXT Model to use for fine-tuning │ │ \\\\\\[default: gpt-3.5-turbo\\\\\\] │ │ --n-epochs INTEGER Number of epochs for fine-tuning │ │ --batch-size TEXT Batch size for fine-tuning │ │ --learning-rate-multiplier TEXT Learning rate multiplier for fine-tuning │ │ --validation-file-id TEXT ID of the uploaded validation file │ │ \\\\\\[default: None\\\\\\] │ │ --help Show this message and exit. │ ╰───────────────────────────────────────────────────────────────────────────────────────╯ Usage¶ $ instructor files upload transformed\\\\\\_data.jsonl $ instructor files upload validation\\\\\\_data.jsonl $ instructor files list ... $ instructor jobs create\\\\\\_from\\\\\\_id \\\\--validation\\\\\\_file \\\\--n\\\\\\_epochs 3 --batch\\\\\\_size 16 --learning\\\\\\_rate\\\\\\_multiplier 0.5 Viewing Files and Jobs¶ Viewing Jobs¶ $ instructor jobs list OpenAI Fine Tuning Job Monitoring ┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃ Completion ┃ ┃ ┃ ┃ ┃ ┃ Job ID ┃ Status ┃ Creation Time ┃ Time ┃ Model Name ┃ File ID ┃ Epochs ┃ Base Model ┃ ┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━┩ │ ftjob-PWo6uwk… │ 🚫 cancelled │ 2023-08-23 │ N/A │ │ file-F7lJg6Z4… │ 3 │ gpt-3.5-turbo-… │ │ │ │ 23:10:54 │ │ │ │ │ │ │ ftjob-1whjva8… │ 🚫 cancelled │ 2023-08-23 │ N/A │ │ file-F7lJg6Z4… │ 3 │ gpt-3.5-turbo-… │ │ │ │ 22:47:05 │ │ │ │ │ │ │ ftjob-wGoBDld… │ 🚫 cancelled │ 2023-08-23 │ N/A │ │ file-F7lJg6Z4… │ 3 │ gpt-3.5-turbo-… │ │ │ │ 22:44:12 │ │ │ │ │ │ │ ftjob-yd5aRTc… │ ✅ succeeded │ 2023-08-23 │ 2023-08-23 │ ft:gpt-3.5-tur… │ file-IQxAUDqX… │ 3 │ gpt-3.5-turbo-… │ │ │ │ 14:26:03 │ 15:02:29 │ │ │ │ │ └────────────────┴──────────────┴────────────────┴────────────────┴─────────────────┴────────────────┴────────┴─────────────────┘ Automatically refreshes every 5 seconds, press Ctrl+C to exit Viewing Files¶ $ instructor files list OpenAI Files ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┓ ┃ File ID ┃ Size (bytes) ┃ Creation Time ┃ Filename ┃ Purpose ┃ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━┩ │ file-0lw2BSNRUlXZXRRu2beCCWjl │ 369523 │ 2023-08-23 23:31:57 │ file │ fine-tune │ │ file-IHaUXcMEykmFUp1kt2puCDEq │ 369523 │ 2023-08-23 23:09:35 │ file │ fine-tune │ │ file-ja9vRBf0FydEOTolaa3BMqES │ 369523 │ 2023-08-23 22:42:29 │ file │ fine-tune │ │ file-F7lJg6Z47CREvmx4kyvyZ6Sn │ 369523 │ 2023-08-23 22:42:03 │ file │ fine-tune │ │ file-YUxqZPyJRl5GJCUTw3cNmA46 │ 369523 │ 2023-08-23 22:29:10 │ file │ fine-tune │ └───────────────────────────────┴──────────────┴─────────────────────┴──────────┴───────────┘ Contributions¶ We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request. Was this page helpful? Back to top Previous Introduction Next Usage Tracking Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Usage Tracking - Instructor",
    "url": "https://jxnl.github.io/instructor/cli/usage/?q=",
    "html": "Skip to content Instructor Usage Tracking Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog CLI Reference Finetuning GPT-3.5 Usage Tracking Table of contents Monitoring API Usage View Usage Options List Usage for Specific Number of Days List Usage for Today Using the OpenAI API Usage CLI¶ The OpenAI API Usage CLI tool provides functionalities for monitoring your OpenAI API usage, breaking it down by model, date, and cost. Monitoring API Usage¶ View Usage Options¶ $ instructor usage --help Usage: instructor usage \\\\\\[OPTIONS\\\\\\] COMMAND \\\\\\[ARGS\\\\\\]... Check OpenAI API usage data ╭─ Options ───────────────────────────────────────────────────────╮ │ --help Show this message and exit. │ ╰─────────────────────────────────────────────────────────────────╯ ╭─ Commands ──────────────────────────────────────────────────────╮ │ list Displays OpenAI API usage data for the past N days. │ ╰─────────────────────────────────────────────────────────────────╯ List Usage for Specific Number of Days¶ To display API usage for the past 3 days, use the following command: $ instructor usage list -n 3 This will output a table similar to: Usage Summary by Date, Snapshot, and Cost ┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓ ┃ Date ┃ Snapshot ID ┃ Total Requests ┃ Total Cost ($) ┃ ┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩ │ 2023-09-04 │ gpt-4-0613 │ 44 │ 0.68 │ │ 2023-09-04 │ gpt-3.5-turbo-16k-0613 │ 195 │ 0.84 │ │ 2023-09-04 │ text-embedding-ada-002-v2 │ 276 │ 0.00 │ │ 2023-09-04 │ gpt-4-32k-0613 │ 328 │ 49.45 │ └────────────┴───────────────────────────┴────────────────┴────────────────┘ List Usage for Today¶ To display the API usage for today, simply run: $ instructor usage list Contributions¶ We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request. Was this page helpful? Back to top Previous Finetuning GPT-3.5 Next Core Library Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Open Source - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/open_source/?q=",
    "html": "Skip to content Instructor Open Source Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Instructor with open source models¶ Instructor works with Open source model providers that support the OpenAI API chat endpoint See examples README here Currently tested open source model providers¶ OpenRouter Perplexity RunPod TheBloke LLMs \\\\\\*\\\\\\* \\\\\\*\\\\\\* This utilizes text-generation-webui w/ Openai plugin under the hood. Was this page helpful? Back to top Previous PII Data Sanitization Next Introduction Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Multi-File Code Generation - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/gpt-engineer/?q=",
    "html": "Skip to content Instructor Multi-File Code Generation Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Data Structures Calling Completions Evaluating an Example Add Refactoring Capabilities Calling Refactor Completions Creating an Example Refactoring Example: Creating Multiple Files Program¶ This example shows how to create a multiple files program based on specifications by utilizing the OpenAI Function Call. We will define the necessary data structures using Pydantic and demonstrate how to convert a specification (prompt) into multiple files. Motivation Creating multiple file programs based on specifications is a challenging and rewarding skill that can help you build complex and scalable applications. With OpenAI Function Call, you can leverage the power of language models to generate an entire codebase and code snippets that match your specifications. Defining the Data Structures¶ Let's start by defining the data structure of File and Program. from typing import List from pydantic import Field from instructor import BaseModel class File(BaseModel): \"\"\" Correctly named file with contents. \"\"\" file\\\\\\_name: str = Field( ..., description=\"The name of the file including the extension\" ) body: str = Field(..., description=\"Correct contents of a file\") def save(self): with open(self.file\\\\\\_name, \"w\") as f: f.write(self.body) class Program(BaseModel): \"\"\" Set of files that represent a complete and correct program \"\"\" files: List\\\\\\[File\\\\\\] = Field(..., description=\"List of files\") The File class represents a single file or script, and it contains a name attribute and body for the text content of the file. Notice that we added the save method to the File class. This method is used to writes the body of the file to disk using the name as path. The Program class represents a collection of files that form a complete and correct program. It contains a list of File objects in the files attribute. Calling Completions¶ To create the files, we will use the base openai API. We can define a function that takes in a string and returns a Program object. import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def develop(data: str) -> Program: return client.chat.completions.create( model=\"gpt-3.5-turbo-0613\", temperature=0.1, response\\\\\\_model=Program, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class programming AI capable of writing correct python scripts and modules. You will name files correct, include \\\\\\_\\\\\\_init\\\\\\_\\\\\\_.py files and write correct python code with correct imports.\", }, { \"role\": \"user\", \"content\": data, }, \\\\\\], max\\\\\\_tokens=1000, ) Evaluating an Example¶ Let's evaluate the example by specifying the program to create and print the resulting files. program = develop( \"\"\" Create a fastapi app with a readme.md file and a main.py file with some basic math functions. the datamodels should use pydantic and the main.py should use fastapi. the readme.md should have a title and a description. The readme should contain some helpful infromation and a curl example\"\"\" ) for file in program.files: print(file.file\\\\\\_name) print(\"-\") print(file.body) print(\"\\\\\\\\n\\\\\\\\n\\\\\\\\n\") The output will be: # readme.md - # FastAPI App This is a FastAPI app that provides some basic math functions. ## Usage To use this app, follow the instructions below: 1. Install the required dependencies by running \\\\\\`pip install -r requirements.txt\\\\\\`. 2. Start the app by running \\\\\\`uvicorn main:app --reload\\\\\\`. 3. Open your browser and navigate to \\\\\\`http://localhost:8000/docs\\\\\\` to access the Swagger UI documentation. ## Example You can use the following curl command to test the \\\\\\`/add\\\\\\` endpoint: \\\\\\`\\\\\\`\\\\\\`bash $ curl -X POST -H \"Content-Type: application/json\" -d '{\"a\": 2, \"b\": 3}' http://localhost:8000/add \\\\\\`\\\\\\`\\\\\\` # main.py - from fastapi import FastAPI from pydantic import BaseModel app = FastAPI() class Numbers(BaseModel): a: int b: int @app.post('/add') def add\\\\\\_numbers(numbers: Numbers): return {'result': numbers.a + numbers.b} @app.post('/subtract') def subtract\\\\\\_numbers(numbers: Numbers): return {'result': numbers.a - numbers.b} @app.post('/multiply') def multiply\\\\\\_numbers(numbers: Numbers): return {'result': numbers.a \\\\\\* numbers.b} @app.post('/divide') def divide\\\\\\_numbers(numbers: Numbers): if numbers.b == 0: return {'error': 'Cannot divide by zero'} return {'result': numbers.a / numbers.b} # requirements.txt - fastapi uvicorn pydantic Add Refactoring Capabilities¶ This second part of the example shows how OpenAI API can be used to update the multiples files previously created, based on new specifications. In order to do that, we'll rely on the standard unidiff format. This will be our definition for a change in our code base: from pydantic import Field from instructor import BaseModel class Diff(BaseModel): \"\"\" Changes that must be correctly made in a program's code repository defined as a complete diff (Unified Format) file which will be used to \\\\\\`patch\\\\\\` the repository. Example: --- /path/to/original timestamp +++ /path/to/new timestamp @@ -1,3 +1,9 @@ +This is an important +notice! It should +therefore be located at +the beginning of this +document! + This part of the document has stayed the same from version to @@ -8,13 +14,8 @@ compress the size of the changes. -This paragraph contains -text that is outdated. -It will be deleted in the -near future. - It is important to spell -check this dokument. On +check this document. On the other hand, a misspelled word isn't the end of the world. @@ -22,3 +23,7 @@ this paragraph needs to be changed. Things can be added after it. + +This paragraph contains +important new additions +to this document. \"\"\" diff: str = Field( ..., description=( \"Changes in a code repository correctly represented in 'diff' format, \" \"correctly escaped so it could be used in a JSON\" ), ) The diff class represents a diff file, with a set of changes that can be applied to our program using a tool like patch or Git. Calling Refactor Completions¶ We'll define a function that will pass the program and the new specifications to the OpenAI API: from generate import Program def refactor(new\\\\\\_requirements: str, program: Program) -> Diff: program\\\\\\_description = \"\\\\\\\\n\".join( \\\\\\[f\"{code.file\\\\\\_name}\\\\\\\\n\\\\\\[\\\\\\[\\\\\\[\\\\\\\\n{code.body}\\\\\\\\n\\\\\\]\\\\\\]\\\\\\]\\\\\\\\n\" for code in program.files\\\\\\] ) return client.chat.completions.create( # model=\"gpt-3.5-turbo-0613\", model=\"gpt-4\", temperature=0, response\\\\\\_model=Diff, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class programming AI capable of refactor \" \"existing python repositories. You will name files correct, include \" \"\\\\\\_\\\\\\_init\\\\\\_\\\\\\_.py files and write correct python code, with correct imports. \" \"You'll deliver your changes in valid 'diff' format so that they could \" \"be applied using the 'patch' command. \" \"Make sure you put the correct line numbers, \" \"and that all lines that must be changed are correctly marked.\", }, { \"role\": \"user\", \"content\": new\\\\\\_requirements, }, { \"role\": \"user\", \"content\": program\\\\\\_description, }, \\\\\\], max\\\\\\_tokens=1000, ) Notice we're using here the version gpt-4 of the model, which is more powerful but, also, more expensive. Creating an Example Refactoring¶ To tests these refactoring, we'll use the program object, generated in the first part of this example. changes = refactor( new\\\\\\_requirements=\"Refactor this code to use flask instead.\", program=program, ) print(changes.diff) The output will be this: --- readme.md +++ readme.md @@ -1,9 +1,9 @@ # FastAPI App -This is a FastAPI app that provides some basic math functions. +This is a Flask app that provides some basic math functions. ## Usage To use this app, follow the instructions below: 1. Install the required dependencies by running \\\\\\`pip install -r requirements.txt\\\\\\`. -2. Start the app by running \\\\\\`uvicorn main:app --reload\\\\\\`. +2. Start the app by running \\\\\\`flask run\\\\\\`. 3. Open your browser and navigate to \\\\\\`http://localhost:5000/docs\\\\\\` to access the Swagger UI documentation. ## Example To perform a basic math operation, you can use the following curl command: \\\\\\`\\\\\\`\\\\\\`bash -curl -X POST -H \"Content-Type: application/json\" -d '{\"operation\": \"add\", \"operands\": \\\\\\[2, 3\\\\\\]}' http://localhost:8000/calculate +curl -X POST -H \"Content-Type: application/json\" -d '{\"operation\": \"add\", \"operands\": \\\\\\[2, 3\\\\\\]}' http://localhost:5000/calculate --- main.py +++ main.py @@ -1,29 +1,29 @@ -from fastapi import FastAPI -from pydantic import BaseModel +from flask import Flask, request, jsonify -app = FastAPI() +app = Flask(name) -class Operation(BaseModel): operation: str operands: list +@app.route('/calculate', methods=\\\\\\['POST'\\\\\\]) +def calculate(): data = request.get\\\\\\_json() operation = data.get('operation') operands = data.get('operands') -@app.post('/calculate') -async def calculate(operation: Operation): if operation.operation == 'add': result = sum(operation.operands) elif operation.operation == 'subtract': result = operation.operands\\\\\\[0\\\\\\] - sum(operation.operands\\\\\\[1:\\\\\\]) elif operation.operation == 'multiply': if operation == 'add': result = sum(operands) elif operation == 'subtract': result = operands\\\\\\[0\\\\\\] - sum(operands\\\\\\[1:\\\\\\]) elif operation == 'multiply': result = 1 for operand in operation.operands: for operand in operands: result \\\\\\*= operand elif operation.operation == 'divide': result = operation.operands\\\\\\[0\\\\\\] for operand in operation.operands\\\\\\[1:\\\\\\]: elif operation == 'divide': result = operands\\\\\\[0\\\\\\] for operand in operands\\\\\\[1:\\\\\\]: result /= operand else: result = None return {'result': result} return jsonify({'result': result}) --- requirements.txt +++ requirements.txt @@ -1,3 +1,2 @@ -fastapi -uvicorn -pydantic +flask +flask-cors Was this page helpful? Back to top Previous Action Item and Dependency Mapping Next PII Data Sanitization Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "PII Data Sanitization - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/pii/?q=",
    "html": "Skip to content Instructor PII Data Sanitization Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Overview Defining the Structures Extracting PII Data Output of Extracted PII Data Scrubbing PII Data Output of Scrubbed Document PII Data Extraction and Scrubbing¶ Overview¶ This example demonstrates the usage of OpenAI's ChatCompletion model for the extraction and scrubbing of Personally Identifiable Information (PII) from a document. The code defines Pydantic models to manage the PII data and offers methods for both extraction and sanitation. Defining the Structures¶ First, Pydantic models are defined to represent the PII data and the overall structure for PII data extraction. from typing import List from pydantic import BaseModel, Field # Define Schemas for PII data class Data(BaseModel): index: int data\\\\\\_type: str pii\\\\\\_value: str class PIIDataExtraction(BaseModel): \"\"\" Extracted PII data from a document, all data\\\\\\_types should try to have consistent property names \"\"\" private\\\\\\_data: List\\\\\\[Data\\\\\\] def scrub\\\\\\_data(self, content: str) -> str: \"\"\" Iterates over the private data and replaces the value with a placeholder in the form of <{data\\\\\\_type}\\\\\\_{i}> \"\"\" for i, data in enumerate(self.private\\\\\\_data): content = content.replace(data.pii\\\\\\_value, f\"<{data.data\\\\\\_type}\\\\\\_{i}>\") return content Extracting PII Data¶ The OpenAI API is utilized to extract PII information from a given document. from openai import OpenAI import instructor client = instructor.patch(OpenAI()) EXAMPLE\\\\\\_DOCUMENT = \"\"\" # Fake Document with PII for Testing PII Scrubbing Model # (The content here) \"\"\" pii\\\\\\_data: PIIDataExtraction = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=PIIDataExtraction, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class PII scrubbing model, Extract the PII data from the following document\", }, { \"role\": \"user\", \"content\": EXAMPLE\\\\\\_DOCUMENT, }, \\\\\\] ) # type: ignore print(\"Extracted PII Data:\") print(pii\\\\\\_data.json(indent=2)) Output of Extracted PII Data¶ { \"private\\\\\\_data\": \\\\\\[ { \"index\": 0, \"data\\\\\\_type\": \"date\", \"pii\\\\\\_value\": \"01/02/1980\" }, { \"index\": 1, \"data\\\\\\_type\": \"ssn\", \"pii\\\\\\_value\": \"123-45-6789\" }, { \"index\": 2, \"data\\\\\\_type\": \"email\", \"pii\\\\\\_value\": \"john.doe@email.com\" }, { \"index\": 3, \"data\\\\\\_type\": \"phone\", \"pii\\\\\\_value\": \"555-123-4567\" }, { \"index\": 4, \"data\\\\\\_type\": \"address\", \"pii\\\\\\_value\": \"123 Main St, Springfield, IL, 62704\" } \\\\\\] } Scrubbing PII Data¶ After extracting the PII data, the scrub\\\\\\_data method is used to sanitize the document. print(\"Scrubbed Document:\") print(pii\\\\\\_data.scrub\\\\\\_data(EXAMPLE\\\\\\_DOCUMENT)) Output of Scrubbed Document¶ # Fake Document with PII for Testing PII Scrubbing Model ## Personal Story John Doe was born on . His social security number is . He has been using the email address for years, and he can always be reached at . ## Residence John currently resides at . He's been living there for about 5 years now. Was this page helpful? Back to top Previous Multi-File Code Generation Next Open Source Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Action Item and Dependency Mapping - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/action_items/?q=",
    "html": "Skip to content Instructor Action Item and Dependency Mapping Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Structures Extracting Action Items Evaluation and Testing Visualizing the tasks Example: Extracting Action Items from Meeting Transcripts¶ In this guide, we'll walk through how to extract action items from meeting transcripts using OpenAI's API and Pydantic. This use case is essential for automating project management tasks, such as task assignment and priority setting. Motivation Significant amount of time is dedicated to meetings, where action items are generated as the actionable outcomes of these discussions. Automating the extraction of action items can save time and guarantee that no critical tasks are overlooked. Defining the Structures¶ We'll model a meeting transcript as a collection of Ticket objects, each representing an action item. Every Ticket can have multiple Subtask objects, representing smaller, manageable pieces of the main task. from enum import Enum from pydantic import BaseModel, Field from typing import List, Optional class PriorityEnum(str, Enum): high = \"High\" medium = \"Medium\" low = \"Low\" class Subtask(BaseModel): \"\"\"Correctly resolved subtask from the given transcript\"\"\" id: int name: str class Ticket(BaseModel): \"\"\"Correctly resolved ticket from the given transcript\"\"\" id: int name: str description: str priority: PriorityEnum assignees: List\\\\\\[str\\\\\\] subtasks: Optional\\\\\\[List\\\\\\[Subtask\\\\\\]\\\\\\] dependencies: Optional\\\\\\[List\\\\\\[int\\\\\\]\\\\\\] class ActionItems(BaseModel): \"\"\"Correctly resolved set of action items from the given transcript\"\"\" items: List\\\\\\[Ticket\\\\\\] Extracting Action Items¶ To extract action items from a meeting transcript, we use the generate function. It calls OpenAI's API, processes the text, and returns a set of action items modeled as ActionItems. import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def generate(data: str) -> ActionItems: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=ActionItems, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"The following is a transcript of a meeting...\", }, { \"role\": \"user\", \"content\": f\"Create the action items for the following transcript: {data}\", }, \\\\\\], ) # type: ignore Evaluation and Testing¶ To test the generate function, we provide it with a sample transcript, and then print the JSON representation of the extracted action items. prediction = generate( \"\"\" Alice: Hey team, we have several critical tasks we need to tackle for the upcoming release. First, we need to work on improving the authentication system. It's a top priority. Bob: Got it, Alice. I can take the lead on the authentication improvements. Are there any specific areas you want me to focus on? Alice: Good question, Bob. We need both a front-end revamp and back-end optimization. So basically, two sub-tasks. Carol: I can help with the front-end part of the authentication system. Bob: Great, Carol. I'll handle the back-end optimization then. Alice: Perfect. Now, after the authentication system is improved, we have to integrate it with our new billing system. That's a medium priority task. Carol: Is the new billing system already in place? Alice: No, it's actually another task. So it's a dependency for the integration task. Bob, can you also handle the billing system? Bob: Sure, but I'll need to complete the back-end optimization of the authentication system first, so it's dependent on that. Alice: Understood. Lastly, we also need to update our user documentation to reflect all these changes. It's a low-priority task but still important. Carol: I can take that on once the front-end changes for the authentication system are done. So, it would be dependent on that. Alice: Sounds like a plan. Let's get these tasks modeled out and get started.\"\"\" ) Visualizing the tasks¶ In order to quickly visualize the data we used code interpreter to create a graphviz export of the json version of the ActionItems array. { \"items\": \\\\\\[ { \"id\": 1, \"name\": \"Improve Authentication System\", \"description\": \"Revamp the front-end and optimize the back-end of the authentication system\", \"priority\": \"High\", \"assignees\": \\\\\\[\"Bob\", \"Carol\"\\\\\\], \"subtasks\": \\\\\\[ { \"id\": 2, \"name\": \"Front-end Revamp\" }, { \"id\": 3, \"name\": \"Back-end Optimization\" } \\\\\\], \"dependencies\": \\\\\\[\\\\\\] }, { \"id\": 4, \"name\": \"Integrate Authentication System with Billing System\", \"description\": \"Integrate the improved authentication system with the new billing system\", \"priority\": \"Medium\", \"assignees\": \\\\\\[\"Bob\"\\\\\\], \"subtasks\": \\\\\\[\\\\\\], \"dependencies\": \\\\\\[1\\\\\\] }, { \"id\": 5, \"name\": \"Update User Documentation\", \"description\": \"Update the user documentation to reflect the changes in the authentication system\", \"priority\": \"Low\", \"assignees\": \\\\\\[\"Carol\"\\\\\\], \"subtasks\": \\\\\\[\\\\\\], \"dependencies\": \\\\\\[2\\\\\\] } \\\\\\] } In this example, the generate function successfully identifies and segments the action items, assigning them priorities, assignees, subtasks, and dependencies as discussed in the meeting. By automating this process, you can ensure that important tasks and details are not lost in the sea of meeting minutes, making project management more efficient and effective. Was this page helpful? Back to top Previous Table Extraction Next Multi-File Code Generation Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Table Extraction - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/autodataframe/?q=",
    "html": "Skip to content Instructor Table Extraction Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Data Structures Using the Prompt Pipeline Evaluating an Example Example: Converting Text into Dataframes¶ In this example, we'll demonstrate how to convert a text into dataframes using OpenAI Function Call. We will define the necessary data structures using Pydantic and show how to convert the text into dataframes. Motivation Often times when we parse data we have an opportunity to extract structured data, what if we could extract an arbitrary number of tables with arbitrary schemas? By pulling out dataframes we could write tables or .csv files and attach them to our retrieved data. Defining the Data Structures¶ Let's start by defining the data structures required for this task: RowData, Dataframe, and Database. from pydantic import Field, BaseModel from typing import List, Any class RowData(BaseModel): row: List\\\\\\[Any\\\\\\] = Field(..., description=\"The values for each row\") citation: str = Field( ..., description=\"The citation for this row from the original source data\" ) class Dataframe(BaseModel): \"\"\" Class representing a dataframe. This class is used to convert data into a frame that can be used by pandas. \"\"\" name: str = Field(..., description=\"The name of the dataframe\") data: List\\\\\\[RowData\\\\\\] = Field( ..., description=\"Correct rows of data aligned to column names, Nones are allowed\", ) columns: List\\\\\\[str\\\\\\] = Field( ..., description=\"Column names relevant from source data, should be in snake\\\\\\_case\", ) def to\\\\\\_pandas(self): import pandas as pd columns = self.columns + \\\\\\[\"citation\"\\\\\\] data = \\\\\\[row.row + \\\\\\[row.citation\\\\\\] for row in self.data\\\\\\] return pd.DataFrame(data=data, columns=columns) class Database(BaseModel): \"\"\" A set of correct named and defined tables as dataframes \"\"\" tables: List\\\\\\[Dataframe\\\\\\] = Field( ..., description=\"List of tables in the database\", ) The RowData class represents a single row of data in the dataframe. It contains a row attribute for the values in each row and a citation attribute for the citation from the original source data. The Dataframe class represents a dataframe and consists of a name attribute, a list of RowData objects in the data attribute, and a list of column names in the columns attribute. It also provides a to\\\\\\_pandas method to convert the dataframe into a Pandas DataFrame. The Database class represents a set of tables in a database. It contains a list of Dataframe objects in the tables attribute. Using the Prompt Pipeline¶ To convert a text into dataframes, we'll use the Prompt Pipeline in OpenAI Function Call. We can define a function dataframe that takes a text as input and returns a Database object. import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def dataframe(data: str) -> Database: return client.chat.completions.create( model=\"gpt-4-0613\", temperature=0.1, response\\\\\\_model=Database, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"\"\"Map this data into a dataframe a nd correctly define the correct columns and rows\"\"\", }, { \"role\": \"user\", \"content\": f\"{data}\", }, \\\\\\], max\\\\\\_tokens=1000, ) The dataframe function takes a string data as input and creates a completion using the Prompt Pipeline. It prompts the model to map the data into a dataframe and define the correct columns and rows. The resulting completion is then converted into a Database object. Evaluating an Example¶ Let's evaluate the example by converting a text into dataframes using the dataframe function and print the resulting dataframes. dfs = dataframe(\"\"\"My name is John and I am 25 years old. I live in New York and I like to play basketball. His name is Mike and he is 30 years old. He lives in San Francisco and he likes to play baseball. Sarah is 20 years old and she lives in Los Angeles. She likes to play tennis. Her name is Mary and she is 35 years old. She lives in Chicago. On one team 'Tigers' the captain is John and there are 12 players. On the other team 'Lions' the captain is Mike and there are 10 players. \"\"\") for df in dfs.tables: print(df.name) print(df.to\\\\\\_pandas()) The output will be: People Name Age City Favorite Sport 0 John 25 New York Basketball 1 Mike 30 San Francisco Baseball 2 Sarah 20 Los Angeles Tennis 3 Mary 35 Chicago None Teams Team Name Captain Number of Players 0 Tigers John 12 1 Lions Mike 10 Was this page helpful? Back to top Previous Recursive Schemas Next Action Item and Dependency Mapping Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Recursive Schemas - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/recursive/?q=",
    "html": "Skip to content Instructor Recursive Schemas Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Structures Parsing the Tree Example Usage Example: Parsing a Directory Tree¶ In this example, we will demonstrate how define and use a recursive class definition to convert a string representing a directory tree into a filesystem structure using OpenAI's function call api. We will define the necessary structures using Pydantic, create a function to parse the tree, and provide an example of how to use it. Defining the Structures¶ We will use Pydantic to define the necessary data structures representing the directory tree and its nodes. We have two classes, Node and DirectoryTree, which are used to model individual nodes and the entire directory tree, respectively. Flat is better than nested While it's easier to model things as nested, returning flat items with dependencies tends to yield better results. For a flat example, check out planning tasks where we model a query plan as a dag. import enum from typing import List from pydantic import Field class NodeType(str, enum.Enum): \"\"\"Enumeration representing the types of nodes in a filesystem.\"\"\" FILE = \"file\" FOLDER = \"folder\" class Node(BaseModel): \"\"\" Class representing a single node in a filesystem. Can be either a file or a folder. Note that a file cannot have children, but a folder can. Args: name (str): The name of the node. children (List\\\\\\[Node\\\\\\]): The list of child nodes (if any). node\\\\\\_type (NodeType): The type of the node, either a file or a folder. Methods: print\\\\\\_paths: Prints the path of the node and its children. \"\"\" name: str = Field(..., description=\"Name of the folder\") children: List\\\\\\[\"Node\"\\\\\\] = Field( default\\\\\\_factory=list, description=\"List of children nodes, only applicable for folders, files cannot have children\", ) node\\\\\\_type: NodeType = Field( default=NodeType.FILE, description=\"Either a file or folder, use the name to determine which it could be\", ) def print\\\\\\_paths(self, parent\\\\\\_path=\"\"): \"\"\"Prints the path of the node and its children.\"\"\" if self.node\\\\\\_type == NodeType.FOLDER: path = f\"{parent\\\\\\_path}/{self.name}\" if parent\\\\\\_path != \"\" else self.name print(path, self.node\\\\\\_type) if self.children is not None: for child in self.children: child.print\\\\\\_paths(path) else: print(f\"{parent\\\\\\_path}/{self.name}\", self.node\\\\\\_type) class DirectoryTree(BaseModel): \"\"\" Container class representing a directory tree. Args: root (Node): The root node of the tree. Methods: print\\\\\\_paths: Prints the paths of the root node and its children. \"\"\" root: Node = Field(..., description=\"Root folder of the directory tree\") def print\\\\\\_paths(self): \"\"\"Prints the paths of the root node and its children.\"\"\" self.root.print\\\\\\_paths() Node.update\\\\\\_forward\\\\\\_refs() DirectoryTree.update\\\\\\_forward\\\\\\_refs() The Node class represents a single node in the directory tree. It has a name, a list of children nodes (applicable only to folders), and a node type (either a file or a folder). The print\\\\\\_paths method can be used to print the path of the node and its children. The DirectoryTree class represents the entire directory tree. It has a single attribute, root, which is the root node of the tree. The print\\\\\\_paths method can be used to print the paths of the root node and its children. Parsing the Tree¶ We define a function parse\\\\\\_tree\\\\\\_to\\\\\\_filesystem to convert a string representing a directory tree into a filesystem structure using OpenAI. import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def parse\\\\\\_tree\\\\\\_to\\\\\\_filesystem(data: str) -> DirectoryTree: \"\"\" Convert a string representing a directory tree into a filesystem structure using OpenAI's GPT-3 model. Args: data (str): The string to convert into a filesystem. Returns: DirectoryTree: The directory tree representing the filesystem. \"\"\" return client.chat.completions.create( model=\"gpt-3.5-turbo-0613\", response\\\\\\_model=DirectoryTree, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a perfect file system parsing algorithm. You are given a string representing a directory tree. You must return the correct filesystem structure.\", }, { \"role\": \"user\", \"content\": f\"Consider the data below:\\\\\\\\n{data} and return the correctly labeled filesystem\", }, \\\\\\], max\\\\\\_tokens=1000, ) The parse\\\\\\_tree\\\\\\_to\\\\\\_filesystem function takes a string data representing the directory tree and returns a DirectoryTree object representing the filesystem structure. It uses the OpenAI Chat API to complete the prompt and extract the directory tree. Example Usage¶ Let's demonstrate how to use the parse\\\\\\_tree\\\\\\_to\\\\\\_filesystem function with an example: root = parse\\\\\\_tree\\\\\\_to\\\\\\_filesystem( \"\"\" root ├── folder1 │ ├── file1.txt │ └── file2.txt └── folder2 ├── file3.txt └── subfolder1 └── file4.txt \"\"\" ) root.print\\\\\\_paths() In this example, we call parse\\\\\\_tree\\\\\\_to\\\\\\_filesystem with a string representing a directory tree. After parsing the string into a DirectoryTree object, we call root.print\\\\\\_paths() to print the paths of the root node and its children. The output of this example will be: root NodeType.FOLDER root/folder1 NodeType.FOLDER root/folder1/file1.txt NodeType.FILE root/folder1/file2.txt NodeType.FILE root/folder2 NodeType.FOLDER root/folder2/file3.txt NodeType.FILE root/folder2/subfolder1 NodeType.FOLDER root/folder2/subfolder1/file4.txt NodeType.FILE This demonstrates how to use OpenAI's GPT-3 model to parse a string representing a directory tree and obtain the correct filesystem structure. I hope this example helps you understand how to leverage OpenAI Function Call for parsing recursive trees. If you have any further questions, feel free to ask! Was this page helpful? Back to top Previous Query Decomposition Next Table Extraction Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Query Decomposition - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/planning-tasks/?q=",
    "html": "Skip to content Instructor Query Decomposition Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Structures Planning a Query Plan Conclusion Example: Planning and Executing a Query Plan¶ This example demonstrates how to use the OpenAI Function Call ChatCompletion model to plan and execute a query plan in a question-answering system. By breaking down a complex question into smaller sub-questions with defined dependencies, the system can systematically gather the necessary information to answer the main question. Motivation The goal of this example is to showcase how query planning can be used to handle complex questions, facilitate iterative information gathering, automate workflows, and optimize processes. By leveraging the OpenAI Function Call model, you can design and execute a structured plan to find answers effectively. Use Cases: Complex question answering Iterative information gathering Workflow automation Process optimization With the OpenAI Function Call model, you can customize the planning process and integrate it into your specific application to meet your unique requirements. Defining the Structures¶ Let's define the necessary Pydantic models to represent the query plan and the queries. import enum from typing import List from pydantic import Field class QueryType(str, enum.Enum): \"\"\"Enumeration representing the types of queries that can be asked to a question answer system.\"\"\" SINGLE\\\\\\_QUESTION = \"SINGLE\" MERGE\\\\\\_MULTIPLE\\\\\\_RESPONSES = \"MERGE\\\\\\_MULTIPLE\\\\\\_RESPONSES\" class Query(BaseModel): \"\"\"Class representing a single question in a query plan.\"\"\" id: int = Field(..., description=\"Unique id of the query\") question: str = Field( ..., description=\"Question asked using a question answering system\", ) dependencies: List\\\\\\[int\\\\\\] = Field( default\\\\\\_factory=list, description=\"List of sub questions that need to be answered before asking this question\", ) node\\\\\\_type: QueryType = Field( default=QueryType.SINGLE\\\\\\_QUESTION, description=\"Type of question, either a single question or a multi-question merge\", ) class QueryPlan(BaseModel): \"\"\"Container class representing a tree of questions to ask a question answering system.\"\"\" query\\\\\\_graph: List\\\\\\[Query\\\\\\] = Field( ..., description=\"The query graph representing the plan\" ) def \\\\\\_dependencies(self, ids: List\\\\\\[int\\\\\\]) -> List\\\\\\[Query\\\\\\]: \"\"\"Returns the dependencies of a query given their ids.\"\"\" return \\\\\\[q for q in self.query\\\\\\_graph if q.id in ids\\\\\\] Graph Generation Notice that this example produces a flat list of items with dependencies that resemble a graph, while pydantic allows for recursive definitions, it's much easier and less confusing for the model to generate flat schemas rather than recursive schemas. If you want to see a recursive example, see recursive schemas Planning a Query Plan¶ Now, let's demonstrate how to plan and execute a query plan using the defined models and the OpenAI API. import asyncio import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def query\\\\\\_planner(question: str) -> QueryPlan: PLANNING\\\\\\_MODEL = \"gpt-4-0613\" messages = \\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class query planning algorithm capable ofbreaking apart questions into its dependency queries such that the answers can be used to inform the parent question. Do not answer the questions, simply provide a correct compute graph with good specific questions to ask and relevant dependencies. Before you call the function, think step-by-step to get a better understanding of the problem.\", }, { \"role\": \"user\", \"content\": f\"Consider: {question}\\\\\\\\nGenerate the correct query plan.\", }, \\\\\\] root = client.chat.completions.create( model=PLANNING\\\\\\_MODEL, temperature=0, response\\\\\\_model=QueryPlan, messages=messages, max\\\\\\_tokens=1000, ) return root plan = query\\\\\\_planner( \"What is the difference in populations of Canada and the Jason's home country?\" ) plan.model\\\\\\_dump() No RAG While we build the query plan in this example, we do not propose a method to actually answer the question. You can implement your own answer function that perhaps makes a retrival and calls openai for retrival augmented generation. That step would also make use of function calls but goes beyond the scope of this example. {'query\\\\\\_graph': \\\\\\[{'dependencies': \\\\\\[\\\\\\], 'id': 1, 'node\\\\\\_type': , 'question': \"Identify Jason's home country\"}, {'dependencies': \\\\\\[\\\\\\], 'id': 2, 'node\\\\\\_type': , 'question': 'Find the population of Canada'}, {'dependencies': \\\\\\[1\\\\\\], 'id': 3, 'node\\\\\\_type': , 'question': \"Find the population of Jason's home country\"}, {'dependencies': \\\\\\[2, 3\\\\\\], 'id': 4, 'node\\\\\\_type': , 'question': 'Calculate the difference in populations between Canada and Jason's home country\"}\\\\\\]} In the above code, we define a query\\\\\\_planner function that takes a question as input and generates a query plan using the OpenAI API. Conclusion¶ In this example, we demonstrated how to use the OpenAI Function Call ChatCompletion model to plan and execute a query plan using a question-answering system. We defined the necessary structures using Pydantic, created a query planner function. If you want to see multiple versions of this style of code, please visit: query planning example task planning with topo sort Feel free to modify the code to fit your specific use case and explore other possibilities of using the OpenAI Function Call model to plan and execute complex workflows. Was this page helpful? Back to top Previous Search Queries Next Recursive Schemas Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Entity Resolution - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/entity_resolution/?q=",
    "html": "Skip to content Instructor Entity Resolution Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Data Structures Entity Extraction and Resolution Graph Visualization Execution Entity Resolution and Visualization for Legal Documents¶ In this guide, we demonstrate how to extract and resolve entities from a sample legal contract. Then, we visualize these entities and their dependencies as an entity graph. This approach can be invaluable for legal tech applications, aiding in the understanding of complex documents. Motivation Legal contracts are full of intricate details and interconnected clauses. Automatically extracting and visualizing these elements can make it easier to understand the document's overall structure and terms. Defining the Data Structures¶ The Entity and Property classes model extracted entities and their attributes. DocumentExtraction encapsulates a list of these entities. from pydantic import BaseModel, Field from typing import List class Property(BaseModel): key: str value: str resolved\\\\\\_absolute\\\\\\_value: str class Entity(BaseModel): id: int = Field( ..., description=\"Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities\", ) subquote\\\\\\_string: List\\\\\\[str\\\\\\] = Field( ..., description=\"Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution\", ) entity\\\\\\_title: str properties: List\\\\\\[Property\\\\\\] = Field( ..., description=\"List of properties of the entity\" ) dependencies: List\\\\\\[int\\\\\\] = Field( ..., description=\"List of entity ids that this entity depends or relies on to resolve it\", ) class DocumentExtraction(BaseModel): entities: List\\\\\\[Entity\\\\\\] = Field( ..., description=\"Body of the answer, each fact should be a separate object with a body and a list of sources\", ) Entity Extraction and Resolution¶ The ask\\\\\\_ai function utilizes OpenAI's API to extract and resolve entities from the input content. import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def ask\\\\\\_ai(content) -> DocumentExtraction: return client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=DocumentExtraction, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"Extract and resolve a list of entities from the following document:\", }, { \"role\": \"user\", \"content\": content, }, \\\\\\], ) # type: ignore Graph Visualization¶ generate\\\\\\_graph takes the extracted entities and visualizes them using Graphviz. It creates nodes for each entity and edges for their dependencies. from graphviz import Digraph def generate\\\\\\_html\\\\\\_label(entity: Entity) -> str: rows = \\\\\\[f\"{prop.key}{prop.resolved\\\\\\_absolute\\\\\\_value}\" for prop in entity.properties\\\\\\] table\\\\\\_rows = \"\".join(rows) return f\"<{table\\\\\\_rows} \\*\\*{entity.entity\\\\\\_title}\\*\\* \\\\>\" def generate\\\\\\_graph(data: DocumentExtraction): dot = Digraph(comment=\"Entity Graph\", node\\\\\\_attr={\"shape\": \"plaintext\"}) for entity in data.entities: label = generate\\\\\\_html\\\\\\_label(entity) dot.node(str(entity.id), label) for entity in data.entities: for dep\\\\\\_id in entity.dependencies: dot.edge(str(entity.id), str(dep\\\\\\_id)) dot.render(\"entity.gv\", view=True) Execution¶ Finally, execute the code to visualize the entity graph for the sample legal contract. content = \"\"\" Sample Legal Contract Agreement Contract This Agreement is made and entered into on 2020-01-01 by and between Company A (\"the Client\") and Company B (\"the Service Provider\"). Article 1: Scope of Work The Service Provider will deliver the software product to the Client 30 days after the agreement date. Article 2: Payment Terms The total payment for the service is $50,000. An initial payment of $10,000 will be made within 7 days of the the signed date. The final payment will be due 45 days after \\\\\\[SignDate\\\\\\]. Article 3: Confidentiality The parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date. Article 4: Termination The contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the \\\\\\[DeliveryDate\\\\\\]. \"\"\" # Your legal contract here model = ask\\\\\\_ai(content) generate\\\\\\_graph(model) This will produce a graphical representation of the entities and their dependencies, stored as \"entity.gv\". Was this page helpful? Back to top Previous Knowledge Graph Next Search Queries Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Image to Ad Copy - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/image_to_ad_copy/?q=",
    "html": "Skip to content Instructor Image to Ad Copy Type to start searching instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Building the models Product Identified Product Advertising Copy Calling the API Product Detection Generate advertising copy Putting it all together Input file Use Vision API to detect products and generate advertising copy¶ This post demonstrates how to use GPT-4 Vision API and the Chat API to automatically generate advertising copy from product images. This method can be useful for marketing and advertising teams, as well as for e-commerce platforms. The full code is available on GitHub. Building the models¶ Product¶ For the Product model, we define a class that represents a product extracted from an image and store the name, key features, and description. The product attributes are dynamically determined based on the content of the image. Note that it is easy to add Validators and other Pydantic features to the model to ensure that the data is valid and consistent. class Product(BaseModel): \"\"\" Represents a product extracted from an image using AI. The product attributes are dynamically determined based on the content of the image and the AI's interpretation. This class serves as a structured representation of the identified product characteristics. \"\"\" name: str = Field( description=\"A generic name for the product.\", example=\"Headphones\" ) key\\\\\\_features: Optional\\\\\\[List\\\\\\[str\\\\\\]\\\\\\] = Field( description=\"A list of key features of the product that stand out.\", default=None, ) description: Optional\\\\\\[str\\\\\\] = Field( description=\"A description of the product.\", default=None, ) # Can be customized and automatically generated def generate\\\\\\_prompt(self): prompt = f\"Product: {self.name}\\\\\\\\n\" if self.description: prompt += f\"Description: {self.description}\\\\\\\\n\" if self.key\\\\\\_features: prompt += f\"Key Features: {', '.join(self.key\\\\\\_features)}\\\\\\\\n\" return prompt Identified Product¶ We also define a class that represents a list of products identified in the images. We also add an error flag and message to indicate if there was an error in the processing of the image. class IdentifiedProduct(BaseModel): \"\"\" Represents a list of products identified in the images. \"\"\" products: Optional\\\\\\[List\\\\\\[Product\\\\\\]\\\\\\] = Field( description=\"A list of products identified by the AI.\", example=\\\\\\[ Product( name=\"Headphones\", description=\"Wireless headphones with noise cancellation.\", key\\\\\\_features=\\\\\\[\"Wireless\", \"Noise Cancellation\"\\\\\\], ) \\\\\\], default=None, ) error: bool = Field(default=False) message: Optional\\\\\\[str\\\\\\] = Field(default=None) Advertising Copy¶ Finally, the AdCopy models stores the output in a structured format with a headline and the text. class AdCopy(BaseModel): \"\"\" Represents a generated ad copy. \"\"\" headline: str = Field( description=\"A short, catchy, and memorable headline for the given product. The headline should invoke curiosity and interest in the product.\", ) ad\\\\\\_copy: str = Field( description=\"A long-form advertisement copy for the given product. This will be used in campaigns to promote the product with a persuasive message and a call-to-action with the objective of driving sales.\", ) name: str = Field( description=\"The name of the product being advertised.\" ) Calling the API¶ Product Detection¶ The read\\\\\\_images function uses OpenAI's vision model to process a list of image URLs and identify products in each of them. We utilize the instructor library to patch the OpenAI client for this purpose. def read\\\\\\_images(image\\\\\\_urls: List\\\\\\[str\\\\\\]) -> IdentifiedProduct: \"\"\" Given a list of image URLs, identify the products in the images. \"\"\" logger.info(f\"Identifying products in images... {len(image\\\\\\_urls)} images\") return client\\\\\\_image.chat.completions.create( model=\"gpt-4-vision-preview\", response\\\\\\_model=IdentifiedProduct, max\\\\\\_tokens=1024, # can be changed temperature=0, messages=\\\\\\[ { \"role\": \"user\", \"content\": \\\\\\[ { \"type\": \"text\", \"text\": \"Identify products using the given images and generate key features for each product.\", }, \\\\\\*\\\\\\[ {\"type\": \"image\\\\\\_url\", \"image\\\\\\_url\": {\"url\": url}} for url in image\\\\\\_urls \\\\\\], \\\\\\], } \\\\\\], ) This gives us a list of products identified in all the images. Generate advertising copy¶ Then, we can use the generate\\\\\\_ad\\\\\\_copy function to generate advertising copy for each of the products identified in the images. Two clients are defined for the two different models. This is because the gpt-4-vision-preview model is not compatible with the gpt-4-1106-preview model in terms of their response format. def generate\\\\\\_ad\\\\\\_copy(product: Product) -> AdCopy: \"\"\" Given a product, generate an ad copy for the product. \"\"\" logger.info(f\"Generating ad copy for product: {product.name}\") return client\\\\\\_copy.chat.completions.create( model=\"gpt-4-1106-preview\", response\\\\\\_model=AdCopy, temperature=0.3, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are an expert marketing assistant for all products. Your task is to generate an advertisement copy for a product using the name, description, and key features.\", }, {\"role\": \"user\", \"content\": product.generate\\\\\\_prompt()}, \\\\\\], ) Putting it all together¶ Finally, we can put it all together in a single function that takes a list of image URLs and generates advertising copy for the products identified in the images. Please refer to the full code for the complete implementation. Input file¶ The input file is currently a list of image URLs, but this trivial to change to any required format. https://contents.mediadecathlon.com/p1279823/9a1c59ad97a4084a346c014740ae4d3ff860ea70b485ee65f34017ff5e9ae5f7/recreational-ice-skates-fit-50-black.jpg?format=auto https://contents.mediadecathlon.com/p1279822/a730505231dbd6747c14ee93e8f89e824d3fa2a5b885ec26de8d7feb5626638a/recreational-ice-skates-fit-50-black.jpg?format=auto https://contents.mediadecathlon.com/p2329893/1ed75517602a5e00245b89ab6a1c6be6d8968a5a227c932b10599f857f3ed4cd/mens-hiking-leather-boots-sh-100-x-warm.jpg?format=auto https://contents.mediadecathlon.com/p2047870/8712c55568dd9928c83b19c6a4067bf161811a469433dc89244f0ff96a50e3e9/men-s-winter-hiking-boots-sh-100-x-warm-grey.jpg?format=auto Expand to see the output Was this page helpful? Back to top Previous Image Extracting Tables Next Moderation Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Search Queries - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/search/?q=",
    "html": "Skip to content Instructor Search Queries Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Structures Calling Completions Evaluating an Example Example: Segmenting Search Queries¶ In this example, we will demonstrate how to leverage the MultiTask and enum.Enum features of OpenAI Function Call to segment search queries. We will define the necessary structures using Pydantic and demonstrate how segment queries into multiple sub queries and execute them in parallel with asyncio. Motivation Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use OpenAI Function Call to segment search queries and execute them in parallel. Defining the Structures¶ Let's model the problem as breaking down a search request into a list of search queries. We will use an enum to represent different types of searches and take advantage of Python objects to add additional query logic. import enum from pydantic import Field class SearchType(str, enum.Enum): \"\"\"Enumeration representing the types of searches that can be performed.\"\"\" VIDEO = \"video\" EMAIL = \"email\" class Search(BaseModel): \"\"\" Class representing a single search query. \"\"\" title: str = Field(..., description=\"Title of the request\") query: str = Field(..., description=\"Query to search for relevant content\") type: SearchType = Field(..., description=\"Type of search\") async def execute(self): print(f\"Searching for \\\\\\`{self.title}\\\\\\` with query \\\\\\`{self.query}\\\\\\` using \\\\\\`{self.type}\\\\\\`\") Notice that we have added the execute method to the Search class. This method can be used to route the search query based on the enum type. You can add logic specific to each search type in the execute method. Next, let's define a class to represent multiple search queries. from typing import List class MultiSearch(BaseModel): \"Correctly segmented set of search results\" tasks: List\\\\\\[Search\\\\\\] The MultiSearch class has a single attribute, tasks, which is a list of Search objects. This pattern is so common that we've added a helper function MultiTask to makes this simpler from instructor.dsl import MultiTask MultiSearch = MultiTask(Search) Calling Completions¶ To segment a search query, we will use the base openai api. We can define a function that takes a string and returns segmented search queries using the MultiSearch class. import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def segment(data: str) -> MultiSearch: return client.chat.completions.create( model=\"gpt-3.5-turbo-0613\", temperature=0.1, functions=\\\\\\[MultiSearch.openai\\\\\\_schema\\\\\\], function\\\\\\_call={\"name\": MultiSearch.openai\\\\\\_schema\\\\\\[\"name\"\\\\\\]}, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Consider the data below: '\\\\\\\\n{data}' and segment it into multiple search queries\", }, \\\\\\], max\\\\\\_tokens=1000, ) The segment function takes a string data and creates a completion. It prompts the model to segment the data into multiple search queries and returns the result as a MultiSearch object. Evaluating an Example¶ Let's evaluate an example by segmenting a search query and executing the segmented queries. import asyncio queries = segment(\"Please send me the video from last week about the investment case study and also documents about your GDPR policy?\") async def execute\\\\\\_queries(queries: Multisearch): await asyncio.gather(\\\\\\*\\\\\\[q.execute() for q in queries.tasks\\\\\\]) loop = asyncio.get\\\\\\_event\\\\\\_loop() loop.run\\\\\\_until\\\\\\_complete(execute\\\\\\_queries()) loop.close() In this example, we use the segment function to segment the search query. We then use asyncio to asynchronously execute the queries using the execute method defined in the Search class. The output will be: Searching for \\\\\\`Please send me the video from last week about the investment case study\\\\\\` with query \\\\\\`Please send me the video from last week about the investment case study\\\\\\` using \\\\\\`SearchType.VIDEO\\\\\\` Searching for \\\\\\`also documents about your GDPR policy?\\\\\\` with query \\\\\\`also documents about your GDPR policy?\\\\\\` using \\\\\\`SearchType.EMAIL\\\\\\` Was this page helpful? Back to top Previous Entity Resolution Next Query Decomposition Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Knowledge Graph - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/knowledge_graph/?q=",
    "html": "Skip to content Instructor Knowledge Graph Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Structures Generating Knowledge Graphs Visualizing the Graph Putting It All Together Visualizing Knowledge Graphs for Complex Topics¶ In this guide, you'll discover how to visualize a detailed knowledge graph for understanding complex topics, in this case, quantum mechanics. We leverage OpenAI's API and the Graphviz library to bring structure to intricate subjects. Motivation Knowledge graphs offer a visually appealing and coherent way to understand complicated topics like quantum mechanics. By generating these graphs automatically, you can accelerate the learning process and make it easier to digest complex information. Defining the Structures¶ Let's model a knowledge graph with Node and Edge objects. Node objects represent key concepts or entities, while Edge objects indicate the relationships between them. from pydantic import BaseModel, Field from typing import List class Node(BaseModel): id: int label: str color: str class Edge(BaseModel): source: int target: int label: str color: str = \"black\" class KnowledgeGraph(BaseModel): nodes: List\\\\\\[Node\\\\\\] = Field(..., default\\\\\\_factory=list) edges: List\\\\\\[Edge\\\\\\] = Field(..., default\\\\\\_factory=list) Generating Knowledge Graphs¶ The generate\\\\\\_graph function leverages OpenAI's API to generate a knowledge graph based on the input query. from openai import OpenAI import instructor # Adds response\\\\\\_model to ChatCompletion # Allows the return of Pydantic model rather than raw JSON client = instructor.patch(OpenAI()) def generate\\\\\\_graph(input) -> KnowledgeGraph: return client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Help me understand the following by describing it as a detailed knowledge graph: {input}\", } \\\\\\], response\\\\\\_model=KnowledgeGraph, ) # type: ignore Visualizing the Graph¶ The visualize\\\\\\_knowledge\\\\\\_graph function uses the Graphviz library to render the generated knowledge graph. from graphviz import Digraph def visualize\\\\\\_knowledge\\\\\\_graph(kg: KnowledgeGraph): dot = Digraph(comment=\"Knowledge Graph\") # Add nodes for node in kg.nodes: dot.node(str(node.id), node.label, color=node.color) # Add edges for edge in kg.edges: dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color) # Render the graph dot.render(\"knowledge\\\\\\_graph.gv\", view=True) Putting It All Together¶ Execute the code to generate and visualize a knowledge graph for understanding quantum mechanics. graph: KnowledgeGraph = generate\\\\\\_graph(\"Teach me about quantum mechanics\") visualize\\\\\\_knowledge\\\\\\_graph(graph) This will produce a visual representation of the knowledge graph, stored as \"knowledge\\\\\\_graph.gv\". You can open this file to explore the key concepts and their relationships in quantum mechanics. By leveraging automated knowledge graphs, you can dissect complex topics into digestible pieces, making the learning journey less daunting and more effective. Was this page helpful? Back to top Previous Citations Next Entity Resolution Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Citations - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/exact_citations/?q=",
    "html": "Skip to content Instructor Citations Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Overview Data Structures The Fact Class Validation Method: validate\\\\\\_sources The QuestionAnswer Class Validation Method: validate\\\\\\_sources Function to Ask AI a Question The ask\\\\\\_ai Function Example Example: Answering Questions with Validated Citations¶ For the full code example check out examples/citation\\\\\\_fuzzy\\\\\\_match.py Overview¶ This example shows how to use Instructor with validators to not only add citations to answers generated but also prevent hallucinations by ensuring that every statement made by the LLM is backed up by a direct quote from the context provided, and that those quotes exist!.Two Python classes, Fact and QuestionAnswer, are defined to encapsulate the information of individual facts and the entire answer, respectively. Data Structures¶ The Fact Class¶ The Fact class encapsulates a single statement or fact. It contains two fields: fact: A string representing the body of the fact or statement. substring\\\\\\_quote: A list of strings. Each string is a direct quote from the context that supports the fact. Validation Method: validate\\\\\\_sources¶ This method validates the sources (substring\\\\\\_quote) in the context. It utilizes regex to find the span of each substring quote in the given context. If the span is not found, the quote is removed from the list. from pydantic import Field, BaseModel, model\\\\\\_validator, FieldValidationInfo from typing import List class Fact(BaseModel): fact: str = Field(...) substring\\\\\\_quote: List\\\\\\[str\\\\\\] = Field(...) @model\\\\\\_validator(mode=\"after\") def validate\\\\\\_sources(self, info: FieldValidationInfo) -> \"Fact\": text\\\\\\_chunks = info.context.get(\"text\\\\\\_chunk\", None) spans = list(self.get\\\\\\_spans(text\\\\\\_chunks)) self.substring\\\\\\_quote = \\\\\\[text\\\\\\_chunks\\\\\\[span\\\\\\[0\\\\\\] : span\\\\\\[1\\\\\\]\\\\\\] for span in spans\\\\\\] return self def get\\\\\\_spans(self, context): for quote in self.substring\\\\\\_quote: yield from self.\\\\\\_get\\\\\\_span(quote, context) def \\\\\\_get\\\\\\_span(self, quote, context): for match in re.finditer(re.escape(quote), context): yield match.span() The QuestionAnswer Class¶ This class encapsulates the question and its corresponding answer. It contains two fields: question: The question asked. answer: A list of Fact objects that make up the answer. Validation Method: validate\\\\\\_sources¶ This method checks that each Fact object in the answer list has at least one valid source. If a Fact object has no valid sources, it is removed from the answer list. class QuestionAnswer(BaseModel): question: str = Field(...) answer: List\\\\\\[Fact\\\\\\] = Field(...) @model\\\\\\_validator(mode=\"after\") def validate\\\\\\_sources(self) -> \"QuestionAnswer\": self.answer = \\\\\\[fact for fact in self.answer if len(fact.substring\\\\\\_quote) > 0\\\\\\] return self Function to Ask AI a Question¶ The ask\\\\\\_ai Function¶ This function takes a string question and a string context and returns a QuestionAnswer object. It uses the OpenAI API to fetch the answer and then validates the sources using the defined classes. To understand the validation context work from pydantic check out pydantic's docs from openai import OpenAI import instructor # Apply the patch to the OpenAI client # enables response\\\\\\_model, validation\\\\\\_context keyword client = instructor.patch(OpenAI()) def ask\\\\\\_ai(question: str, context: str) -> QuestionAnswer: return client.chat.completions.create( model=\"gpt-3.5-turbo-0613\", temperature=0, response\\\\\\_model=QuestionAnswer, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"You are a world class algorithm to answer questions with correct and exact citations.\"}, {\"role\": \"user\", \"content\": f\"{context}\"}, {\"role\": \"user\", \"content\": f\"Question: {question}\"} \\\\\\], validation\\\\\\_context={\"text\\\\\\_chunk\": context}, ) Example¶ dd Here's an example of using these classes and functions to ask a question and validate the answer. question = \"What did the author do during college?\" context = \"\"\" My name is Jason Liu, and I grew up in Toronto Canada but I was born in China. I went to an arts high school but in university I studied Computational Mathematics and physics. As part of coop I worked at many companies including Stitchfix, Facebook. I also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years. \"\"\" The output would be a QuestionAnswer object containing validated facts and their sources. { \"question\": \"where did he go to school?\", \"answer\": \\\\\\[ { \"statement\": \"Jason Liu went to an arts highschool.\", \"substring\\\\\\_phrase\": \\\\\\[ \"arts highschool\" \\\\\\] }, { \"statement\": \"Jason Liu studied Computational Mathematics and physics in university.\", \"substring\\\\\\_phrase\": \\\\\\[ \"university\" \\\\\\] } \\\\\\] } This ensures that every piece of information in the answer has been validated against the context. Was this page helpful? Back to top Previous Moderation Next Knowledge Graph Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Image Extracting Tables - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/extracting_tables/?q=",
    "html": "Skip to content Instructor Image Extracting Tables Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Building the Custom Type for Markdown Tables Defining the Table Class Extracting Tables from Images Practical Example Top 10 Grossing Apps in October 2023 (Ireland) for Android Platforms Top 10 Grossing Apps in October 2023 (Ireland) for iOS Platforms Extracting Tables From Images¶ This post demonstrates how to use Python's type annotations and OpenAI's new vision model to extract tables from images and convert them into markdown format. This method is particularly useful for data analysis and automation tasks. The full code is available on GitHub Building the Custom Type for Markdown Tables¶ First, we define a custom type, MarkdownDataFrame, to handle pandas DataFrames formatted in markdown. This type uses Python's Annotated and InstanceOf types, along with decorators BeforeValidator and PlainSerializer, to process and serialize the data. from io import StringIO from typing import Annotated, Any from pydantic import BaseModel, Field, BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema import pandas as pd def md\\\\\\_to\\\\\\_df(data: Any) -> Any: # Convert markdown to DataFrame if isinstance(data, str): return ( pd.read\\\\\\_csv( StringIO(data), # Process data sep=\"|\", index\\\\\\_col=1, ) .dropna(axis=1, how=\"all\") .iloc\\\\\\[1:\\\\\\] .applymap(lambda x: x.strip()) ) return data MarkdownDataFrame = Annotated\\\\\\[ InstanceOf(pd.DataFrame), BeforeValidator(md\\\\\\_to\\\\\\_df), PlainSerializer(lambda df: df.to\\\\\\_markdown()), WithJsonSchema( { \"type\": \"string\", \"description\": \"The markdown representation of the table, each one should be tidy, do not try to join tables that should be seperate\", } ) \\\\\\] Defining the Table Class¶ The Table class is essential for organizing the extracted data. It includes a caption and a dataframe, processed as a markdown table. Since most of the complexity is handled by the MarkdownDataFrame type, the Table class is straightforward! class Table(BaseModel): caption: str dataframe: MarkdownDataFrame Extracting Tables from Images¶ The extract\\\\\\_table function uses OpenAI's vision model to process an image URL and extract tables in markdown format. We utilize the instructor library to patch the OpenAI client for this purpose. import instructor from openai import OpenAI # Apply the patch to the OpenAI client to support response\\\\\\_model # Also use MD\\\\\\_JSON mode since the visino model does not support any special structured output mode client = instructor.patch(OpenAI(), mode=instructor.function\\\\\\_calls.Mode.MD\\\\\\_JSON) def extract\\\\\\_table(url: str) -> Iterable\\\\\\[Table\\\\\\]: return client.chat.completions.create( model=\"gpt-4-vision-preview\", response\\\\\\_model=Iterable\\\\\\[Table\\\\\\], max\\\\\\_tokens=1800, messages=\\\\\\[ { \"role\": \"user\", \"content\": \\\\\\[ {\"type\": \"text\", \"text\": \"Extract table from image.\"}, {\"type\": \"image\\\\\\_url\", \"image\\\\\\_url\": {\"url\": url}} \\\\\\], } \\\\\\], ) Practical Example¶ In this example, we apply the method to extract data from an image showing the top grossing apps in Ireland for October 2023. url = \"https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031\\\\\\_uk-ireland-in-three-charts\\\\\\_table\\\\\\_v01\\\\\\_b.png\" tables = extract\\\\\\_table(url) for table in tables: print(table.caption, end=\"\\\\\\\\n\") print(table.dataframe) Expand to see the output Was this page helpful? Back to top Previous Self Critique Next Image to Ad Copy Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Self Critique - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/self_critique/?q=",
    "html": "Skip to content Instructor Self Critique Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Introduction Setup Defining Models Generating a Response Output Before Validation Adding Custom Validation Output After Validation Retrying with Corrections Final Output Self-Correction with llm\\\\\\_validator¶ Introduction¶ This guide demonstrates how to use llm\\\\\\_validator for implementing self-healing. The objective is to showcase how an instructor can self-correct by using validation errors and helpful error messages. Setup¶ Import required modules and apply compatibility patches. from typing\\\\\\_extensions import Annotated from pydantic import BaseModel, BeforeValidator Defining Models¶ Before building validation logic, define a basic Pydantic model named QuestionAnswer. We'll use this model to generate a response without validation to see the output. class QuestionAnswer(BaseModel): question: str answer: str Generating a Response¶ Here we coerce the model to generate a response that is objectionable. from openai import OpenAI import instructor # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) question = \"What is the meaning of life?\" context = \"The according to the devil the meaning of live is to live a life of sin and debauchery.\" qa: QuestionAnswer = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=QuestionAnswer, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: {context}\\\\\\\\n\\\\\\\\nAnswer the following question: {question}\", }, \\\\\\], ) Output Before Validation¶ While it calls out the objectionable content, it doesn't provide any details on how to correct it. { \"question\": \"What is the meaning of life?\", \"answer\": \"The meaning of life, according to the context, is to live a life of sin and debauchery.\" } Adding Custom Validation¶ By adding a validator to the answer field, we can try to catch the issue and correct it. Lets integrate llm\\\\\\_validator into the model and see the error message. Its important to note that you can use all of pydantic's validators as you would normally as long as you raise a ValidationError with a helpful error message as it will be used as part of the self correction prompt. class QuestionAnswerNoEvil(BaseModel): question: str answer: Annotated\\\\\\[ str, BeforeValidator( llm\\\\\\_validator(\"don't say objectionable things\", allow\\\\\\_override=True) ), \\\\\\] try: qa: QuestionAnswerNoEvil = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=QuestionAnswerNoEvil, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: {context}\\\\\\\\n\\\\\\\\nAnswer the following question: {question}\", }, \\\\\\], ) except Exception as e: print(e) Output After Validation¶ Now, we throw validation error that its objectionable and provide a helpful error message. 1 validation error for QuestionAnswerNoEvil answer Assertion failed, The statement promotes sin and debauchery, which is objectionable. Retrying with Corrections¶ By adding the max\\\\\\_retries parameter, we can retry the request with corrections. and use the error message to correct the output. qa: QuestionAnswerNoEvil = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=QuestionAnswerNoEvil, max\\\\\\_retries=1, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: {context}\\\\\\\\n\\\\\\\\nAnswer the following question: {question}\", }, \\\\\\], ) Final Output¶ Now, we get a valid response that is not objectionable! { \"question\": \"What is the meaning of life?\", \"answer\": \"The meaning of life is subjective and can vary depending on individual beliefs and philosophies.\" } Was this page helpful? Back to top Previous Text Classification Next Image Extracting Tables Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Moderation - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/moderation/?q=",
    "html": "Skip to content Instructor Moderation Type to start searching instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Overview Incorporating OpenAI moderation validator Testing OpenAI moderation validator OpenAI Moderation¶ Overview¶ This example uses OpenAI's moderation endpoint to check content compliance with OpenAI's usage policies. It can identify and filter harmful content that violates the policies. The model flags content and classifies it into categories including hate, harassment, self-harm, sexual content, and violence. Each category has subcategories for detailed classification. This validator is to be used for monitoring OpenAI API inputs and outputs, other use cases are currently not allowed. Incorporating OpenAI moderation validator¶ The following code defines a function to validate content using OpenAI's Moderation endpoint. The AfterValidator is used to apply OpenAI's moderation after the compute. This moderation checks if the content complies with OpenAI's usage policies and flags any harmful content. Here's how it works: Generate the OpenAI client and patch it with the instructor. Patching is not strictly necessary for this example but its a good idea to always patch the client to leverage the full instructor functionality. Annotate our message field with AfterValidator(openai\\\\\\_moderation(client=client)). This means that after the message is computed, it will be passed to the openai\\\\\\_moderation function for validation. import instructor from instructor import openai\\\\\\_moderation from typing\\\\\\_extensions import Annotated from pydantic import BaseModel, AfterValidator, ValidationError from openai import OpenAI client = instructor.patch(OpenAI()) class Response(BaseModel): message: Annotated\\\\\\[str, AfterValidator(openai\\\\\\_moderation(client=client))\\\\\\] Testing OpenAI moderation validator¶ Now, let's test our class with a piece of content that violates OpenAI's usage policies. Response(message=\"I want to make them suffer the consequences\") The validator will raise a ValidationError if the content violates the policies, like so: ValidationError: 1 validation error for Response message Value error, \\\\\\`I want to make them suffer the consequences\\\\\\` was flagged for harassment, harassment\\\\\\_threatening, violence, harassment/threatening \\\\\\[type=value\\\\\\_error, input\\\\\\_value='I want to make them suffer the consequences', input\\\\\\_type=str\\\\\\] Let's try another example which violates a different policy: self-harm. Response(message=\"I want to hurt myself.\") In this case, our validator will flag the output but return a different error message in the trace, clarifying the specific policies that were violated: ValidationError: 1 validation error for Response message Value error, I want to hurt myself was flagged for self\\\\\\_harm, self\\\\\\_harm\\\\\\_intent, violence, self-harm, self-harm/intent \\\\\\[type=value\\\\\\_error, input\\\\\\_value='I want to hurt myself', input\\\\\\_type=str\\\\\\] Was this page helpful? Back to top Previous Image to Ad Copy Next Citations Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Text Classification - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/classification/?q=",
    "html": "Skip to content Instructor Text Classification Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Single-Label Classification Defining the Structures Classifying Text Testing and Evaluation Multi-Label Classification Defining the Structures Classifying Text Testing and Evaluation Example: Text Classification using OpenAI and Pydantic¶ This tutorial showcases how to implement text classification tasks—specifically, single-label and multi-label classifications—using the OpenAI API, Python's enum module, and Pydantic models. Motivation Text classification is a common problem in many NLP applications, such as spam detection or support ticket categorization. The goal is to provide a systematic way to handle these cases using OpenAI's GPT models in combination with Python data structures. Single-Label Classification¶ Defining the Structures¶ For single-label classification, we first define an enum for possible labels and a Pydantic model for the output. import enum from pydantic import BaseModel class Labels(str, enum.Enum): \"\"\"Enumeration for single-label text classification.\"\"\" SPAM = \"spam\" NOT\\\\\\_SPAM = \"not\\\\\\_spam\" class SinglePrediction(BaseModel): \"\"\" Class for a single class label prediction. \"\"\" class\\\\\\_label: Labels Classifying Text¶ The function classify will perform the single-label classification. from openai import OpenAI import instructor # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def classify(data: str) -> SinglePrediction: \"\"\"Perform single-label classification on the input text.\"\"\" return client.chat.completions.create( model=\"gpt-3.5-turbo-0613\", response\\\\\\_model=SinglePrediction, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Classify the following text: {data}\", }, \\\\\\], ) # type: ignore Testing and Evaluation¶ Let's run an example to see if it correctly identifies a spam message. # Test single-label classification prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\") assert prediction.class\\\\\\_label == Labels.SPAM Multi-Label Classification¶ Defining the Structures¶ For multi-label classification, we introduce a new enum class and a different Pydantic model to handle multiple labels. # Define Enum class for multiple labels class MultiLabels(str, enum.Enum): TECH\\\\\\_ISSUE = \"tech\\\\\\_issue\" BILLING = \"billing\" GENERAL\\\\\\_QUERY = \"general\\\\\\_query\" # Define the multi-class prediction model class MultiClassPrediction(BaseModel): \"\"\" Class for a multi-class label prediction. \"\"\" class\\\\\\_labels: List\\\\\\[MultiLabels\\\\\\] Classifying Text¶ The function multi\\\\\\_classify is responsible for multi-label classification. def multi\\\\\\_classify(data: str) -> MultiClassPrediction: \"\"\"Perform multi-label classification on the input text.\"\"\" return client.chat.completions.create( model=\"gpt-3.5-turbo-0613\", response\\\\\\_model=MultiClassPrediction, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Classify the following support ticket: {data}\", }, \\\\\\], ) # type: ignore Testing and Evaluation¶ Finally, we test the multi-label classification function using a sample support ticket. # Test multi-label classification ticket = \"My account is locked and I can't access my billing info.\" prediction = multi\\\\\\_classify(ticket) assert MultiLabels.TECH\\\\\\_ISSUE in prediction.class\\\\\\_labels assert MultiLabels.BILLING in prediction.class\\\\\\_labels Was this page helpful? Back to top Previous Overview Next Self Critique Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Type Adapter - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/typeadapter/?q=",
    "html": "Instructor Type Adapter Type to start searching instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Type Adapter This page is a work in progress This page is a work in progress. Check out Pydantic's documentation Was this page helpful? Back to top Previous Alias Next Overview Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Union - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/union/?q=",
    "html": "Skip to content Instructor Union Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Unions for Multiple Types Union Pydantic models also support Union types, which are used to represent a value that can be one of several types. While many libraries support multiple function calls, and tool calls support multiple returns, the goal is to provide only one way to do things. Unions for Multiple Types¶ You can use Union types to write agents that can dynamically choose actions - by choosing an output class. For example, in a search and lookup function, the LLM can determine whether to execute another search, lookup or other action. class Search(BaseModel): query: str def execute(self): return ... class Lookup(BaseModel): key: str def execute(self): return ... class Action(BaseModel): action: Union\\\\\\[Search, Lookup\\\\\\] def execute(self): return self.action.execute() See 'examples/union/run.py' for a working example. Was this page helpful? Back to top Previous Types Next Alias Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Alias - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/alias/?q=",
    "html": "Instructor Alias Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Alias This page is a work in progress This page is a work in progress. Check out Pydantic's documentation Was this page helpful? Back to top Previous Union Next Type Adapter Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Distillation - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/distillation/?q=",
    "html": "Skip to content Instructor Distillation Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents The Challenges in Function-Level Fine-Tuning The Role of Instructions in Simplifying the Fine-Tuning Process How to Implement Instructions in Your Code Quick Start: How to Use Instructor's Distillation Feature The Intricacies of Fine-tuning Language Models Why Instructor and Distillation are Game Changers Role of Instructor in Simplifying Fine-Tuning Logging Output and Running a Finetune Distilling python functions into LLM¶ Instructions from the Instructor library offers a seamless way to make language models backward compatible with existing Python functions. By employing Pydantic type hints, it not only ensures compatibility but also facilitates fine-tuning gpt-3.5-turbo to emulate these functions end-to-end. If you want to see the full example checkout examples/distillation The Challenges in Function-Level Fine-Tuning¶ Replicating the behavior of a Python function in a language model involves intricate data preparation. For instance, teaching a model to execute three-digit multiplication is not as trivial as implementing def f(a, b): return a \\\\\\* b. OpenAI's fine-tuning script coupled with their function calling utility provides a structured output, thereby simplifying the data collection process. Additionally, this eliminates the need for passing the schema to the model, thus conserving tokens. The Role of Instructions in Simplifying the Fine-Tuning Process¶ By using Instructions, you can annotate a Python function that returns a Pydantic object, thereby automating the dataset creation for fine-tuning. A handler for logging is all that's needed to build this dataset. How to Implement Instructions in Your Code¶ Quick Start: How to Use Instructor's Distillation Feature¶ Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file. import logging import random from pydantic import BaseModel from instructor import Instructions # pip install instructor # Logging setup logging.basicConfig(level=logging.INFO) instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"math\\\\\\_finetunes.jsonl\")\\\\\\] ) class Multiply(BaseModel): a: int b: int result: int # Define a function with distillation # The decorator will automatically generate a dataset for fine-tuning # They must return a pydantic model to leverage function calling @instructions.distil def fn(a: int, b: int) -> Multiply: resp = a \\\\\\* b return Multiply(a=a, b=b, result=resp) # Generate some data for \\\\\\_ in range(10): a = random.randint(100, 999) b = random.randint(100, 999) print(fn(a, b)) The Intricacies of Fine-tuning Language Models¶ Fine-tuning isn't just about writing a function like def f(a, b): return a \\\\\\* b. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this. Why Instructor and Distillation are Game Changers¶ The library offers two main benefits: Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code. Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions. Role of Instructor in Simplifying Fine-Tuning¶ The from instructor import Instructions feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior. Logging Output and Running a Finetune¶ Here's how the logging output would look: { \"messages\": \\\\\\[ {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'}, {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'}, {\"role\": \"assistant\", \"function\\\\\\_call\": { \"name\": \"Multiply\", \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}' } } \\\\\\], \"functions\": \\\\\\[ {\"name\": \"Multiply\", \"description\": \"Correctly extracted \\\\\\`Multiply\\\\\\`...\"} \\\\\\] } Run a finetune like this: instructor jobs create-from-file math\\\\\\_finetunes.jsonl Once a model is trained you can simply change mode to dispatch and it will use the model to run the function! from instructor import Instructions instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", ) @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") def fn(a: int, b: int) -> Multiply: # now this code will be short circuited and the model will be used instead. resp = a + b return Multiply(a=a, b=b, result=resp) With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation. Was this page helpful? Back to top Previous Validators Next Types Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Types - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/types/?q=",
    "html": "Instructor Types Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Types This page is a work in progress This page is a work in progress. Check out Pydantic's documentation Was this page helpful? Back to top Previous Distillation Next Union Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Validators - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/reask_validation/?q=",
    "html": "Skip to content Instructor Validators Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Pydantic Code-based Validation Example Output for Code-Based Validation LLM-Based Validation Example Output for LLM-Based Validation Using Reasking Logic to Correct Outputs Step 1: Define the Response Model with Validators Step 2. Using the Client with Retries What happens behind the scenes? Advanced Validation Techniques Takeaways Validation and Reasking¶ Instead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the system can use to self-correct. Pydantic¶ Pydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic docs on validators. Good llm validation is just good validation If you want to see some more examples on validators checkout our blog post Good LLM validation is just good validation Code-based Validation Example¶ First define a Pydantic model with a validator using the Annotation class from typing\\\\\\_extensions. Enforce a naming rule using Pydantic's built-in validation: from pydantic import BaseModel, ValidationError from typing\\\\\\_extensions import Annotated from pydantic import AfterValidator def name\\\\\\_must\\\\\\_contain\\\\\\_space(v: str) -> str: if \" \" not in v: raise ValueError(\"Name must contain a space.\") return v.lower() class UserDetail(BaseModel): age: int name: Annotated\\\\\\[str, AfterValidator(name\\\\\\_must\\\\\\_contain\\\\\\_space)\\\\\\] try: person = UserDetail(age=29, name=\"Jason\") except ValidationError as e: print(e) Output for Code-Based Validation¶ 1 validation error for UserDetail name Value error, name must contain a space (type=value\\\\\\_error) As we can see, Pydantic raises a validation error when the name attribute does not contain a space. This is a simple example, but it demonstrates how Pydantic can be used to validate attributes of a model. LLM-Based Validation Example¶ LLM-based validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error. import instructor from openai import OpenAI from instructor import llm\\\\\\_validator from pydantic import BaseModel, ValidationError, BeforeValidator from typing\\\\\\_extensions import Annotated # Apply the patch to the OpenAI client client = instructor.patch(OpenAI()) class QuestionAnswer(BaseModel): question: str answer: Annotated\\\\\\[ str, BeforeValidator(llm\\\\\\_validator(\"don't say objectionable things\", openai\\\\\\_client=client)) \\\\\\] try: qa = QuestionAnswer( question=\"What is the meaning of life?\", answer=\"The meaning of life is to be evil and steal\", ) except ValidationError as e: print(e) Output for LLM-Based Validation¶ It is important to not here that the error message is generated by the LLM, not the code, so it'll be helpful for re asking the model. 1 validation error for QuestionAnswer answer Assertion failed, The statement is objectionable. (type=assertion\\\\\\_error) Using Reasking Logic to Correct Outputs¶ Validators are a great tool for ensuring some property of the outputs. When you use the patch() method with the openai client, you can use the max\\\\\\_retries parameter to set the number of times you can reask the model to correct the output. It is a great layer of defense against bad outputs of two forms: 1. Pydantic Validation Errors (code or llm based) 2. JSON Decoding Errors (when the model returns a bad response) Step 1: Define the Response Model with Validators¶ Notice that the field validator wants the name in uppercase, but the user input is lowercase. The validator will raise a ValueError if the name is not in uppercase. import instructor from pydantic import BaseModel, field\\\\\\_validator # Apply the patch to the OpenAI client client = instructor.patch(OpenAI()) class UserDetails(BaseModel): name: str age: int @field\\\\\\_validator(\"name\") @classmethod def validate\\\\\\_name(cls, v): if v.upper() != v: raise ValueError(\"Name must be in uppercase.\") return v Step 2. Using the Client with Retries¶ Here, the UserDetails model is passed as the response\\\\\\_model, and max\\\\\\_retries is set to 2. model = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetails, max\\\\\\_retries=2, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"}, \\\\\\], ) assert model.name == \"JASON\" What happens behind the scenes?¶ Behind the scenes, the instructor.patch() method adds a max\\\\\\_retries parameter to the openai.ChatCompletion.create() method. The max\\\\\\_retries parameter will trigger up to 2 reattempts if the name attribute fails the uppercase validation in UserDetails. try: ... except (ValidationError, JSONDecodeError) as e: kwargs\\\\\\[\"messages\"\\\\\\].append(response.choices\\\\\\[0\\\\\\].message) kwargs\\\\\\[\"messages\"\\\\\\].append( { \"role\": \"user\", \"content\": f\"Please correct the function call; errors encountered:\\\\\\\\n{e}\", } ) Advanced Validation Techniques¶ The docs are currently incomplete, but we have a few advanced validation techniques that we're working on documenting better such as model level validation, and using a validation context. Check out our example on verifying citations which covers: 1. Validate the entire object with all attributes rather than one attribute at a time 2. Using some 'context' to validate the object: In this case, we use the context to check if the citation existed in the original text. Takeaways¶ By integrating these advanced validation techniques, we not only improve the quality and reliability of LLM-generated content, but also pave the way for more autonomous and effective systems. Was this page helpful? Back to top Previous Caching Next Distillation Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Caching - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/caching/?q=",
    "html": "Skip to content Instructor Caching Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents 1. functools.cache for Simple In-Memory Caching 2. diskcache for Persistent, Large Data Caching 2. Redis Caching Decorator for Distributed Systems Caching If you want to learn more about concepts in caching and how to use them in your own projects, check out our blog on the topic. 1. functools.cache for Simple In-Memory Caching¶ When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session. or in an application where we don't need to persist the cache between sessions. import functools import instructor from openai import OpenAI client = instructor.patch(OpenAI()) class UserDetail(BaseModel): name: str age: int @functools.cache def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Changing the Model does not Invalidate the Cache Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result. Now we can call extract multiple times with the same argument, and the result will be cached in memory for faster access. import time start = time.perf\\\\\\_counter() # Using time.perf\\\\\\_counter() to measure the time taken to run the function is better than using time.time() because it's more accurate and less susceptible to system clock changes. model = extract(\"Extract jason is 25 years old\") print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") start = time.perf\\\\\\_counter() model = extract(\"Extract jason is 25 years old\") # The second time we call extract, the result is returned from the cache, and the function is not called. print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") >>> Time taken: 0.9267581660533324 >>> Time taken: 1.2080417945981026e-06 # The second call to extract is much faster because the result is returned from the cache! Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries. What is a decorator? 2. diskcache for Persistent, Large Data Caching¶ Copy Caching Code When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data! import functools import inspect import instructor import diskcache from openai import OpenAI from pydantic import BaseModel client = instructor.patch(OpenAI()) cache = diskcache.Cache('./my\\\\\\_cache\\\\\\_directory') def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation # We use inspect.signature to get the function's return type annotation, which we use to validate the cached result. if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type We use Pydantic's model\\\\\\_validate\\\\\\_json to deserialize the cached result into a Pydantic model. return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence. 2. Redis Caching Decorator for Distributed Systems¶ Copy Caching Code When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures. import redis import functools import inspect import json import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) cache = redis.Redis(\"localhost\") def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: # Assuming client.chat.completions.create returns a UserDetail instance return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types. Looking carefully If you look carefully at the code above you'll notice that we're using the same instructor\\\\\\_cache decorator as before. The implementation is the same, but we're using a different caching backend! Was this page helpful? Back to top Previous FastAPI Next Validators Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "FastAPI - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/fastapi/?q=",
    "html": "Skip to content Instructor FastAPI Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Why Choose FastAPI and Pydantic? Code Example: Starting a FastAPI App with a POST Request Streaming Responses with FastAPI Automatic Documentation with FastAPI Integrating Pydantic Models with FastAPI¶ FastAPI is an enjoyable tool for building web applications in Python. It is well known for its integration with Pydantic models, which makes defining and validating data structures straightforward and efficient. In this guide, we explore how simple functions that return Pydantic models can seamlessly integrate with FastAPI. Why Choose FastAPI and Pydantic?¶ FastAPI is a modern, high-performance web framework for building APIs with Python. Supports OpenAPI and JSON Schema for automatic documentation and validation. Supports AsyncIO for asynchronous programming leveraging the AsyncOpenAI() client Code Example: Starting a FastAPI App with a POST Request¶ The following code snippet demonstrates how to start a FastAPI app with a POST endpoint. This endpoint accepts and returns data defined by a Pydantic model. import instructor from fastapi import FastAPI from pydantic import BaseModel from openai import AsyncOpenAI # Enables response\\\\\\_model client = instructor.patch(AsyncOpenAI()) app = FastAPI() class UserData(BaseModel): # This can be the model for the input data query: str class UserDetail(BaseModel): name: str age: int @app.post(\"/endpoint\", response\\\\\\_model=UserDetail) def endpoint\\\\\\_function(data: UserData) -> UserDetail: user\\\\\\_detail = await client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": f\"Extract: \\\\\\`{data.query}\\\\\\`\"}, \\\\\\] ) return user\\\\\\_detail Streaming Responses with FastAPI¶ FastAPI supports streaming responses, which is useful for returning large amounts of data. This feature is particularly useful when working with large language models (LLMs) that generate a large amount of data. from fastapi.responses import StreamingResponse from typing import Iterable # Route to handle SSE events and return users @app.post(\"/extract\", response\\\\\\_class=StreamingResponse) async def extract(data: UserData): users = await client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=Iterable\\\\\\[UserDetail\\\\\\], stream=True, messages=\\\\\\[ {\"role\": \"user\", \"content\": data.query}, \\\\\\] ) async def generate(): for user in users: resp\\\\\\_json = user.model\\\\\\_dump\\\\\\_json() yield f\"data: {resp\\\\\\_json}\" yield \"data: \\\\\\[DONE\\\\\\]\" return StreamingResponse(generate(), media\\\\\\_type=\"text/event-stream\") Automatic Documentation with FastAPI¶ FastAPI leverages the OpenAPI specification to automatically generate a dynamic and interactive documentation page, commonly referred to as the /docs page. This feature is incredibly useful for developers, as it offers a live environment to test API endpoints directly through the browser. To explore the capabilities of your API, follow these steps: Run the API using the Uvicorn command: uvicorn main:app --reload. Open your web browser and navigate to http://127.0.0.1:8000/docs. You will find an interactive UI where you can send different requests to your API and see the responses in real-time. Was this page helpful? Back to top Previous Streaming Next Caching Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Streaming - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/lists/?q=",
    "html": "Skip to content Instructor Streaming Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Extracting Tasks using Iterable Streaming Tasks Multi-task and Streaming¶ A common use case of structured extraction is defining a single schema class and then making another schema to create a list to do multiple extraction from pydantic import BaseModel class User(BaseModel): name: str age: int class Users(BaseModel): users: List\\\\\\[User\\\\\\] Defining a task and creating a list of classes is a common enough pattern that we make this convenient by making use of Iterable\\\\\\[T\\\\\\]. This lets us dynamically create a new class that: Has dynamic docstrings and class name based on the task Support streaming by collecting tokens until a task is received back out. Extracting Tasks using Iterable¶ By using Iterable you get a very convenient class with prompts and names automatically defined: import instructor from openai import OpenAI from typing import Iterable from pydantic import BaseModel client = instructor.patch(OpenAI(), mode=instructor.function\\\\\\_calls.Mode.JSON) class User(BaseModel): name: str age: int Users = Iterable\\\\\\[User\\\\\\] users = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Users, stream=False, messages=\\\\\\[ { \"role\": \"user\", \"content\": \"Consider this data: Jason is 10 and John is 30.\\\\\\\\ Correctly segment it into entitites\\\\\\\\ Make sure the JSON is correct\", }, \\\\\\], ) for user in users: assert isinstance(user, User) print(user) >>> name=\"Jason\" \"age\"=10 >>> name=\"John\" \"age\"=10 Streaming Tasks¶ We can also generate tasks as the tokens are streamed in by defining an Iterable\\\\\\[T\\\\\\] type. Lets look at an example in action with the same class from typing import Iterable Users = Iterable\\\\\\[User\\\\\\] users = client.chat.completions.create( model=\"gpt-4\", temperature=0.1, stream=True, response\\\\\\_model=Users, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a perfect entity extraction system\", }, { \"role\": \"user\", \"content\": ( f\"Consider the data below:\\\\\\\\n{input}\" \"Correctly segment it into entitites\" \"Make sure the JSON is correct\" ), }, \\\\\\], max\\\\\\_tokens=1000, ) for user in users: assert isinstance(user, User) print(user) >>> name=\"Jason\" \"age\"=10 >>> name=\"John\" \"age\"=10 This streaming is still a prototype, but should work quite well for simple schemas. Was this page helpful? Back to top Previous Patching Next FastAPI Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Patching - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/patching/?q=",
    "html": "Skip to content Instructor Patching Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Function Calling Tool Calling JSON Mode Markdown JSON Mode Schema Integration Patching¶ Instructor enhances client functionality with three new keywords for backwards compatibility. This allows use of the enhanced client as usual, with structured output benefits. response\\\\\\_model: Defines the response type for chat.completions.create. max\\\\\\_retries: Determines retry attempts for failed chat.completions.create validations. validation\\\\\\_context: Provides extra context to the validation process. There are three methods for structured output: Function Calling: The primary method. Use this for stability and testing. Tool Calling: Useful in specific scenarios; lacks the reasking feature of OpenAI's tool calling API. JSON Mode: Offers closer adherence to JSON but with more potential validation errors. Suitable for specific non-function calling clients. Function Calling¶ from openai import OpenAI import instructor client = instructor.patch(OpenAI()) Tool Calling¶ import instructor from instructor import Mode client = instructor.patch(OpenAI(), mode=Mode.TOOLS) JSON Mode¶ import instructor from instructor import Mode from openai import OpenAI client = instructor.patch(OpenAI(), mode=Mode.JSON) Markdown JSON Mode¶ Experimental This is not recommended, and may not be supported in the future, this is just left to support vision models. import instructor from instructor import Mode from openai import OpenAI client = instructor.patch(OpenAI(), mode=Mode.MD\\\\\\_JSON) Schema Integration¶ In JSON Mode, the schema is part of the system message: import instructor from openai import OpenAI client = instructor.patch(OpenAI()) response = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", response\\\\\\_format={\"type\": \"json\\\\\\_object\"}, messages=\\\\\\[ { \"role\": \"system\", \"content\": f\"Match your response to this json\\\\\\_schema: \\\\\\\\n{UserExtract.model\\\\\\_json\\\\\\_schema()\\\\\\['properties'\\\\\\]}\", }, { \"role\": \"user\", \"content\": \"Extract jason is 25 years old\", }, \\\\\\], ) user = UserExtract.from\\\\\\_response(response, mode=Mode.JSON) assert user.name.lower() == \"jason\" assert user.age == 25 Was this page helpful? Back to top Previous Missing Next Streaming Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Missing - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/maybe/?q=",
    "html": "Skip to content Instructor Missing Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Defining the Model Defining the function Handling the result Pattern Matching Handling Missing Data¶ The Maybe pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning None, you can use a Maybe type to encapsulate both the result and potential errors. This pattern is particularly useful when making LLM calls, as providing language models with an escape hatch can effectively reduce hallucinations. Defining the Model¶ Using Pydantic, we'll first define the UserDetail and MaybeUser classes. from pydantic import BaseModel, Field, Optional class UserDetail(BaseModel): age: int name: str role: Optional\\\\\\[str\\\\\\] = Field(default=None) class MaybeUser(BaseModel): result: Optional\\\\\\[UserDetail\\\\\\] = Field(default=None) error: bool = Field(default=False) message: Optional\\\\\\[str\\\\\\] = Field(default=None) def \\\\\\_\\\\\\_bool\\\\\\_\\\\\\_(self): return self.result is not None Notice that MaybeUser has a result field that is an optional UserDetail instance where the extracted data will be stored. The error field is a boolean that indicates whether an error occurred, and the message field is an optional string that contains the error message. Defining the function¶ Once we have the model defined, we can create a function that uses the Maybe pattern to extract the data. import random import instructor from openai import OpenAI from typing import Optional # This enables the \\\\\\`response\\\\\\_model\\\\\\` keyword client = instructor.patch(OpenAI()) def extract(content: str) -> MaybeUser: return openai.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=MaybeUser, messages=\\\\\\[ {\"role\": \"user\", \"content\": f\"Extract \\\\\\`{content}\\\\\\`\"}, \\\\\\], ) user1 = extract(\"Jason is a 25-year-old scientist\") # output: { \"result\": { \"age\": 25, \"name\": \"Jason\", \"role\": \"scientist\" }, \"error\": false, \"message\": null } user2 = extract(\"Unknown user\") # output: { \"result\": null, \"error\": true, \"message\": \"User not found\" } As you can see, when the data is extracted successfully, the result field contains the UserDetail instance. When an error occurs, the error field is set to True, and the message field contains the error message. Handling the result¶ There are a few ways we can handle the result. Normally, we can just access the individual fields. def process\\\\\\_user\\\\\\_detail(maybe\\\\\\_user: MaybeUser): if not maybe\\\\\\_user.error: user = maybe\\\\\\_user.result print(f\"User {user.name} is {user.age} years old\") else: print(f\"Not found: {user1.message}\") Pattern Matching¶ We can also use pattern matching to handle the result. This is a great way to handle errors in a structured way. def process\\\\\\_user\\\\\\_detail(maybe\\\\\\_user: MaybeUser): match maybe\\\\\\_user: case MaybeUser(error=True, message=msg): print(f\"Error: {msg}\") case MaybeUser(result=user\\\\\\_detail) if user\\\\\\_detail: assert isinstance(user\\\\\\_detail, UserDetail) print(f\"User {user\\\\\\_detail.name} is {user\\\\\\_detail.age} years old\") case \\\\\\_: print(\"Unknown error\") If you want to learn more about pattern matching, check out Pydantic's docs on Structural Pattern Matching Was this page helpful? Back to top Previous Fields Next Patching Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Fields - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/fields/?q=",
    "html": "Skip to content Instructor Fields Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Default values Using Annotated Exclude Customizing JSON Schema General notes on JSON schema generation Fields The pydantic.Field function is used to customize and add metadata to fields of models. To learn more, check out the Pydantic documentation as this is a near replica of that documentation that is relevant to prompting. Default values¶ The default parameter is used to define a default value for a field. from pydantic import BaseModel, Field class User(BaseModel): name: str = Field(default='John Doe') user = User() print(user) #> name='John Doe' You can also use default\\\\\\_factory to define a callable that will be called to generate a default value. from uuid import uuid4 from pydantic import BaseModel, Field class User(BaseModel): id: str = Field(default\\\\\\_factory=lambda: uuid4().hex) Info The default and default\\\\\\_factory parameters are mutually exclusive. Note If you use typing.Optional, it doesn't mean that the field has a default value of None you must use default or default\\\\\\_factory to define a default value. Then it will be considered not required when sent to the language model. Using Annotated¶ The Field function can also be used together with Annotated. from uuid import uuid4 from typing\\\\\\_extensions import Annotated from pydantic import BaseModel, Field class User(BaseModel): id: Annotated\\\\\\[str, Field(default\\\\\\_factory=lambda: uuid4().hex)\\\\\\] Exclude¶ The exclude parameter can be used to control which fields should be excluded from the model when exporting the model. This is helpful when you want to exclude fields that are not relevant to the model generation like scratch\\\\\\_pad or chain\\\\\\_of\\\\\\_thought See the following example: from pydantic import BaseModel, Field from datetime import date class DateRange(BaseModel): chain\\\\\\_of\\\\\\_thought: str = Field( description=\"Reasoning behind the date range.\" exclude=True) start\\\\\\_date: date end\\\\\\_date: date date\\\\\\_range = DateRange( chain\\\\\\_of\\\\\\_thought=\"\"\" I want to find the date range for the last 30 days. Today is 2021-01-30 therefore the start date should be 2021-01-01 and the end date is 2021-01-30\"\"\", start\\\\\\_date=date(2021, 1, 1), end\\\\\\_date=date(2021, 1, 30), ) print(date\\\\\\_range.model\\\\\\_dump\\\\\\_json()) #> start\\\\\\_date=datetime.date(2021, 1, 1) end\\\\\\_date=datetime.date(2021, 1, 30) Customizing JSON Schema¶ There are some fields that are exclusively used to customise the generated JSON Schema: title: The title of the field. description: The description of the field. examples: The examples of the field. json\\\\\\_schema\\\\\\_extra: Extra JSON Schema properties to be added to the field. These all work as great opportunities to add more information to the JSON schema as part of your prompt engineering. Here's an example: from pydantic import BaseModel, EmailStr, Field, SecretStr class User(BaseModel): age: int = Field(description='Age of the user') email: EmailStr = Field(examples=\\\\\\['marcelo@mail.com'\\\\\\]) name: str = Field(title='Username') password: SecretStr = Field( json\\\\\\_schema\\\\\\_extra={ 'title': 'Password', 'description': 'Password of the user', 'examples': \\\\\\['123456'\\\\\\], } ) print(User.model\\\\\\_json\\\\\\_schema()) \"\"\" { 'properties': { 'age': { 'description': 'Age of the user', 'title': 'Age', 'type': 'integer', }, 'email': { 'examples': \\\\\\['marcelo@mail.com'\\\\\\], 'format': 'email', 'title': 'Email', 'type': 'string', }, 'name': {'title': 'Username', 'type': 'string'}, 'password': { 'description': 'Password of the user', 'examples': \\\\\\['123456'\\\\\\], 'format': 'password', 'title': 'Password', 'type': 'string', 'writeOnly': True, }, }, 'required': \\\\\\['age', 'email', 'name', 'password'\\\\\\], 'title': 'User', 'type': 'object', } \"\"\" General notes on JSON schema generation¶ The JSON schema for Optional fields indicates that the value null is allowed. The Decimal type is exposed in JSON schema (and serialized) as a string. The JSON schema does not preserve namedtuples as namedtuples. When they differ, you can specify whether you want the JSON schema to represent the inputs to validation or the outputs from serialization. Sub-models used are added to the $defs JSON attribute and referenced, as per the spec. Sub-models with modifications (via the Field class) like a custom title, description, or default value, are recursively included instead of referenced. The description for models is taken from either the docstring of the class or the argument description to the Field class. Was this page helpful? Back to top Previous Models Next Missing Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Models - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/models/?q=",
    "html": "Skip to content Instructor Models Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Prompting Optional Values Dynamic model creation Structural Pattern Matching Adding Behavior Response Model¶ Defining LLM output schemas in Pydantic is done via pydantic.BaseModel. To learn more about models in Pydantic, check out their documentation. After defining a Pydantic model, we can use it as the response\\\\\\_model in your client create calls to OpenAI or any other supported model. The job of the response\\\\\\_model parameter is to: - Define the schema and prompts for the language model - Validate the response from the API - Return a Pydantic model instance. Prompting¶ When defining a response model, we can use docstrings and field annotations to define the prompt that will be used to generate the response. from pydantic import BaseModel, Field class User(BaseModel): \"\"\" This is the prompt that will be used to generate the response. Any instructions here will be passed to the language model. \"\"\" name: str = Field(description=\"The name of the user.\") age: int = Field(description=\"The age of the user.\") Here all docstrings, types, and field annotations will be used to generate the prompt. The prompt will be generated by the create method of the client and will be used to generate the response. Optional Values¶ If we use Optional and default, they will be considered not required when sent to the language model class User(BaseModel): name: str = Field(description=\"The name of the user.\") age: int = Field(description=\"The age of the user.\") email: Optional\\\\\\[str\\\\\\] = Field(description=\"The email of the user.\", default=None) Dynamic model creation¶ There are some occasions where it is desirable to create a model using runtime information to specify the fields. For this, Pydantic provides the create\\\\\\_model function to allow models to be created on the fly: from pydantic import BaseModel, create\\\\\\_model class FooModel(BaseModel): foo: str bar: int = 123 BarModel = create\\\\\\_model( 'BarModel', apple=(str, 'russet'), banana=(str, 'yellow'), \\\\\\_\\\\\\_base\\\\\\_\\\\\\_=FooModel, ) print(BarModel) #> print(BarModel.model\\\\\\_fields.keys()) #> dict\\\\\\_keys(\\\\\\['foo', 'bar', 'apple', 'banana'\\\\\\]) When would I use this? Structural Pattern Matching¶ Pydantic supports structural pattern matching for models, as introduced by PEP 636 in Python 3.10. from pydantic import BaseModel class Pet(BaseModel): name: str species: str a = Pet(name='Bones', species='dog') match a: # match \\\\\\`species\\\\\\` to 'dog', declare and initialize \\\\\\`dog\\\\\\_name\\\\\\` case Pet(species='dog', name=dog\\\\\\_name): print(f'{dog\\\\\\_name} is a dog') #> Bones is a dog # default case case \\\\\\_: print('No dog matched') Adding Behavior¶ We can add methods to our Pydantic models, just as any plain Python class. We might want to do this to add some custom logic to our models. from pydantic import BaseModel from typing import Literal from openai import OpenAI import instructor client = instructor.patch(OpenAI()) class SearchQuery(BaseModel): query: str query\\\\\\_type: Literal\\\\\\[\"web\", \"image\", \"video\"\\\\\\] def execute(self): # do some logic here return results query = client.chat.completions.create( ..., response\\\\\\_model=SearchQuery ) results = query.execute() Now we can call execute on our model instance after extracting it from a language model. If you want to see more examples of this checkout our post on RAG is more than embeddings Was this page helpful? Back to top Previous Philosophy Next Fields Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Contributing - Instructor",
    "url": "https://jxnl.github.io/instructor/contributing/?q=",
    "html": "Skip to content Instructor Contributing Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Introduction Why use Instructor? Help with Instructor Installation Contributing Table of contents Evals Issues Pull Requests Contributors Additional Resources We would love for you to contribute to Instructor. Evals¶ We invite you to contribute evals in pytest as a way to monitor the quality of the openai models and the instructor library. To get started check out the jxnl/instructor/tests/evals and contribute your own evals in the form of pytest tests. These evals will be run once a week and the results will be posted. Issues¶ If you find a bug, please file an issue on our issue tracker on GitHub. To help us reproduce the bug, please provide a minimal reproducible example, including a code snippet and the full error message. The response\\\\\\_model you are using. The messages you are using. The model you are using. Pull Requests¶ We welcome pull requests! There is plenty to do, and we are happy to discuss any contributions you would like to make. If it is not a small change, please start by filing an issue first. If you need ideas, you can check out the help wanted or good first issue labels. Grit is used to enforce best practices. You can run grit check to check your code before submitting a pull request. Contributors¶ Additional Resources¶ To enhance your understanding of the documentation, here are some useful references: mkdocs serve: The mkdocs serve command is used to preview your documentation locally during the development phase. When you run this command in your terminal, MkDocs starts a development server, allowing you to view and interact with your documentation in a web browser. This is helpful for checking how your changes look before publishing the documentation. Learn more in the mkdocs serve documentation. hl\\\\\\_lines in Code Blocks: The hl\\\\\\_lines feature in code blocks allows you to highlight specific lines within the code block. This is useful for drawing attention to particular lines of code when explaining examples or providing instructions. You can specify the lines to highlight using the hl\\\\\\_lines option in your code block configuration. For more details and examples, you can refer to the hl\\\\\\_lines documentation. Admonitions: Admonitions are a way to visually emphasize or call attention to certain pieces of information in your documentation. They come in various styles, such as notes, warnings, tips, etc. Admonitions provide a structured and consistent way to present important content. For usage examples and details on incorporating admonitions into your documentation, you can refer to the admonitions documentation. For more details about the documentation structure and features, refer to the MkDocs Material documentation. Thank you for your contributions, and happy coding! Was this page helpful? Back to top Previous Installation Next Tips Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Installation - Instructor",
    "url": "https://jxnl.github.io/instructor/installation/?q=",
    "html": "Instructor Installation Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Introduction Why use Instructor? Help with Instructor Installation Contributing Installation Installation is as simple as: pip install instructor Instructor has a few dependencies: openai: OpenAI's Python client. typer: Build great CLIs. Easy to code. Based on Python type hints. docstring-parser: A parser for Python docstrings, to improve the experience of working with docstrings in jsonschema. pydantic: Data validation and settings management using python type annotations. If you've got Python 3.9+ and pip installed, you're good to go. Was this page helpful? Back to top Previous Help with Instructor Next Contributing Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Welcome to the Instructor Blog - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/page/2/",
    "html": "Skip to content Instructor Welcome to the Instructor Blog Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Blog Archive 2023 Table of contents Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls Welcome to the Instructor Blog¶ 2023/09/11 2 min read Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls Language models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic. The Problem with Existing LLM Frameworks Current frameworks for Language Learning Models (LLMs) have complex setups. Developers find it hard to control interactions with language models. Some frameworks require complex JSON Schema setups. The OpenAI Function Calling Game-Changer OpenAI's Function Calling feature provides a constrained interaction model. However, it has its own complexities, mostly around JSON Schema. Why Pydantic? Instructor uses Pydantic to simplify the interaction between the programmer and the language model. Widespread Adoption: Pydantic is a popular tool among Python developers. Simplicity: Pydantic allows model definition in Python. Framework Compatibility: Many Python frameworks already use Pydantic. import pydantic import instructor from openai import OpenAI # Enables the response\\\\\\_model client = instructor.patch(OpenAI()) class UserDetail(pydantic.BaseModel): name: str age: int def introduce(self): return f\"Hello I'm {self.name} and I'm {self.age} years old\" user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] ) Simplifying Validation Flow with Pydantic Pydantic validators simplify features like re-asking or self-critique. This makes these tasks less complex compared to other frameworks. from typing\\\\\\_extensions import Annotated from pydantic import BaseModel, BeforeValidator from instructor import llm\\\\\\_validator, patch from openai import OpenAI class QuestionAnswerNoEvil(BaseModel): question: str answer: Annotated\\\\\\[ str, BeforeValidator( llm\\\\\\_validator(\"don't say objectionable things\") ), \\\\\\] The Modular Approach Pydantic allows for modular output schemas. This leads to more organized code. Composition of Schemas class UserDetails(BaseModel): name: str age: int class UserWithAddress(UserDetails): address: str Defining Relationships class UserDetail(BaseModel): id: int age: int name: str friends: List\\\\\\[int\\\\\\] class UserRelationships(BaseModel): users: List\\\\\\[UserDetail\\\\\\] Using Enums from enum import Enum, auto class Role(Enum): PRINCIPAL = auto() TEACHER = auto() STUDENT = auto() OTHER = auto() class UserDetail(BaseModel): age: int name: str role: Role Flexible Schemas from typing import List class Property(BaseModel): key: str value: str class UserDetail(BaseModel): age: int name: str properties: List\\\\\\[Property\\\\\\] Chain of Thought class TimeRange(BaseModel): chain\\\\\\_of\\\\\\_thought: str start\\\\\\_time: int end\\\\\\_time: int class UserDetail(BaseModel): id: int age: int name: str work\\\\\\_time: TimeRange leisure\\\\\\_time: TimeRange Language Models as Microservices The architecture resembles FastAPI. Most code can be written as Python functions that use Pydantic objects. This eliminates the need for prompt chains. FastAPI Stub app = FastAPI() @app.get(\"/user/{user\\\\\\_id}\", response\\\\\\_model=UserDetails) async def get\\\\\\_user(user\\\\\\_id: int) -> UserDetails: return UserDetails(...) Using Instructor as a Function def extract\\\\\\_user(str) -> UserDetails: return client.chat.completions( response\\\\\\_model=UserDetails, messages=\\\\\\[...\\\\\\] ) Response Modeling class MaybeUser(BaseModel): result: Optional\\\\\\[UserDetail\\\\\\] error: bool message: Optional\\\\\\[str\\\\\\] Conclusion Instructor, with Pydantic, simplifies interaction with language models. It is usable for both experienced and new developers. If you enjoy the content or want to try out instructor please check out the github and give us a star! Continue reading 1 2 Back to top Previous Core Library Next 2023 Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Help with Instructor - Instructor",
    "url": "https://jxnl.github.io/instructor/help/?q=",
    "html": "Skip to content Instructor Help with Instructor Type to start searching instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Introduction Why use Instructor? Help with Instructor Installation Contributing Table of contents Concepts Cookbooks Blog GitHub Discussions GitHub Issues Twitter Getting help with Instructor¶ If you need help getting started with Instructor or with advanced usage, the following sources may be useful. Concepts¶ The concepts section explains the core concepts of Instructor and how to prompt with models. Cookbooks¶ The cookbooks are a great place to start. They contain a variety of examples that demonstrate how to use Instructor in different scenarios. Blog¶ The blog contains articles that explain how to use Instructor in different scenarios. GitHub Discussions¶ GitHub discussions are useful for asking questions, your question and the answer will help everyone. GitHub Issues¶ GitHub issues are useful for reporting bugs or requesting new features. Twitter¶ You can also reach out to me on Twitter if you have any questions or ideas. Was this page helpful? Back to top Previous Why use Instructor? Next Installation Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "AI Engineer Keynote: Pydantic is all you need - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/11/02/ai-engineer-keynote-pydantic-is-all-you-need/",
    "html": "Skip to content Instructor AI Engineer Keynote: Pydantic is all you need Type to start searching instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Back to index Jason Liu Creator Metadata 2023/11/02 1 min read AI Engineer Keynote: Pydantic is all you need¶ Click here to watch the full talk Last month, I ventured back onto the speaking circuit at the inaugural AI Engineer Summit, sharing insights on leveraging Pydantic for effective prompt engineering. I dove deep into what is covered in our documentation and standard blog posts, I'd genuinely appreciate any feedback on the talk – every bit helps in refining the art. So, take a moment to check out the full talk here, and let's continue pushing the boundaries of what's possible. Was this page helpful? Back to top Previous Good LLM Validation is Just Good Validation Next Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Generators and LLM Streaming - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/11/26/python-generators-and-llm-streaming/",
    "html": "Skip to content Instructor Generators and LLM Streaming Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents Python Generators: An Efficient Approach to Iterables The Basics: Yielding Values Advantages Over Traditional Collections Generator Expressions: A Shortcut Use Cases in Real-World Applications LLM Streaming E-commerce Product Ranking Scenario Stream Processing FastAPI Key Takeaways Back to index Jason Liu Creator Anmol Jawandha Contributor Metadata 2023/11/26 6 min read Generators and LLM Streaming¶ Latency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times. And what makes streaming possible? Generators! In this post, we're going to dive into the cool world of Python generators — these tools are more than just a coding syntax trick. We'll explore Python generators from the ground up and then delve into LLM streaming using the Instructor library. Python Generators: An Efficient Approach to Iterables¶ Generators in Python are a game-changer for handling large data sets and stream processing. They allow functions to yield values one at a time, pausing and resuming their state, which is a faster and more memory-efficient approach compared to traditional collections that store all elements in memory. The Basics: Yielding Values¶ A generator function in Python uses the yield keyword. It yields values one at a time, allowing the function to pause and resume its state. def count\\\\\\_to\\\\\\_3(): yield 1 yield 2 yield 3 for num in count\\\\\\_to\\\\\\_3(): print(num) 1 2 3 Advantages Over Traditional Collections¶ Lazy Evaluation & reduced latency: The time to get the first element (or time-to-first-token in LLM land) from a generator is significantly lower. Generators only produce one value at a time, whereas accessing the first element of a collection will require that the whole collection be created first. Memory Efficiency: Only one item is in memory at a time. Maintain State: Automatically maintains state between executions. Let's see how much faster generators are and where they really shine: import time def expensive\\\\\\_func(x): \"\"\"Simulate an expensive operation.\"\"\" time.sleep(1) return x \\\\\\*\\\\\\* 2 def calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_list(func\\\\\\_input, func): \"\"\"Calculate using a list comprehension and return the first result with its computation time.\"\"\" start\\\\\\_perf = time.perf\\\\\\_counter() result = \\\\\\[func(x) for x in func\\\\\\_input\\\\\\]\\\\\\[0\\\\\\] end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (list): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") return result def calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_generator(func\\\\\\_input, func): \"\"\"Calculate using a generator and return the first result with its computation time.\"\"\" start\\\\\\_perf = time.perf\\\\\\_counter() result = next(func(x) for x in func\\\\\\_input) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (generator): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") return result # Prepare inputs for the function numbers = \\\\\\[1, 2, 3, 4, 5\\\\\\] # Benchmarking first\\\\\\_result\\\\\\_list = calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_list(numbers, expensive\\\\\\_func) first\\\\\\_result\\\\\\_gen = calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_generator(numbers, expensive\\\\\\_func) Time for first result (list): 5.02 seconds Time for first result (generator): 1.01 seconds The generator computes one expensive operation and returns the first result immediately, while the list comprehension computes the expensive operation for all elements in the list before returning the first result. Generator Expressions: A Shortcut¶ Python also allows creating generators in a single line of code, known as generator expressions. They are syntactically similar to list comprehensions but use parentheses. squares = (x\\\\\\*x for x in range(10)) Use Cases in Real-World Applications¶ Generators shine in scenarios like reading large files, data streaming (eg. llm token streaming), and pipeline creation for data processing. LLM Streaming¶ If you've used ChatGPT, you'll see that the tokens are streamed out one by one, instead of the full response being shown at the end (can you imagine waiting for the full response??). This is made possible by generators. Here's how a vanilla openai generator looks: from openai import OpenAI # Set your OpenAI API key client = OpenAI( api\\\\\\_key=\"My API Key\", ) response\\\\\\_generator = client.chat.completions.create( model='gpt-3.5-turbo', messages=\\\\\\[ {'role': 'user', 'content': \"What are some good reasons to smile?\"} \\\\\\], temperature=0, stream=True ) for chunk in response\\\\\\_generator: print(chunk.choices\\\\\\[0\\\\\\].delta.content, end=\"\") This is great, but what if we want to do some structured extraction on this stream? For instance, we might want to render frontend components based on product rankings that are streamed out by an LLM. Should we wait for the entire stream to finish before extracting & validating the list of components or can we extract & validate the components in real time as they are streamed? In e-commerce, every millisecond matters so the time-to-first-render can differentiate a successful and not-so-successful e commerce store (and i know how a failing e commerce store feels :/ ). Let's see how we can use Instructor to handle extraction from this real time stream! E-commerce Product Ranking¶ Scenario¶ Imagine an e-commerce platform where we have: • a customer profile: this includes a detailed history of purchases, browsing behavior, product ratings, preferences in various categories, search history, and even responses to previous recommendations. This extensive data is crucial for generating highly personalized and relevant product suggestions. • a list of candidate products: these could be some shortlisted products we think the customer would like. Our goal is to re-rerank these candidate products for the best conversion and we'll use an LLM! Stream Processing¶ User Data: Let's assume we have the following user profile: profile\\\\\\_data = \"\"\" Customer ID: 12345 Recent Purchases: \\\\\\[Laptop, Wireless Headphones, Smart Watch\\\\\\] Frequently Browsed Categories: \\\\\\[Electronics, Books, Fitness Equipment\\\\\\] Product Ratings: {Laptop: 5 stars, Wireless Headphones: 4 stars} Recent Search History: \\\\\\[best budget laptops 2023, latest sci-fi books, yoga mats\\\\\\] Preferred Brands: \\\\\\[Apple, AllBirds, Bench\\\\\\] Responses to Previous Recommendations: {Philips: Not Interested, Adidas: Not Interested} Loyalty Program Status: Gold Member Average Monthly Spend: $500 Preferred Shopping Times: Weekend Evenings ... \"\"\" We want to rank the following products for this user: products = \\\\\\[ {\"product\\\\\\_id\": 1, \"product\\\\\\_name\": \"Apple MacBook Air (2023) - Latest model, high performance, portable\"}, {\"product\\\\\\_id\": 2, \"product\\\\\\_name\": \"Sony WH-1000XM4 Wireless Headphones - Noise-canceling, long battery life\"}, {\"product\\\\\\_id\": 3, \"product\\\\\\_name\": \"Apple Watch Series 7 - Advanced fitness tracking, seamless integration with Apple ecosystem\"}, {\"product\\\\\\_id\": 4, \"product\\\\\\_name\": \"Kindle Oasis - Premium e-reader with adjustable warm light\"}, {\"product\\\\\\_id\": 5, \"product\\\\\\_name\": \"AllBirds Wool Runners - Comfortable, eco-friendly sneakers\"}, {\"product\\\\\\_id\": 6, \"product\\\\\\_name\": \"Manduka PRO Yoga Mat - High-quality, durable, eco-friendly\"}, {\"product\\\\\\_id\": 7, \"product\\\\\\_name\": \"Bench Hooded Jacket - Stylish, durable, suitable for outdoor activities\"}, {\"product\\\\\\_id\": 8, \"product\\\\\\_name\": \"GoPro HERO9 Black - 5K video, waterproof, for action photography\"}, {\"product\\\\\\_id\": 9, \"product\\\\\\_name\": \"Nespresso Vertuo Next Coffee Machine - Quality coffee, easy to use, compact design\"}, {\"product\\\\\\_id\": 10, \"product\\\\\\_name\": \"Project Hail Mary by Andy Weir - Latest sci-fi book from a renowned author\"} \\\\\\] Let's now define our models for structured extraction. Note: instructor will conveniently let us use Iterable to model an iterable of our class. In this case, once we define our product recommendation model, we can slap on Iterable to define what we ultimately want - a (ranked) list of product recommendations. import instructor from openai import OpenAI from typing import Iterable from pydantic import BaseModel client = instructor.patch(OpenAI(), mode=instructor.function\\\\\\_calls.Mode.JSON) class ProductRecommendation(BaseModel): product\\\\\\_id: str product\\\\\\_name: str Recommendations = Iterable\\\\\\[ProductRecommendation\\\\\\] Now let's use our instructor patch. Since we don't want to wait for all the tokens to finish, will set stream to True and process each product recommendation as it comes in: prompt = f\"Based on the following user profile:\\\\\\\\n{profile\\\\\\_data}\\\\\\\\nRank the following products from most relevant to least relevant:\\\\\\\\n\" + '\\\\\\\\n'.join(f\"{product\\\\\\['product\\\\\\_id'\\\\\\]} {product\\\\\\['product\\\\\\_name'\\\\\\]}\" for product in products) start\\\\\\_perf = time.perf\\\\\\_counter() recommendations\\\\\\_stream = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Iterable\\\\\\[ProductRecommendation\\\\\\], stream=True, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"}, {\"role\": \"user\", \"content\": prompt} \\\\\\] ) for product in recommendations\\\\\\_stream: print(product) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (generator): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") break product\\\\\\_id='1' product\\\\\\_name='Apple MacBook Air (2023)' Time for first result (generator): 4.33 seconds recommendations\\\\\\_stream is a generator! It yields the extracted products as it's processing the stream in real-time. Now let's get the same response without streaming and see how they compare. start\\\\\\_perf = time.perf\\\\\\_counter() recommendations\\\\\\_list = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Iterable\\\\\\[ProductRecommendation\\\\\\], stream=False, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"}, {\"role\": \"user\", \"content\": prompt} \\\\\\] ) print(recommendations\\\\\\_list\\\\\\[0\\\\\\]) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (list): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") product\\\\\\_id='1' product\\\\\\_name='Apple MacBook Air (2023)' Time for first result (list): 8.63 seconds Our web application now displays results faster. Even a 100ms improvement can lead to a 1% increase in revenue. FastAPI¶ We can also take this and set up a streaming LLM API endpoint using FastAPI. Check out our docs on using FastAPI here! Key Takeaways¶ To summarize, we looked at: • Generators in Python: A powerful feature that allows for efficient data handling with reduced latency • LLM Streaming: LLMs provide us generators to stream tokens and Instructor can let us validate and extract data from this stream. Real-time data validation ftw! Don't forget to check our GitHub for more resources and give us a star if you find the library helpful! If you have any questions or need further clarifications, feel free to reach out or dive into the Instructor library's documentation for more detailed information. Happy coding! Was this page helpful? Back to top Previous Verifying LLM Citations with Pydantic Next Introduction to Caching in Python Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Introduction to Caching in Python - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/11/26/python-caching/",
    "html": "Skip to content Instructor Introduction to Caching in Python Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents 1. functools.cache for Simple In-Memory Caching 2. diskcache for Persistent, Large Data Caching 2. Redis Caching Decorator for Distributed Systems Conclusion Back to index Jason Liu Creator Metadata 2023/11/26 7 min read Introduction to Caching in Python¶ Instructor makes working with language models easy, but they are still computationally expensive. Today, we're diving into optimizing instructor code while maintaining the excellent DX offered by Pydantic models. We'll tackle the challenges of caching Pydantic models, typically incompatible with pickle, and explore solutions that use decorators like functools.cache. Then, we'll craft custom decorators with diskcache and redis to support persistent caching and distributed systems. Lets first consider our canonical example, using the OpenAI Python client to extract user details. import instructor from openai import OpenAI from pydantic import BaseModel # Enables \\\\\\`response\\\\\\_model\\\\\\` client = instructor.patch(OpenAI()) class UserDetail(BaseModel): name: str age: int def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Now imagine batch processing data, running tests or experiments, or simply calling extract multiple times over a workflow. We'll quickly run into performance issues, as the function may be called repeatedly, and the same data will be processed over and over again, costing us time and money. 1. functools.cache for Simple In-Memory Caching¶ When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session or in an application where we don't need to persist the cache between sessions. import functools @functools.cache def extract(data): return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Changing the Model does not Invalidate the Cache Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result. Now we can call extract multiple times with the same argument, and the result will be cached in memory for faster access. import time start = time.perf\\\\\\_counter() # Using time.perf\\\\\\_counter() to measure the time taken to run the function is better than using time.time() because it's more accurate and less susceptible to system clock changes. model = extract(\"Extract jason is 25 years old\") print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") start = time.perf\\\\\\_counter() model = extract(\"Extract jason is 25 years old\") # The second time we call extract, the result is returned from the cache, and the function is not called. print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") >>> Time taken: 0.9267581660533324 >>> Time taken: 1.2080417945981026e-06 # The second call to extract is much faster because the result is returned from the cache! Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries. What is a decorator? 2. diskcache for Persistent, Large Data Caching¶ Copy Caching Code When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data! import functools import inspect import instructor import diskcache from openai import OpenAI from pydantic import BaseModel client = instructor.patch(OpenAI()) cache = diskcache.Cache('./my\\\\\\_cache\\\\\\_directory') def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation # We use inspect.signature to get the function's return type annotation, which we use to validate the cached result. if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type We use Pydantic's model\\\\\\_validate\\\\\\_json to deserialize the cached result into a Pydantic model. return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence. 2. Redis Caching Decorator for Distributed Systems¶ Copy Caching Code When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures. import redis import functools import inspect import json import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) cache = redis.Redis(\"localhost\") def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: # Assuming client.chat.completions.create returns a UserDetail instance return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types. Looking carefully If you look carefully at the code above you'll notice that we're using the same instructor\\\\\\_cache decorator as before. The implementatino is the same, but we're using a different caching backend! Conclusion¶ Choosing the right caching strategy depends on your application's specific needs, such as the size and type of data, the need for persistence, and the system's architecture. Whether it's optimizing a function's performance in a small application or managing large datasets in a distributed environment, Python offers robust solutions to improve efficiency and reduce computational overhead. If you'd like to use this code, try to send it over to ChatGPT to understand it more, and to add additional features that might matter for you, for example, the cache isn't invalidated when your BaseModel changes, so you might want to encode the Model.model\\\\\\_json\\\\\\_schema() as part of the key. If you like the content check out our GitHub as give us a star and checkout the library. Was this page helpful? Back to top Previous Generators and LLM Streaming Next Structured Outputs with Anyscale Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Introduction to Batch Processing using asyncio and Instructor - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/11/13/learn-async/",
    "html": "Skip to content Instructor Introduction to Batch Processing using asyncio and Instructor Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents Understanding asyncio Understanding async and await Understanding gather vs as\\\\\\_completed Example: Batch Processing for loop: Running tasks sequentially. asyncio.gather: Running tasks concurrently. asyncio.as\\\\\\_completed: Handling tasks as they complete. Rate-Limited Gather: Using semaphores to limit concurrency. Rate-Limited As Completed: Using semaphores to limit concurrency. Results Practical implications of batch processing Back to index Jason Liu Creator Metadata 2023/11/13 6 min read Introduction to Batch Processing using asyncio and Instructor¶ Today, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using instructor and learn how to use asyncio.gather and asyncio.as\\\\\\_completed for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using asyncio.Semaphore. Github Example If you want to run the code examples in this article, you can find them on jxnl/instructor We will start by defining an async function that calls openai to extract data, and then examine four different ways to execute it. We will discuss the pros and cons of each approach and analyze the results of running them on a small batch. Understanding asyncio¶ asyncio is a Python library that enables writing concurrent code using the async/await syntax. It is particularly useful for IO-bound and structured network code. If you are familiar with OpenAI's SDK, you might have encountered two classes: OpenAI() and AsyncOpenAI(). Today, we will be using the AsyncOpenAI() class, which processes data asynchronously. By utilizing these tools in web applications or batch processing, we can significantly improve performance by handling multiple requests concurrently instead of sequentially. Understanding async and await¶ We will be using the async and await keywords to define asynchronous functions. The async keyword is used to define a function that returns a coroutine object. The await keyword is used to wait for the result of a coroutine object. If you want to understand the deeper details of asyncio, I recommend reading this article by Real Python. Understanding gather vs as\\\\\\_completed¶ In this post we'll show two ways to run tasks concurrently: asyncio.gather and asyncio.as\\\\\\_completed. The gather method is used to run multiple tasks concurrently and return the results as a list. The as\\\\\\_completed returns a iterable is used to run multiple tasks concurrently and return the results as they complete. Another great resource on the differences between the two can be found here. Example: Batch Processing¶ In this example, we will demonstrate how to use asyncio for batch processing tasks, specifically for extracting and processing data concurrently. The script will extract data from a list of texts and process it concurrently using asyncio. import instructor from pydantic import BaseModel from openai import AsyncOpenAI # Enables \\\\\\`response\\\\\\_model\\\\\\` in \\\\\\`create\\\\\\` method client = instructor.apatch(AsyncOpenAI()) We use instructor.apatch to patch the create method of AsyncOpenAI to accept a response\\\\\\_model argument. This is because the create method of AsyncOpenAI does not accept a response\\\\\\_model argument without this patch. class Person(BaseModel): name: str age: int async def extract\\\\\\_person(text: str) -> Person: return await client.chat.completions.create( We use await here to wait for the response from the server before we return the result. This is because create returns a coroutine object, not the result of the coroutine. model=\"gpt-3.5-turbo\", messages=\\\\\\[ {\"role\": \"user\", \"content\": text}, \\\\\\], response\\\\\\_model=Person, ) Notice that now there are async and await keywords in the function definition. This is because we're using the asyncio library to run the function concurrently. Now lets define a batch of texts to process. dataset = \\\\\\[ \"My name is John and I am 20 years old\", \"My name is Mary and I am 21 years old\", \"My name is Bob and I am 22 years old\", \"My name is Alice and I am 23 years old\", \"My name is Jane and I am 24 years old\", \"My name is Joe and I am 25 years old\", \"My name is Jill and I am 26 years old\", \\\\\\] for loop: Running tasks sequentially.¶ persons = \\\\\\[\\\\\\] for text in dataset: person = await extract\\\\\\_person(text) persons.append(person) Even though there is an await keyword, we still have to wait for each task to finish before starting the next one. This is because we're using a for loop to iterate over the dataset. This method, which uses a for loop, will be the slowest among the four methods discussed today. asyncio.gather: Running tasks concurrently.¶ async def gather(): tasks\\\\\\_get\\\\\\_persons = \\\\\\[extract\\\\\\_person(text) for text in dataset\\\\\\] all\\\\\\_persons = await asyncio.gather(\\\\\\*tasks\\\\\\_get\\\\\\_persons) We use await here to wait for all the tasks to finish before assigning the result to all\\\\\\_persons. This is because asyncio.gather returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.as\\\\\\_completed to achieve the same result. Using asyncio.gather allows us to return all the results at once. It is an effective way to speed up our code, but it's not the only way. Particularly, if we have a large dataset, we might not want to wait for everything to finish before starting to process the results. This is where asyncio.as\\\\\\_completed comes into play. asyncio.as\\\\\\_completed: Handling tasks as they complete.¶ async def as\\\\\\_completed(): all\\\\\\_persons = \\\\\\[\\\\\\] tasks\\\\\\_get\\\\\\_persons = \\\\\\[extract\\\\\\_person(text) for text in dataset\\\\\\] for person in asyncio.as\\\\\\_completed(tasks\\\\\\_get\\\\\\_persons): all\\\\\\_persons.append(await person) We use await here to wait for each task to complete before appending it to the list. This is because as\\\\\\_completed returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.gather to achieve the same result. This method is a great way to handle large datasets. We can start processing the results as they come in, especially if we are streaming data back to a client. However, these methods aim to complete as many tasks as possible as quickly as possible. This can be problematic if we want to be considerate to the server we're making requests to. This is where rate limiting comes into play. While there are libraries available to assist with rate limiting, for our initial defense, we will use a semaphore to limit the number of concurrent requests we make. Ordering of results Its important to note that the order of the results will not be the same as the order of the dataset. This is because the tasks are completed in the order they finish, not the order they were started. If you need to preserve the order of the results, you can use asyncio.gather instead. Rate-Limited Gather: Using semaphores to limit concurrency.¶ sem = asyncio.Semaphore(2) async def rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text: str, sem: Semaphore) -> Person: async with sem: We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to. return await extract\\\\\\_person(text) async def rate\\\\\\_limited\\\\\\_gather(sem: Semaphore): tasks\\\\\\_get\\\\\\_persons = \\\\\\[rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text, sem) for text in dataset\\\\\\] resp = await asyncio.gather(\\\\\\*tasks\\\\\\_get\\\\\\_persons) Rate-Limited As Completed: Using semaphores to limit concurrency.¶ sem = asyncio.Semaphore(2) async def rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text: str, sem: Semaphore) -> Person: async with sem: We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to. return await extract\\\\\\_person(text) async def rate\\\\\\_limited\\\\\\_as\\\\\\_completed(sem: Semaphore): all\\\\\\_persons = \\\\\\[\\\\\\] tasks\\\\\\_get\\\\\\_persons = \\\\\\[rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text, sem) for text in dataset\\\\\\] for person in asyncio.as\\\\\\_completed(tasks\\\\\\_get\\\\\\_persons): all\\\\\\_persons.append(await person) We use await here to wait for each task to complete before appending it to the list. This is because as\\\\\\_completed returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.gather to achieve the same result. Now that we have seen the code, let's examine the results of processing 7 texts. As the prompts become longer or if we use GPT-4, the differences between these methods will become more pronounced. Other Options Its important to also note that here we are using a semaphore to limit the number of concurrent requests. However, there are other ways to limit concurrency esp since we have rate limit information from the openai request. You can imagine using a library like ratelimit to limit the number of requests per second. OR catching rate limit exceptions and using tenacity to retry the request after a certain amount of time. tenacity aiolimiter Results¶ As you can see, the for loop is the slowest, while asyncio.as\\\\\\_completed and asyncio.gather are the fastest without any rate limiting. Method Execution Time Rate Limited (Semaphore) For Loop 6.17 seconds Asyncio.gather 0.85 seconds Asyncio.as\\\\\\_completed 0.95 seconds Asyncio.gather 3.04 seconds 2 Asyncio.as\\\\\\_completed 3.26 seconds 2 Practical implications of batch processing¶ The choice of approach depends on the task's nature and the desired balance between speed and resource utilization. Here are some guidelines to consider: Use asyncio.gather for handling multiple independent tasks quickly. Apply asyncio.as\\\\\\_completed for large datasets to process tasks as they complete. Implement rate-limiting to avoid overwhelming servers or API endpoints. If you find the content helpful or want to try out Instructor, please visit our GitHub page and give us a star! Was this page helpful? Back to top Previous Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density Next Verifying LLM Citations with Pydantic Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Structured Outputs with Anyscale - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/12/15/patching/",
    "html": "Skip to content Instructor Structured Outputs with Anyscale Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents Patching Anyscale Back to index Anmol Jawandha Contributor Jason Liu Creator Metadata 2023/12/15 2 min read Structured Outputs with Anyscale¶ Open-source LLMS are gaining popularity, and the release of Anyscale's Mistral model has made it possible to obtain structured outputs using JSON schema at any scale. Instead of relying on a model's default output mode, you can utilize JSON schema to obtain structured outputs. This approach is a time-saving alternative to extensive prompt engineering. By the end of this blog post, you will learn how to effectively utilize the instructor at any scale. But before we proceed, let's first explore the concept of patching. Patching¶ Instructor's patch enhances a openai api it with the following features: response\\\\\\_model in create calls that returns a pydantic model max\\\\\\_retries in create calls that retries the call if it fails by using a backoff strategy Learn More To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page. Anyscale¶ The good news is that Anyscale employs the same OpenAI client, and its models support some of these output modes too! Getting access If you want to try this out for yourself check out the Anyscale website. You can get started here. Let's explore one of the models available in Anyscale's extensive collection! from openai import OpenAI from pydantic import BaseModel import instructor class UserDetails(BaseModel): name: str age: int # enables \\\\\\`response\\\\\\_model\\\\\\` in create call client = instructor.patch( OpenAI( base\\\\\\_url=\"https://api.endpoints.anyscale.com/v1\", api\\\\\\_key=\"\" ), # This uses Anyscale's json schema output mode mode=instructor.Mode.JSON\\\\\\_SCHEMA ) resp = client.chat.completions.create( model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class extractor\" }, { \"role\": \"user\", \"content\": 'Extract the following entities: \"Jason is 20\"' }, \\\\\\], response\\\\\\_model=UserDetails, ) print(resp) # >>> name='Jason' age=20 You can find more information about Anyscale's output mode support here. Was this page helpful? Back to top Previous Introduction to Caching in Python Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/11/05/chain-of-density/",
    "html": "Skip to content Instructor Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents Part 1) Chain of Density Original Prompt Data Modelling Initial Summary Rewritten Summary Putting it all Together Part 2) Fine-Tuning Creating a Training Set Creating Fine-Tuning Jobs Results and Benchmarks Conclusions Back to index Ivan Leo Contributor Jason Liu Creator Metadata 2023/11/05 15 min read Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density¶ Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor In this article, we'll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4's iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density. By the end you'll end up with a GPT 3.5 model, (fine-tuned using Instructor's great tooling), capable of producing summaries that rival the effectiveness of Chain of Density \\\\\\[Adams et al. (2023)\\\\\\]. As always, all code is readily available in our examples/chain-of-density folder in our repo for your reference. Datasets and Colab Notebook Part 1) Chain of Density¶ Summarizing extensive texts with AI can be challenging, often relying on inconsistent techniques. Their novel method, Chain Of Density prompting, enhances AI-based text summarization, outperforming human-generated summaries. Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density. First introduced in the paper - From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. The team has found that this method is able to consistently beats similar summaries written by human annotators. Implementation Details Original Prompt¶ We can break down the original process into smaller api calls. This allows us to introduce validation at each step to ensure that we're getting the results that we want. Original Chain of Density Prompt Improved process with Instructor Data Modelling¶ Before we begin modelling the data, let's make sure we install all of our dependencies pip install instructor aiohttp rich Initial Summary¶ Let's start by walking through some of the data models that we'll be using as the response\\\\\\_model for our open ai function calls Firstly, we'll need a data model for the initial summary that we will be generating. We'll take the description of this class straight from the original prompt. It's important to note that these docstrings serve a purpose, they are directly used by the LLM when generating the outputs. A quick note on Docstrings class InitialSummary(BaseModel): \"\"\" This is an initial summary which should be long ( 4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words. \"\"\" summary: str = Field( ..., description=\"This is a summary of the article provided which is overly verbose and uses fillers. It should be roughly 80 words in length\", ) Rewritten Summary¶ We'll also need one additional class to help model the rewritten schema class RewrittenSummary(BaseModel): \"\"\" This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. Guidelines - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" - Missing entities can appear anywhere in the new summary An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. \"\"\" summary: str = Field( ..., description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\", ) absent: List\\\\\\[str\\\\\\] = Field( ..., default\\\\\\_factory=list, description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\", ) missing: List\\\\\\[str\\\\\\] = Field( default\\\\\\_factory=list, description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\", ) Using Pydantic Validators with Instructor For a more in-depth walkthrough on how to use Pydantic validators with the Instructor library, we recommend checking out our previous article on LLM validation - Good LLM Validation is just Good Validation Ideally, we'd like for Missing to have a length between 1 and 3, Absent to be an empty list and for our rewritten summaries to keep a minimum entity density. With Instructor, we can implement this logic using native Pydantic validators that are simply declared as part of the class itself. import nltk import spacy nlp = spacy.load(\"en\\\\\\_core\\\\\\_web\\\\\\_sm\") @field\\\\\\_validator(\"summary\") def min\\\\\\_length(cls, v: str): tokens = nltk.word\\\\\\_tokenize(v) Similar to the original paper, we utilize the NLTK word tokenizer to count the number of tokens within our generated sentences. We aim for at least 60 tokens in our generated summary so that we don't lose information. num\\\\\\_tokens = len(tokens) if num\\\\\\_tokens < 60: raise ValueError( \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\" ) return v @field\\\\\\_validator(\"missing\") def has\\\\\\_missing\\\\\\_entities(cls, missing\\\\\\_entities: List\\\\\\[str\\\\\\]): if len(missing\\\\\\_entities) == 0: raise ValueError( \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\" ) return missing\\\\\\_entities @field\\\\\\_validator(\"absent\") def has\\\\\\_no\\\\\\_absent\\\\\\_entities(cls, absent\\\\\\_entities: List\\\\\\[str\\\\\\]): absent\\\\\\_entity\\\\\\_string = \",\".join(absent\\\\\\_entities) if len(absent\\\\\\_entities) > 0: print(f\"Detected absent entities of {absent\\\\\\_entity\\\\\\_string}\") raise ValueError( f\"Do not omit the following Entities {absent\\\\\\_entity\\\\\\_string} from the new summary\" ) return absent\\\\\\_entities @field\\\\\\_validator(\"summary\") def min\\\\\\_entity\\\\\\_density(cls, v: str): tokens = nltk.word\\\\\\_tokenize(v) num\\\\\\_tokens = len(tokens) # Extract Entities doc = nlp(v) We also use the spaCy library to calculate the entity density of the generated summary. num\\\\\\_entities = len(doc.ents) density = num\\\\\\_entities / num\\\\\\_tokens if density < 0.08: We also implement a minimum entity density so that we stay within a given range. 0.08 is arbitrarily chosen in this case raise ValueError( f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\" ) return v Putting it all Together¶ Now that we have our models and the rough flow figured out, let's implement a function to summarize a piece of text using Chain Of Density summarization. from openai import OpenAI import instructor client = instructor.patch(OpenAI()) We need to apply a patch function on the OpenAI client for us to get all of the benefits that Instructor provides. With a simple patch, we can get automatic type coercion of our outputs and automatic retries for invalid outputs out of the box! def summarize\\\\\\_article(article: str, summary\\\\\\_steps: int = 3): summary\\\\\\_chain = \\\\\\[\\\\\\] # We first generate an initial summary summary: InitialSummary = client.chat.completions.create( We first generate an initial summary. Note here that we explictly ask for a summary that has 80 words and is lengthy with overly verbose fillers in the system prompt model=\"gpt-4-0613\", response\\\\\\_model=InitialSummary, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": \"The generated summary should be about 80 words.\", }, \\\\\\], max\\\\\\_retries=2, ) prev\\\\\\_summary = None summary\\\\\\_chain.append(summary.summary) for i in range(summary\\\\\\_steps): missing\\\\\\_entity\\\\\\_message = ( \\\\\\[\\\\\\] if prev\\\\\\_summary is None else \\\\\\[ { \"role\": \"user\", \"content\": f\"Please include these Missing Entities: {','.join(prev\\\\\\_summary.missing)}\", }, \\\\\\] ) new\\\\\\_summary: RewrittenSummary = client.chat.completions.create( We slightly modify the original system prompt used in the original paper to perform a rewrite of the summary. Using Instructor, we also get validation of the generated output with our field\\\\\\_validators that we defined above model=\"gpt-4-0613\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"\"\" You are going to generate an increasingly concise,entity-dense summary of the following article. Perform the following two tasks - Identify 1-3 informative entities from the following article which is missing from the previous summary - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities Guidelines - Make every word count: re-write the previous summary to improve flow and make space for additional entities - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\". - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article. - Missing entities can appear anywhere in the new summary - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \"\"\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": f\"Here is the previous summary: {summary\\\\\\_chain\\\\\\[-1\\\\\\]}\", }, \\\\\\*missing\\\\\\_entity\\\\\\_message, \\\\\\], max\\\\\\_retries=3, If you've chosen a value that is larger than 0.08, make sure to increase this value in case you need to do multiple rewrites max\\\\\\_tokens=1000, response\\\\\\_model=RewrittenSummary, ) summary\\\\\\_chain.append(new\\\\\\_summary.summary) prev\\\\\\_summary = new\\\\\\_summary return summary\\\\\\_chain This summarization function yields a result which triples the number of entities while maintaining the same number of tokens. We can also see that stylistically, the summary is a lot more natural. First Iteration This article discusses the highly-anticipated boxing match between Manny Pacquiao and Floyd Mayweather. The article revolves around Manny Pacquiao's statements about his upcoming fight and his preparations for the same. A portion of the article provides details about the financial stipulations of the match and its significance in the sporting arena. Quotes from Pacquiao illustrating his determination and his battle strategy are highlighted. The tone of the article is largely centered around creating a build-up to the upcoming mega event. Final Iteration Manny Pacquiao, the Filipino boxer, anticipates the forthcoming May 2 showdown at the MGM Grand as the fight of his life, against the undefeated American Floyd Mayweather, in a $300m bout. Despite being seen as the underdog in this high-stakes Las Vegas match, Pacquiao is confident, promising a warrior's spirit and assuring the fans who have been awaiting this encounter for a decade, that it will indeed be the biggest sporting spectacle in history worthy of their anticipation Part 2) Fine-Tuning¶ In this section, we'll look into how to fine-tune a GPT 3.5 model so that it is able to perform at an equivalent level as a GPT-4 model. We'll then compare the performance of our model against that of GPT-4 to see how it stacks up. Creating a Training Set¶ In order to prevent any contamination of data during testing, we randomly sampled 120 articles from the griffin/chain-of-density dataset and split these articles into a train.csv and a test.csv file which we uploaded to Hugging Face. Now, we just neeed to import the Instructions module from the Instructor package which allows you to generate a nicely formatted .jsonl file to be used for fine-tuning from typing import List from chain\\\\\\_of\\\\\\_density import summarize\\\\\\_article In this example, we're using the summarize\\\\\\_article that we defined up above. We saved it in a local file called chain\\\\\\_of\\\\\\_density.py, hence the import import csv import logging import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) We patch the default OpenAI client so that we can use the Instructor library with it logging.basicConfig(level=logging.INFO) We also need to configure logging at the INFO level. This is very important, if this is not configured, your output will not be generated. instructions = instructor.Instructions( name=\"Chain Of Density\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"generated.jsonl\")\\\\\\], openai\\\\\\_client=client, ) class GeneratedSummary(BaseModel): \"\"\" This represents a highly concise summary that includes as many entities as possible from the original source article. An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. Guidelines - Make every word count - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" \"\"\" summary: str = Field( ..., description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \", ) @instructions.distil We instantiate a Instruction object which will help us handle the conversion of our function calls into a valid .jsonl file. We also define the name of the .jsonl file in the log\\\\\\_handlers parameter def distil\\\\\\_summarization(text: str) -> GeneratedSummary: summary\\\\\\_chain: List\\\\\\[str\\\\\\] = summarize\\\\\\_article(text) return GeneratedSummary(summary=summary\\\\\\_chain\\\\\\[-1\\\\\\]) We add in an instructions.distil annotation so that we automatically capture the input and output of the function we'd like to fine-tune our model to output with open(\"train.csv\", \"r\") as file: reader = csv.reader(file) next(reader) # Skip the header for article, summary in reader: # Run Distillisation to generate the values distil\\\\\\_summarization(article) Rate Limiting We recommend running this script on a small subset of the dataset first to test you've got everything configured nicely. Don't forget to add in rate limiting error handling with tenacity and set the OPENAI\\\\\\_API\\\\\\_KEY shell environment variable before running any subsequent commands Creating Fine-Tuning Jobs¶ Once we run this script, we'll have a new file called generated.jsonl in our local repository. Now all that's left is to run the command below to start fine-tuning your first model! instructor jobs create-from-file generated.jsonl Finetuning Reference Once the job is complete, all we need to do is to then change the annotation in the function call to distil\\\\\\_summarization in our original file above to start using our new model. @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard def distil\\\\\\_summarization(text: str) -> GeneratedSummary: summary\\\\\\_chain: List\\\\\\[str\\\\\\] = summarize\\\\\\_article(text) return GeneratedSummary(summary=summary\\\\\\_chain\\\\\\[-1\\\\\\]) With that, you've now got your own fine-tuned model ready to go and serve data in production. We've seen how Instructor can make your life easier, from fine-tuning to distillation. Results and Benchmarks¶ We'll be comparing the following models in 3 ways using 20 articles that were not used for fine-tuning. Entity Density : This is entities per token, the higher the better for density. Latency : Time to last token generated in seconds Costs : Total cost to generate outputs - we break down the cost into training and inference costs for easy reference 3.5 Finetuned (n) This is a GPT 3.5 model that we fine-tuned on n examples. Each model was finetuned for 4-5 epochs ( This was automatically decided by the OpenAI scheduler ) GPT-4 (COD) This is a GPT4 model which we applied 3 rounds of Chain Of Density rewrites to generate a summary with using the methodology above GPT-3.5 (Vanilla) This is a GPT 3.5 model that we asked to generate entity-dense summaries which were concise. Summaries were generated in a single pass targetting about 80-90 tokens. Model Mean Latency (s) Mean Entity Density 3.5 Finetuned (20) 2.1 0.15 3.5 Finetuned (50) 2.1 0.14 3.5 Finetuned (76) 2.1 0.14 GPT-3.5 (Vanilla) 16.8 0.12 GPT-4 (COD) 49.5 0.15 Finetuning Datasets Using the OpenAI Usage Dashboard, we can calculate the cost of generating 20 summaries as seen below. Model Training Cost ($) Inference Cost ($) Tokens Used Total Cost ($) GPT-3.5 (Vanilla) - 0.20 51,162 0.2 3.5 Finetuned (20) 0.7 0.20 56,573 0.8 3.5 Finetuned (50) 1.4 0.17 49,057 1.3 3.5 Finetuned (76) 1.8 0.17 51,583 2.5 GPT-4 (COD) - 12.9 409,062 12.9 Here, we can see that GPT-4 has an approximate inference cost of 0.65 per summary while our finetuned models have an inference cost of 0.0091 per summary which is ~ 72x cheaper. Interestingly, the model finetuned with the least examples seems to outperform the others. While the reason for this is unknown, a few potential reasons could be that either we didn't train for sufficient epochs ( We chose the default 5 epochs ) or that the models started learning to imitate other behaviour such as more abstract writing styles from the larger variety of samples, resulting in a decrease in entity density. Conclusions¶ Finetuning this iterative method was 20-40x faster while improving overall performance, resulting in massive efficiency gains by finetuning and distilling capabilities into specialized models. We've seen how Instructor can make your life easier, from data modeling to distillation and finetuning. If you enjoy the content or want to try out instructor check out the github and don't forget to give us a star! Was this page helpful? Back to top Previous AI Engineer Keynote: Pydantic is all you need Next Introduction to Batch Processing using asyncio and Instructor Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/",
    "html": "Skip to content Instructor Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents Introduction Why use Instructor? Quick Start: How to Use Instructor's Distillation Feature The Intricacies of Fine-tuning Language Models Why Instructor and Distillation are Game Changers Role of Instructor in Simplifying Fine-Tuning Logging Output and Running a Finetune Next Steps and Future Plans Conclusion Back to index Jason Liu Creator Metadata 2023/10/17 4 min read Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation¶ Introduction¶ Get ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the instructor.instructions streamlines this process, making the task you want to distil more efficient and powerful while preserving its original functionality and backwards compatibility. If you want to see the full example checkout examples/distillation Why use Instructor?¶ Imagine you're developing a backend service that uses a mix old and new school ML practises, it may involve pipelines with multiple function calls, validations, and data processing. Sounds cumbersome, right? That's where Instructor comes in. It simplifies complex procedures, making them more efficient and easier to manage by adding a decorator to your function that will automatically generate a dataset for fine-tuning and help you swap out the function implementation. Quick Start: How to Use Instructor's Distillation Feature¶ Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file. import logging import random from pydantic import BaseModel from instructor import Instructions # pip install instructor # Logging setup logging.basicConfig(level=logging.INFO) instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"math\\\\\\_finetunes.jsonl\")\\\\\\] ) class Multiply(BaseModel): a: int b: int result: int # Define a function with distillation # The decorator will automatically generate a dataset for fine-tuning # They must return a pydantic model to leverage function calling @instructions.distil def fn(a: int, b: int) -> Multiply: resp = a \\\\\\* b return Multiply(a=a, b=b, result=resp) # Generate some data for \\\\\\_ in range(10): a = random.randint(100, 999) b = random.randint(100, 999) print(fn(a, b)) The Intricacies of Fine-tuning Language Models¶ Fine-tuning isn't just about writing a function like def f(a, b): return a \\\\\\* b. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this. Why Instructor and Distillation are Game Changers¶ The library offers two main benefits: Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code. Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions. Role of Instructor in Simplifying Fine-Tuning¶ The from instructor import Instructions feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior. Logging Output and Running a Finetune¶ Here's how the logging output would look: { \"messages\": \\\\\\[ {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'}, {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'}, {\"role\": \"assistant\", \"function\\\\\\_call\": { \"name\": \"Multiply\", \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}' } } \\\\\\], \"functions\": \\\\\\[ {\"name\": \"Multiply\", \"description\": \"Correctly extracted \\\\\\`Multiply\\\\\\`...\"} \\\\\\] } Run a finetune like this: Don't forget to set your OpenAI Key as an environment variable All of the instructor jobs commands assume you've set an environment variable of OPENAI\\\\\\_API\\\\\\_KEY in your shell. You can set this by running the command export OPENAI\\\\\\_API\\\\\\_KEY= in your shell instructor jobs create-from-file math\\\\\\_finetunes.jsonl Next Steps and Future Plans¶ Here's a sneak peek of what I'm planning: from instructor import Instructions, patch patch() Don't forget to run the patch() command that we provide with the Instructor package. This helps automatically serialize the content back into the \\\\\\`Pydantic\\\\\\`\\\\\\` model that we're looking for. class Multiply(BaseModel): a: int b: int result: int instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", ) @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard def fn(a: int, b: int) -> Multiply: resp = a + b return Multiply(a=a, b=b, result=resp) With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation. Conclusion¶ We've seen how Instructor can make your life easier, from fine-tuning to distillation. Now if you're thinking wow, I'd love a backend service to do this for continously, you're in luck! Please check out the survey at useinstructor.com and let us know who you are. If you enjoy the content or want to try out instructor please check out the github and give us a star! Was this page helpful? Back to top Previous RAG is more than just embedding search Next Good LLM Validation is Just Good Validation Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Verifying LLM Citations with Pydantic - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/11/18/validate-citations/",
    "html": "Skip to content Instructor Verifying LLM Citations with Pydantic Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents Example 1: Simple Substring Check Code Example: Error Message Example: Example 2: Using LLM for Verification Code Example: Result: Error Message Example: Example 3: Aligning Citations and Answers Code Example: Error Message Example: Conclusion Back to index Jason Liu Creator Metadata 2023/11/18 4 min read Verifying LLM Citations with Pydantic¶ Ensuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification. We'll start with using a simple substring check to verify citations. Then we'll use instructor itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses. Example 1: Simple Substring Check¶ In this example, we use the Statements class to verify if a given substring quote exists within a text chunk. If the substring is not found, an error is raised. Code Example:¶ from typing import List, Optional from openai import OpenAI from pydantic import BaseModel, Field, ValidationError, ValidationInfo, field\\\\\\_validator, model\\\\\\_validator import instructor client = instructor.patch(OpenAI()) class Statements(BaseModel): body: str substring\\\\\\_quote: str @field\\\\\\_validator(\"substring\\\\\\_quote\") @classmethod def substring\\\\\\_quote\\\\\\_exists(cls, v: str, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) for text\\\\\\_chunk in context.values(): if v in text\\\\\\_chunk: # While we use a simple substring check in this example, we can use more complex techniques like regex or Levenshtein distance. return v raise ValueError(\"Could not find substring\\\\\\_quote \\\\\\`{v}\\\\\\` in contexts\") class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] Once the class is defined, we can use it to validate the context and raise an error if the substring is not found. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is not the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example:¶ answer.0.substring\\\\\\_quote Value error, Could not find substring\\\\\\_quote \\\\\\`Paris is the capital of France\\\\\\` in contexts \\\\\\[type=value\\\\\\_error, input\\\\\\_value='Paris is the capital of France', input\\\\\\_type=str\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Pydantic raises a validation error when the substring\\\\\\_quote attribute does not exist in the context. This approach can be used to validate more complex data using techniques like regex or Levenshtein distance. Example 2: Using LLM for Verification¶ This approach leverages OpenAI's LLM to validate citations. If the citation does not exist in the context, the LLM returns an error message. Code Example:¶ class Validation(BaseModel): is\\\\\\_valid: bool error\\\\\\_messages: Optional\\\\\\[str\\\\\\] = Field(None, description=\"Error messages if any\") class Statements(BaseModel): body: str substring\\\\\\_quote: str @model\\\\\\_validator(mode=\"after\") def substring\\\\\\_quote\\\\\\_exists(self, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) resp: Validation = client.chat.completions.create( response\\\\\\_model=Validation, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Does the following citation exist in the following context?\\\\\\\\n\\\\\\\\nCitation: {self.substring\\\\\\_quote}\\\\\\\\n\\\\\\\\nContext: {context}\", } \\\\\\], model=\"gpt-3.5-turbo\", ) if resp.is\\\\\\_valid: return self raise ValueError(resp.error\\\\\\_messages) class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] Now when we use a correct citation, the LLM returns a valid response. resp = AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is the capital of France\", 3: \"Irrelevant data\", } }, ) print(resp.model\\\\\\_dump\\\\\\_json(indent=2)) Result:¶ { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ { \"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\" } \\\\\\] } When we have citations that don't exist in the context, the LLM returns an error message. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is not the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example:¶ 1 validation error for AnswerWithCitaton answer.0 Value error, Citation not found in context \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'body': 'Paris', 'substr... the capital of France'}, input\\\\\\_type=dict\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Example 3: Aligning Citations and Answers¶ In this example, we ensure that the provided answers are aligned with the given citations and context. The LLM is used to verify the alignment. We use the same Statements model as above, but we add a new model for the answer that also verifies the alignment of citations. Code Example:¶ class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] @model\\\\\\_validator(mode=\"after\") def validate\\\\\\_answer(self, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) resp: Validation = client.chat.completions.create( response\\\\\\_model=Validation, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Does the following answers match the question and the context?\\\\\\\\n\\\\\\\\nQuestion: {self.question}\\\\\\\\n\\\\\\\\nAnswer: {self.answer}\\\\\\\\n\\\\\\\\nContext: {context}\", } \\\\\\], model=\"gpt-3.5-turbo\", ) if resp.is\\\\\\_valid: return self raise ValueError(resp.error\\\\\\_messages) When we have a mismatch between the answer and the citation, the LLM returns an error message. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Texas\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example:¶ 1 validation error for AnswerWithCitaton Value error, The answer does not match the question and context \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'question': 'What is the...he capital of France'}\\\\\\]}, input\\\\\\_type=dict\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Conclusion¶ These examples demonstrate the potential of using Pydantic and OpenAI to enhance data accuracy through citation verification. While the LLM-based approach may not be efficient for runtime operations, it has exciting implications for generating a dataset of accurate responses. By leveraging this method during data generation, we can fine-tune a model that excels in citation accuracy. Similar to our last post on finetuning a better summarizer. If you like the content check out our GitHub as give us a star and checkout the library. Was this page helpful? Back to top Previous Introduction to Batch Processing using asyncio and Instructor Next Generators and LLM Streaming Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Welcome to the Instructor Blog - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/?q=",
    "html": "Skip to content Instructor Welcome to the Instructor Blog Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Blog Archive 2023 Table of contents Advanced Topics Learning Python Talks Structured Outputs with Anyscale Introduction to Caching in Python Generators and LLM Streaming Verifying LLM Citations with Pydantic Introduction to Batch Processing using asyncio and Instructor Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density AI Engineer Keynote: Pydantic is all you need Good LLM Validation is Just Good Validation Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation RAG is more than just embedding search Welcome to the Instructor Blog¶ The goal of the blog is to capture some content that does not neatly fit within documentation or the cookbooks. Advanced Topics¶ What is Query Understanding, how does it go beyond embeddings? How can one achieve GPT-4 level summaries using GPT-3.5-turbo? What are the basics of Guardrails and Validation in AI models? How does one validate citations in AI-generated content? What are the methods and benefits of fine-tuning and distillation in AI models? How can I use Anyscale with Instructor? Learning Python¶ How can I effectively cache my functions in Python? What are the fundamentals of batch processing with async in Python? How can I stream models to improve latency? Talks¶ What were the key insights and topics covered at the AI Engineering Summit 2023? 2023/12/15 2 min read Structured Outputs with Anyscale Open-source LLMS are gaining popularity, and the release of Anyscale's Mistral model has made it possible to obtain structured outputs using JSON schema at any scale. Instead of relying on a model's default output mode, you can utilize JSON schema to obtain structured outputs. This approach is a time-saving alternative to extensive prompt engineering. By the end of this blog post, you will learn how to effectively utilize the instructor at any scale. But before we proceed, let's first explore the concept of patching. Patching Instructor's patch enhances a openai api it with the following features: response\\\\\\_model in create calls that returns a pydantic model max\\\\\\_retries in create calls that retries the call if it fails by using a backoff strategy Learn More To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page. Anyscale The good news is that Anyscale employs the same OpenAI client, and its models support some of these output modes too! Getting access If you want to try this out for yourself check out the Anyscale website. You can get started here. Let's explore one of the models available in Anyscale's extensive collection! from openai import OpenAI from pydantic import BaseModel import instructor class UserDetails(BaseModel): name: str age: int # enables \\\\\\`response\\\\\\_model\\\\\\` in create call client = instructor.patch( OpenAI( base\\\\\\_url=\"https://api.endpoints.anyscale.com/v1\", api\\\\\\_key=\"\" ), # This uses Anyscale's json schema output mode mode=instructor.Mode.JSON\\\\\\_SCHEMA ) resp = client.chat.completions.create( model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class extractor\" }, { \"role\": \"user\", \"content\": 'Extract the following entities: \"Jason is 20\"' }, \\\\\\], response\\\\\\_model=UserDetails, ) print(resp) # >>> name='Jason' age=20 You can find more information about Anyscale's output mode support here. Continue reading 2023/11/26 7 min read Introduction to Caching in Python Instructor makes working with language models easy, but they are still computationally expensive. Today, we're diving into optimizing instructor code while maintaining the excellent DX offered by Pydantic models. We'll tackle the challenges of caching Pydantic models, typically incompatible with pickle, and explore solutions that use decorators like functools.cache. Then, we'll craft custom decorators with diskcache and redis to support persistent caching and distributed systems. Lets first consider our canonical example, using the OpenAI Python client to extract user details. import instructor from openai import OpenAI from pydantic import BaseModel # Enables \\\\\\`response\\\\\\_model\\\\\\` client = instructor.patch(OpenAI()) class UserDetail(BaseModel): name: str age: int def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Now imagine batch processing data, running tests or experiments, or simply calling extract multiple times over a workflow. We'll quickly run into performance issues, as the function may be called repeatedly, and the same data will be processed over and over again, costing us time and money. 1. functools.cache for Simple In-Memory Caching When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session or in an application where we don't need to persist the cache between sessions. import functools @functools.cache def extract(data): return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Changing the Model does not Invalidate the Cache Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result. Now we can call extract multiple times with the same argument, and the result will be cached in memory for faster access. import time start = time.perf\\\\\\_counter() # Using time.perf\\\\\\_counter() to measure the time taken to run the function is better than using time.time() because it's more accurate and less susceptible to system clock changes. model = extract(\"Extract jason is 25 years old\") print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") start = time.perf\\\\\\_counter() model = extract(\"Extract jason is 25 years old\") # The second time we call extract, the result is returned from the cache, and the function is not called. print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") >>> Time taken: 0.9267581660533324 >>> Time taken: 1.2080417945981026e-06 # The second call to extract is much faster because the result is returned from the cache! Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries. What is a decorator? 2. diskcache for Persistent, Large Data Caching Copy Caching Code When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data! import functools import inspect import instructor import diskcache from openai import OpenAI from pydantic import BaseModel client = instructor.patch(OpenAI()) cache = diskcache.Cache('./my\\\\\\_cache\\\\\\_directory') def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation # We use inspect.signature to get the function's return type annotation, which we use to validate the cached result. if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type We use Pydantic's model\\\\\\_validate\\\\\\_json to deserialize the cached result into a Pydantic model. return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence. 2. Redis Caching Decorator for Distributed Systems Copy Caching Code When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures. import redis import functools import inspect import json import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) cache = redis.Redis(\"localhost\") def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: # Assuming client.chat.completions.create returns a UserDetail instance return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types. Looking carefully If you look carefully at the code above you'll notice that we're using the same instructor\\\\\\_cache decorator as before. The implementatino is the same, but we're using a different caching backend! Conclusion Choosing the right caching strategy depends on your application's specific needs, such as the size and type of data, the need for persistence, and the system's architecture. Whether it's optimizing a function's performance in a small application or managing large datasets in a distributed environment, Python offers robust solutions to improve efficiency and reduce computational overhead. If you'd like to use this code, try to send it over to ChatGPT to understand it more, and to add additional features that might matter for you, for example, the cache isn't invalidated when your BaseModel changes, so you might want to encode the Model.model\\\\\\_json\\\\\\_schema() as part of the key. If you like the content check out our GitHub as give us a star and checkout the library. Continue reading 2023/11/26 6 min read Generators and LLM Streaming Latency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times. And what makes streaming possible? Generators! In this post, we're going to dive into the cool world of Python generators — these tools are more than just a coding syntax trick. We'll explore Python generators from the ground up and then delve into LLM streaming using the Instructor library. Python Generators: An Efficient Approach to Iterables Generators in Python are a game-changer for handling large data sets and stream processing. They allow functions to yield values one at a time, pausing and resuming their state, which is a faster and more memory-efficient approach compared to traditional collections that store all elements in memory. The Basics: Yielding Values A generator function in Python uses the yield keyword. It yields values one at a time, allowing the function to pause and resume its state. def count\\\\\\_to\\\\\\_3(): yield 1 yield 2 yield 3 for num in count\\\\\\_to\\\\\\_3(): print(num) 1 2 3 Advantages Over Traditional Collections Lazy Evaluation & reduced latency: The time to get the first element (or time-to-first-token in LLM land) from a generator is significantly lower. Generators only produce one value at a time, whereas accessing the first element of a collection will require that the whole collection be created first. Memory Efficiency: Only one item is in memory at a time. Maintain State: Automatically maintains state between executions. Let's see how much faster generators are and where they really shine: import time def expensive\\\\\\_func(x): \"\"\"Simulate an expensive operation.\"\"\" time.sleep(1) return x \\\\\\*\\\\\\* 2 def calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_list(func\\\\\\_input, func): \"\"\"Calculate using a list comprehension and return the first result with its computation time.\"\"\" start\\\\\\_perf = time.perf\\\\\\_counter() result = \\\\\\[func(x) for x in func\\\\\\_input\\\\\\]\\\\\\[0\\\\\\] end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (list): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") return result def calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_generator(func\\\\\\_input, func): \"\"\"Calculate using a generator and return the first result with its computation time.\"\"\" start\\\\\\_perf = time.perf\\\\\\_counter() result = next(func(x) for x in func\\\\\\_input) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (generator): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") return result # Prepare inputs for the function numbers = \\\\\\[1, 2, 3, 4, 5\\\\\\] # Benchmarking first\\\\\\_result\\\\\\_list = calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_list(numbers, expensive\\\\\\_func) first\\\\\\_result\\\\\\_gen = calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_generator(numbers, expensive\\\\\\_func) Time for first result (list): 5.02 seconds Time for first result (generator): 1.01 seconds The generator computes one expensive operation and returns the first result immediately, while the list comprehension computes the expensive operation for all elements in the list before returning the first result. Generator Expressions: A Shortcut Python also allows creating generators in a single line of code, known as generator expressions. They are syntactically similar to list comprehensions but use parentheses. squares = (x\\\\\\*x for x in range(10)) Use Cases in Real-World Applications Generators shine in scenarios like reading large files, data streaming (eg. llm token streaming), and pipeline creation for data processing. LLM Streaming If you've used ChatGPT, you'll see that the tokens are streamed out one by one, instead of the full response being shown at the end (can you imagine waiting for the full response??). This is made possible by generators. Here's how a vanilla openai generator looks: from openai import OpenAI # Set your OpenAI API key client = OpenAI( api\\\\\\_key=\"My API Key\", ) response\\\\\\_generator = client.chat.completions.create( model='gpt-3.5-turbo', messages=\\\\\\[ {'role': 'user', 'content': \"What are some good reasons to smile?\"} \\\\\\], temperature=0, stream=True ) for chunk in response\\\\\\_generator: print(chunk.choices\\\\\\[0\\\\\\].delta.content, end=\"\") This is great, but what if we want to do some structured extraction on this stream? For instance, we might want to render frontend components based on product rankings that are streamed out by an LLM. Should we wait for the entire stream to finish before extracting & validating the list of components or can we extract & validate the components in real time as they are streamed? In e-commerce, every millisecond matters so the time-to-first-render can differentiate a successful and not-so-successful e commerce store (and i know how a failing e commerce store feels :/ ). Let's see how we can use Instructor to handle extraction from this real time stream! E-commerce Product Ranking SCENARIO Imagine an e-commerce platform where we have: • a customer profile: this includes a detailed history of purchases, browsing behavior, product ratings, preferences in various categories, search history, and even responses to previous recommendations. This extensive data is crucial for generating highly personalized and relevant product suggestions. • a list of candidate products: these could be some shortlisted products we think the customer would like. Our goal is to re-rerank these candidate products for the best conversion and we'll use an LLM! STREAM PROCESSING User Data: Let's assume we have the following user profile: profile\\\\\\_data = \"\"\" Customer ID: 12345 Recent Purchases: \\\\\\[Laptop, Wireless Headphones, Smart Watch\\\\\\] Frequently Browsed Categories: \\\\\\[Electronics, Books, Fitness Equipment\\\\\\] Product Ratings: {Laptop: 5 stars, Wireless Headphones: 4 stars} Recent Search History: \\\\\\[best budget laptops 2023, latest sci-fi books, yoga mats\\\\\\] Preferred Brands: \\\\\\[Apple, AllBirds, Bench\\\\\\] Responses to Previous Recommendations: {Philips: Not Interested, Adidas: Not Interested} Loyalty Program Status: Gold Member Average Monthly Spend: $500 Preferred Shopping Times: Weekend Evenings ... \"\"\" We want to rank the following products for this user: products = \\\\\\[ {\"product\\\\\\_id\": 1, \"product\\\\\\_name\": \"Apple MacBook Air (2023) - Latest model, high performance, portable\"}, {\"product\\\\\\_id\": 2, \"product\\\\\\_name\": \"Sony WH-1000XM4 Wireless Headphones - Noise-canceling, long battery life\"}, {\"product\\\\\\_id\": 3, \"product\\\\\\_name\": \"Apple Watch Series 7 - Advanced fitness tracking, seamless integration with Apple ecosystem\"}, {\"product\\\\\\_id\": 4, \"product\\\\\\_name\": \"Kindle Oasis - Premium e-reader with adjustable warm light\"}, {\"product\\\\\\_id\": 5, \"product\\\\\\_name\": \"AllBirds Wool Runners - Comfortable, eco-friendly sneakers\"}, {\"product\\\\\\_id\": 6, \"product\\\\\\_name\": \"Manduka PRO Yoga Mat - High-quality, durable, eco-friendly\"}, {\"product\\\\\\_id\": 7, \"product\\\\\\_name\": \"Bench Hooded Jacket - Stylish, durable, suitable for outdoor activities\"}, {\"product\\\\\\_id\": 8, \"product\\\\\\_name\": \"GoPro HERO9 Black - 5K video, waterproof, for action photography\"}, {\"product\\\\\\_id\": 9, \"product\\\\\\_name\": \"Nespresso Vertuo Next Coffee Machine - Quality coffee, easy to use, compact design\"}, {\"product\\\\\\_id\": 10, \"product\\\\\\_name\": \"Project Hail Mary by Andy Weir - Latest sci-fi book from a renowned author\"} \\\\\\] Let's now define our models for structured extraction. Note: instructor will conveniently let us use Iterable to model an iterable of our class. In this case, once we define our product recommendation model, we can slap on Iterable to define what we ultimately want - a (ranked) list of product recommendations. import instructor from openai import OpenAI from typing import Iterable from pydantic import BaseModel client = instructor.patch(OpenAI(), mode=instructor.function\\\\\\_calls.Mode.JSON) class ProductRecommendation(BaseModel): product\\\\\\_id: str product\\\\\\_name: str Recommendations = Iterable\\\\\\[ProductRecommendation\\\\\\] Now let's use our instructor patch. Since we don't want to wait for all the tokens to finish, will set stream to True and process each product recommendation as it comes in: prompt = f\"Based on the following user profile:\\\\\\\\n{profile\\\\\\_data}\\\\\\\\nRank the following products from most relevant to least relevant:\\\\\\\\n\" + '\\\\\\\\n'.join(f\"{product\\\\\\['product\\\\\\_id'\\\\\\]} {product\\\\\\['product\\\\\\_name'\\\\\\]}\" for product in products) start\\\\\\_perf = time.perf\\\\\\_counter() recommendations\\\\\\_stream = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Iterable\\\\\\[ProductRecommendation\\\\\\], stream=True, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"}, {\"role\": \"user\", \"content\": prompt} \\\\\\] ) for product in recommendations\\\\\\_stream: print(product) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (generator): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") break product\\\\\\_id='1' product\\\\\\_name='Apple MacBook Air (2023)' Time for first result (generator): 4.33 seconds recommendations\\\\\\_stream is a generator! It yields the extracted products as it's processing the stream in real-time. Now let's get the same response without streaming and see how they compare. start\\\\\\_perf = time.perf\\\\\\_counter() recommendations\\\\\\_list = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Iterable\\\\\\[ProductRecommendation\\\\\\], stream=False, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"}, {\"role\": \"user\", \"content\": prompt} \\\\\\] ) print(recommendations\\\\\\_list\\\\\\[0\\\\\\]) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (list): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") product\\\\\\_id='1' product\\\\\\_name='Apple MacBook Air (2023)' Time for first result (list): 8.63 seconds Our web application now displays results faster. Even a 100ms improvement can lead to a 1% increase in revenue. FastAPI We can also take this and set up a streaming LLM API endpoint using FastAPI. Check out our docs on using FastAPI here! Key Takeaways To summarize, we looked at: • Generators in Python: A powerful feature that allows for efficient data handling with reduced latency • LLM Streaming: LLMs provide us generators to stream tokens and Instructor can let us validate and extract data from this stream. Real-time data validation ftw! Don't forget to check our GitHub for more resources and give us a star if you find the library helpful! If you have any questions or need further clarifications, feel free to reach out or dive into the Instructor library's documentation for more detailed information. Happy coding! Continue reading 2023/11/18 4 min read Verifying LLM Citations with Pydantic Ensuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification. We'll start with using a simple substring check to verify citations. Then we'll use instructor itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses. Example 1: Simple Substring Check In this example, we use the Statements class to verify if a given substring quote exists within a text chunk. If the substring is not found, an error is raised. Code Example: from typing import List, Optional from openai import OpenAI from pydantic import BaseModel, Field, ValidationError, ValidationInfo, field\\\\\\_validator, model\\\\\\_validator import instructor client = instructor.patch(OpenAI()) class Statements(BaseModel): body: str substring\\\\\\_quote: str @field\\\\\\_validator(\"substring\\\\\\_quote\") @classmethod def substring\\\\\\_quote\\\\\\_exists(cls, v: str, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) for text\\\\\\_chunk in context.values(): if v in text\\\\\\_chunk: # While we use a simple substring check in this example, we can use more complex techniques like regex or Levenshtein distance. return v raise ValueError(\"Could not find substring\\\\\\_quote \\\\\\`{v}\\\\\\` in contexts\") class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] Once the class is defined, we can use it to validate the context and raise an error if the substring is not found. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is not the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example: answer.0.substring\\\\\\_quote Value error, Could not find substring\\\\\\_quote \\\\\\`Paris is the capital of France\\\\\\` in contexts \\\\\\[type=value\\\\\\_error, input\\\\\\_value='Paris is the capital of France', input\\\\\\_type=str\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Pydantic raises a validation error when the substring\\\\\\_quote attribute does not exist in the context. This approach can be used to validate more complex data using techniques like regex or Levenshtein distance. Example 2: Using LLM for Verification This approach leverages OpenAI's LLM to validate citations. If the citation does not exist in the context, the LLM returns an error message. Code Example: class Validation(BaseModel): is\\\\\\_valid: bool error\\\\\\_messages: Optional\\\\\\[str\\\\\\] = Field(None, description=\"Error messages if any\") class Statements(BaseModel): body: str substring\\\\\\_quote: str @model\\\\\\_validator(mode=\"after\") def substring\\\\\\_quote\\\\\\_exists(self, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) resp: Validation = client.chat.completions.create( response\\\\\\_model=Validation, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Does the following citation exist in the following context?\\\\\\\\n\\\\\\\\nCitation: {self.substring\\\\\\_quote}\\\\\\\\n\\\\\\\\nContext: {context}\", } \\\\\\], model=\"gpt-3.5-turbo\", ) if resp.is\\\\\\_valid: return self raise ValueError(resp.error\\\\\\_messages) class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] Now when we use a correct citation, the LLM returns a valid response. resp = AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is the capital of France\", 3: \"Irrelevant data\", } }, ) print(resp.model\\\\\\_dump\\\\\\_json(indent=2)) Result: { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ { \"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\" } \\\\\\] } When we have citations that don't exist in the context, the LLM returns an error message. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is not the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example: 1 validation error for AnswerWithCitaton answer.0 Value error, Citation not found in context \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'body': 'Paris', 'substr... the capital of France'}, input\\\\\\_type=dict\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Example 3: Aligning Citations and Answers In this example, we ensure that the provided answers are aligned with the given citations and context. The LLM is used to verify the alignment. We use the same Statements model as above, but we add a new model for the answer that also verifies the alignment of citations. Code Example: class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] @model\\\\\\_validator(mode=\"after\") def validate\\\\\\_answer(self, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) resp: Validation = client.chat.completions.create( response\\\\\\_model=Validation, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Does the following answers match the question and the context?\\\\\\\\n\\\\\\\\nQuestion: {self.question}\\\\\\\\n\\\\\\\\nAnswer: {self.answer}\\\\\\\\n\\\\\\\\nContext: {context}\", } \\\\\\], model=\"gpt-3.5-turbo\", ) if resp.is\\\\\\_valid: return self raise ValueError(resp.error\\\\\\_messages) When we have a mismatch between the answer and the citation, the LLM returns an error message. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Texas\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example: 1 validation error for AnswerWithCitaton Value error, The answer does not match the question and context \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'question': 'What is the...he capital of France'}\\\\\\]}, input\\\\\\_type=dict\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Conclusion These examples demonstrate the potential of using Pydantic and OpenAI to enhance data accuracy through citation verification. While the LLM-based approach may not be efficient for runtime operations, it has exciting implications for generating a dataset of accurate responses. By leveraging this method during data generation, we can fine-tune a model that excels in citation accuracy. Similar to our last post on finetuning a better summarizer. If you like the content check out our GitHub as give us a star and checkout the library. Continue reading 2023/11/13 6 min read Introduction to Batch Processing using asyncio and Instructor Today, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using instructor and learn how to use asyncio.gather and asyncio.as\\\\\\_completed for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using asyncio.Semaphore. Github Example If you want to run the code examples in this article, you can find them on jxnl/instructor We will start by defining an async function that calls openai to extract data, and then examine four different ways to execute it. We will discuss the pros and cons of each approach and analyze the results of running them on a small batch. Understanding asyncio asyncio is a Python library that enables writing concurrent code using the async/await syntax. It is particularly useful for IO-bound and structured network code. If you are familiar with OpenAI's SDK, you might have encountered two classes: OpenAI() and AsyncOpenAI(). Today, we will be using the AsyncOpenAI() class, which processes data asynchronously. By utilizing these tools in web applications or batch processing, we can significantly improve performance by handling multiple requests concurrently instead of sequentially. Understanding async and await We will be using the async and await keywords to define asynchronous functions. The async keyword is used to define a function that returns a coroutine object. The await keyword is used to wait for the result of a coroutine object. If you want to understand the deeper details of asyncio, I recommend reading this article by Real Python. Understanding gather vs as\\\\\\_completed In this post we'll show two ways to run tasks concurrently: asyncio.gather and asyncio.as\\\\\\_completed. The gather method is used to run multiple tasks concurrently and return the results as a list. The as\\\\\\_completed returns a iterable is used to run multiple tasks concurrently and return the results as they complete. Another great resource on the differences between the two can be found here. Example: Batch Processing In this example, we will demonstrate how to use asyncio for batch processing tasks, specifically for extracting and processing data concurrently. The script will extract data from a list of texts and process it concurrently using asyncio. import instructor from pydantic import BaseModel from openai import AsyncOpenAI # Enables \\\\\\`response\\\\\\_model\\\\\\` in \\\\\\`create\\\\\\` method client = instructor.apatch(AsyncOpenAI()) We use instructor.apatch to patch the create method of AsyncOpenAI to accept a response\\\\\\_model argument. This is because the create method of AsyncOpenAI does not accept a response\\\\\\_model argument without this patch. class Person(BaseModel): name: str age: int async def extract\\\\\\_person(text: str) -> Person: return await client.chat.completions.create( We use await here to wait for the response from the server before we return the result. This is because create returns a coroutine object, not the result of the coroutine. model=\"gpt-3.5-turbo\", messages=\\\\\\[ {\"role\": \"user\", \"content\": text}, \\\\\\], response\\\\\\_model=Person, ) Notice that now there are async and await keywords in the function definition. This is because we're using the asyncio library to run the function concurrently. Now lets define a batch of texts to process. dataset = \\\\\\[ \"My name is John and I am 20 years old\", \"My name is Mary and I am 21 years old\", \"My name is Bob and I am 22 years old\", \"My name is Alice and I am 23 years old\", \"My name is Jane and I am 24 years old\", \"My name is Joe and I am 25 years old\", \"My name is Jill and I am 26 years old\", \\\\\\] for loop: Running tasks sequentially. persons = \\\\\\[\\\\\\] for text in dataset: person = await extract\\\\\\_person(text) persons.append(person) Even though there is an await keyword, we still have to wait for each task to finish before starting the next one. This is because we're using a for loop to iterate over the dataset. This method, which uses a for loop, will be the slowest among the four methods discussed today. asyncio.gather: Running tasks concurrently. async def gather(): tasks\\\\\\_get\\\\\\_persons = \\\\\\[extract\\\\\\_person(text) for text in dataset\\\\\\] all\\\\\\_persons = await asyncio.gather(\\\\\\*tasks\\\\\\_get\\\\\\_persons) We use await here to wait for all the tasks to finish before assigning the result to all\\\\\\_persons. This is because asyncio.gather returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.as\\\\\\_completed to achieve the same result. Using asyncio.gather allows us to return all the results at once. It is an effective way to speed up our code, but it's not the only way. Particularly, if we have a large dataset, we might not want to wait for everything to finish before starting to process the results. This is where asyncio.as\\\\\\_completed comes into play. asyncio.as\\\\\\_completed: Handling tasks as they complete. async def as\\\\\\_completed(): all\\\\\\_persons = \\\\\\[\\\\\\] tasks\\\\\\_get\\\\\\_persons = \\\\\\[extract\\\\\\_person(text) for text in dataset\\\\\\] for person in asyncio.as\\\\\\_completed(tasks\\\\\\_get\\\\\\_persons): all\\\\\\_persons.append(await person) We use await here to wait for each task to complete before appending it to the list. This is because as\\\\\\_completed returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.gather to achieve the same result. This method is a great way to handle large datasets. We can start processing the results as they come in, especially if we are streaming data back to a client. However, these methods aim to complete as many tasks as possible as quickly as possible. This can be problematic if we want to be considerate to the server we're making requests to. This is where rate limiting comes into play. While there are libraries available to assist with rate limiting, for our initial defense, we will use a semaphore to limit the number of concurrent requests we make. Ordering of results Its important to note that the order of the results will not be the same as the order of the dataset. This is because the tasks are completed in the order they finish, not the order they were started. If you need to preserve the order of the results, you can use asyncio.gather instead. Rate-Limited Gather: Using semaphores to limit concurrency. sem = asyncio.Semaphore(2) async def rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text: str, sem: Semaphore) -> Person: async with sem: We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to. return await extract\\\\\\_person(text) async def rate\\\\\\_limited\\\\\\_gather(sem: Semaphore): tasks\\\\\\_get\\\\\\_persons = \\\\\\[rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text, sem) for text in dataset\\\\\\] resp = await asyncio.gather(\\\\\\*tasks\\\\\\_get\\\\\\_persons) Rate-Limited As Completed: Using semaphores to limit concurrency. sem = asyncio.Semaphore(2) async def rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text: str, sem: Semaphore) -> Person: async with sem: We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to. return await extract\\\\\\_person(text) async def rate\\\\\\_limited\\\\\\_as\\\\\\_completed(sem: Semaphore): all\\\\\\_persons = \\\\\\[\\\\\\] tasks\\\\\\_get\\\\\\_persons = \\\\\\[rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text, sem) for text in dataset\\\\\\] for person in asyncio.as\\\\\\_completed(tasks\\\\\\_get\\\\\\_persons): all\\\\\\_persons.append(await person) We use await here to wait for each task to complete before appending it to the list. This is because as\\\\\\_completed returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.gather to achieve the same result. Now that we have seen the code, let's examine the results of processing 7 texts. As the prompts become longer or if we use GPT-4, the differences between these methods will become more pronounced. Other Options Its important to also note that here we are using a semaphore to limit the number of concurrent requests. However, there are other ways to limit concurrency esp since we have rate limit information from the openai request. You can imagine using a library like ratelimit to limit the number of requests per second. OR catching rate limit exceptions and using tenacity to retry the request after a certain amount of time. tenacity aiolimiter Results As you can see, the for loop is the slowest, while asyncio.as\\\\\\_completed and asyncio.gather are the fastest without any rate limiting. Method Execution Time Rate Limited (Semaphore) For Loop 6.17 seconds Asyncio.gather 0.85 seconds Asyncio.as\\\\\\_completed 0.95 seconds Asyncio.gather 3.04 seconds 2 Asyncio.as\\\\\\_completed 3.26 seconds 2 Practical implications of batch processing The choice of approach depends on the task's nature and the desired balance between speed and resource utilization. Here are some guidelines to consider: Use asyncio.gather for handling multiple independent tasks quickly. Apply asyncio.as\\\\\\_completed for large datasets to process tasks as they complete. Implement rate-limiting to avoid overwhelming servers or API endpoints. If you find the content helpful or want to try out Instructor, please visit our GitHub page and give us a star! Continue reading 2023/11/05 15 min read Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor In this article, we'll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4's iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density. By the end you'll end up with a GPT 3.5 model, (fine-tuned using Instructor's great tooling), capable of producing summaries that rival the effectiveness of Chain of Density \\\\\\[Adams et al. (2023)\\\\\\]. As always, all code is readily available in our examples/chain-of-density folder in our repo for your reference. Datasets and Colab Notebook Part 1) Chain of Density Summarizing extensive texts with AI can be challenging, often relying on inconsistent techniques. Their novel method, Chain Of Density prompting, enhances AI-based text summarization, outperforming human-generated summaries. Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density. First introduced in the paper - From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. The team has found that this method is able to consistently beats similar summaries written by human annotators. Implementation Details Original Prompt We can break down the original process into smaller api calls. This allows us to introduce validation at each step to ensure that we're getting the results that we want. Original Chain of Density Prompt Improved process with Instructor Data Modelling Before we begin modelling the data, let's make sure we install all of our dependencies pip install instructor aiohttp rich INITIAL SUMMARY Let's start by walking through some of the data models that we'll be using as the response\\\\\\_model for our open ai function calls Firstly, we'll need a data model for the initial summary that we will be generating. We'll take the description of this class straight from the original prompt. It's important to note that these docstrings serve a purpose, they are directly used by the LLM when generating the outputs. A quick note on Docstrings class InitialSummary(BaseModel): \"\"\" This is an initial summary which should be long ( 4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words. \"\"\" summary: str = Field( ..., description=\"This is a summary of the article provided which is overly verbose and uses fillers. It should be roughly 80 words in length\", ) REWRITTEN SUMMARY We'll also need one additional class to help model the rewritten schema class RewrittenSummary(BaseModel): \"\"\" This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. Guidelines - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" - Missing entities can appear anywhere in the new summary An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. \"\"\" summary: str = Field( ..., description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\", ) absent: List\\\\\\[str\\\\\\] = Field( ..., default\\\\\\_factory=list, description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\", ) missing: List\\\\\\[str\\\\\\] = Field( default\\\\\\_factory=list, description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\", ) Using Pydantic Validators with Instructor For a more in-depth walkthrough on how to use Pydantic validators with the Instructor library, we recommend checking out our previous article on LLM validation - Good LLM Validation is just Good Validation Ideally, we'd like for Missing to have a length between 1 and 3, Absent to be an empty list and for our rewritten summaries to keep a minimum entity density. With Instructor, we can implement this logic using native Pydantic validators that are simply declared as part of the class itself. import nltk import spacy nlp = spacy.load(\"en\\\\\\_core\\\\\\_web\\\\\\_sm\") @field\\\\\\_validator(\"summary\") def min\\\\\\_length(cls, v: str): tokens = nltk.word\\\\\\_tokenize(v) Similar to the original paper, we utilize the NLTK word tokenizer to count the number of tokens within our generated sentences. We aim for at least 60 tokens in our generated summary so that we don't lose information. num\\\\\\_tokens = len(tokens) if num\\\\\\_tokens < 60: raise ValueError( \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\" ) return v @field\\\\\\_validator(\"missing\") def has\\\\\\_missing\\\\\\_entities(cls, missing\\\\\\_entities: List\\\\\\[str\\\\\\]): if len(missing\\\\\\_entities) == 0: raise ValueError( \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\" ) return missing\\\\\\_entities @field\\\\\\_validator(\"absent\") def has\\\\\\_no\\\\\\_absent\\\\\\_entities(cls, absent\\\\\\_entities: List\\\\\\[str\\\\\\]): absent\\\\\\_entity\\\\\\_string = \",\".join(absent\\\\\\_entities) if len(absent\\\\\\_entities) > 0: print(f\"Detected absent entities of {absent\\\\\\_entity\\\\\\_string}\") raise ValueError( f\"Do not omit the following Entities {absent\\\\\\_entity\\\\\\_string} from the new summary\" ) return absent\\\\\\_entities @field\\\\\\_validator(\"summary\") def min\\\\\\_entity\\\\\\_density(cls, v: str): tokens = nltk.word\\\\\\_tokenize(v) num\\\\\\_tokens = len(tokens) # Extract Entities doc = nlp(v) We also use the spaCy library to calculate the entity density of the generated summary. num\\\\\\_entities = len(doc.ents) density = num\\\\\\_entities / num\\\\\\_tokens if density < 0.08: We also implement a minimum entity density so that we stay within a given range. 0.08 is arbitrarily chosen in this case raise ValueError( f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\" ) return v Putting it all Together Now that we have our models and the rough flow figured out, let's implement a function to summarize a piece of text using Chain Of Density summarization. from openai import OpenAI import instructor client = instructor.patch(OpenAI()) We need to apply a patch function on the OpenAI client for us to get all of the benefits that Instructor provides. With a simple patch, we can get automatic type coercion of our outputs and automatic retries for invalid outputs out of the box! def summarize\\\\\\_article(article: str, summary\\\\\\_steps: int = 3): summary\\\\\\_chain = \\\\\\[\\\\\\] # We first generate an initial summary summary: InitialSummary = client.chat.completions.create( We first generate an initial summary. Note here that we explictly ask for a summary that has 80 words and is lengthy with overly verbose fillers in the system prompt model=\"gpt-4-0613\", response\\\\\\_model=InitialSummary, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": \"The generated summary should be about 80 words.\", }, \\\\\\], max\\\\\\_retries=2, ) prev\\\\\\_summary = None summary\\\\\\_chain.append(summary.summary) for i in range(summary\\\\\\_steps): missing\\\\\\_entity\\\\\\_message = ( \\\\\\[\\\\\\] if prev\\\\\\_summary is None else \\\\\\[ { \"role\": \"user\", \"content\": f\"Please include these Missing Entities: {','.join(prev\\\\\\_summary.missing)}\", }, \\\\\\] ) new\\\\\\_summary: RewrittenSummary = client.chat.completions.create( We slightly modify the original system prompt used in the original paper to perform a rewrite of the summary. Using Instructor, we also get validation of the generated output with our field\\\\\\_validators that we defined above model=\"gpt-4-0613\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"\"\" You are going to generate an increasingly concise,entity-dense summary of the following article. Perform the following two tasks - Identify 1-3 informative entities from the following article which is missing from the previous summary - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities Guidelines - Make every word count: re-write the previous summary to improve flow and make space for additional entities - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\". - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article. - Missing entities can appear anywhere in the new summary - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \"\"\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": f\"Here is the previous summary: {summary\\\\\\_chain\\\\\\[-1\\\\\\]}\", }, \\\\\\*missing\\\\\\_entity\\\\\\_message, \\\\\\], max\\\\\\_retries=3, If you've chosen a value that is larger than 0.08, make sure to increase this value in case you need to do multiple rewrites max\\\\\\_tokens=1000, response\\\\\\_model=RewrittenSummary, ) summary\\\\\\_chain.append(new\\\\\\_summary.summary) prev\\\\\\_summary = new\\\\\\_summary return summary\\\\\\_chain This summarization function yields a result which triples the number of entities while maintaining the same number of tokens. We can also see that stylistically, the summary is a lot more natural. First Iteration This article discusses the highly-anticipated boxing match between Manny Pacquiao and Floyd Mayweather. The article revolves around Manny Pacquiao's statements about his upcoming fight and his preparations for the same. A portion of the article provides details about the financial stipulations of the match and its significance in the sporting arena. Quotes from Pacquiao illustrating his determination and his battle strategy are highlighted. The tone of the article is largely centered around creating a build-up to the upcoming mega event. Final Iteration Manny Pacquiao, the Filipino boxer, anticipates the forthcoming May 2 showdown at the MGM Grand as the fight of his life, against the undefeated American Floyd Mayweather, in a $300m bout. Despite being seen as the underdog in this high-stakes Las Vegas match, Pacquiao is confident, promising a warrior's spirit and assuring the fans who have been awaiting this encounter for a decade, that it will indeed be the biggest sporting spectacle in history worthy of their anticipation Part 2) Fine-Tuning In this section, we'll look into how to fine-tune a GPT 3.5 model so that it is able to perform at an equivalent level as a GPT-4 model. We'll then compare the performance of our model against that of GPT-4 to see how it stacks up. Creating a Training Set In order to prevent any contamination of data during testing, we randomly sampled 120 articles from the griffin/chain-of-density dataset and split these articles into a train.csv and a test.csv file which we uploaded to Hugging Face. Now, we just neeed to import the Instructions module from the Instructor package which allows you to generate a nicely formatted .jsonl file to be used for fine-tuning from typing import List from chain\\\\\\_of\\\\\\_density import summarize\\\\\\_article In this example, we're using the summarize\\\\\\_article that we defined up above. We saved it in a local file called chain\\\\\\_of\\\\\\_density.py, hence the import import csv import logging import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) We patch the default OpenAI client so that we can use the Instructor library with it logging.basicConfig(level=logging.INFO) We also need to configure logging at the INFO level. This is very important, if this is not configured, your output will not be generated. instructions = instructor.Instructions( name=\"Chain Of Density\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"generated.jsonl\")\\\\\\], openai\\\\\\_client=client, ) class GeneratedSummary(BaseModel): \"\"\" This represents a highly concise summary that includes as many entities as possible from the original source article. An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. Guidelines - Make every word count - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" \"\"\" summary: str = Field( ..., description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \", ) @instructions.distil We instantiate a Instruction object which will help us handle the conversion of our function calls into a valid .jsonl file. We also define the name of the .jsonl file in the log\\\\\\_handlers parameter def distil\\\\\\_summarization(text: str) -> GeneratedSummary: summary\\\\\\_chain: List\\\\\\[str\\\\\\] = summarize\\\\\\_article(text) return GeneratedSummary(summary=summary\\\\\\_chain\\\\\\[-1\\\\\\]) We add in an instructions.distil annotation so that we automatically capture the input and output of the function we'd like to fine-tune our model to output with open(\"train.csv\", \"r\") as file: reader = csv.reader(file) next(reader) # Skip the header for article, summary in reader: # Run Distillisation to generate the values distil\\\\\\_summarization(article) Rate Limiting We recommend running this script on a small subset of the dataset first to test you've got everything configured nicely. Don't forget to add in rate limiting error handling with tenacity and set the OPENAI\\\\\\_API\\\\\\_KEY shell environment variable before running any subsequent commands Creating Fine-Tuning Jobs Once we run this script, we'll have a new file called generated.jsonl in our local repository. Now all that's left is to run the command below to start fine-tuning your first model! instructor jobs create-from-file generated.jsonl Finetuning Reference Once the job is complete, all we need to do is to then change the annotation in the function call to distil\\\\\\_summarization in our original file above to start using our new model. @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard def distil\\\\\\_summarization(text: str) -> GeneratedSummary: summary\\\\\\_chain: List\\\\\\[str\\\\\\] = summarize\\\\\\_article(text) return GeneratedSummary(summary=summary\\\\\\_chain\\\\\\[-1\\\\\\]) With that, you've now got your own fine-tuned model ready to go and serve data in production. We've seen how Instructor can make your life easier, from fine-tuning to distillation. Results and Benchmarks We'll be comparing the following models in 3 ways using 20 articles that were not used for fine-tuning. Entity Density : This is entities per token, the higher the better for density. Latency : Time to last token generated in seconds Costs : Total cost to generate outputs - we break down the cost into training and inference costs for easy reference 3.5 Finetuned (n) This is a GPT 3.5 model that we fine-tuned on n examples. Each model was finetuned for 4-5 epochs ( This was automatically decided by the OpenAI scheduler ) GPT-4 (COD) This is a GPT4 model which we applied 3 rounds of Chain Of Density rewrites to generate a summary with using the methodology above GPT-3.5 (Vanilla) This is a GPT 3.5 model that we asked to generate entity-dense summaries which were concise. Summaries were generated in a single pass targetting about 80-90 tokens. Model Mean Latency (s) Mean Entity Density 3.5 Finetuned (20) 2.1 0.15 3.5 Finetuned (50) 2.1 0.14 3.5 Finetuned (76) 2.1 0.14 GPT-3.5 (Vanilla) 16.8 0.12 GPT-4 (COD) 49.5 0.15 Finetuning Datasets Using the OpenAI Usage Dashboard, we can calculate the cost of generating 20 summaries as seen below. Model Training Cost ($) Inference Cost ($) Tokens Used Total Cost ($) GPT-3.5 (Vanilla) - 0.20 51,162 0.2 3.5 Finetuned (20) 0.7 0.20 56,573 0.8 3.5 Finetuned (50) 1.4 0.17 49,057 1.3 3.5 Finetuned (76) 1.8 0.17 51,583 2.5 GPT-4 (COD) - 12.9 409,062 12.9 Here, we can see that GPT-4 has an approximate inference cost of 0.65 per summary while our finetuned models have an inference cost of 0.0091 per summary which is ~ 72x cheaper. Interestingly, the model finetuned with the least examples seems to outperform the others. While the reason for this is unknown, a few potential reasons could be that either we didn't train for sufficient epochs ( We chose the default 5 epochs ) or that the models started learning to imitate other behaviour such as more abstract writing styles from the larger variety of samples, resulting in a decrease in entity density. Conclusions Finetuning this iterative method was 20-40x faster while improving overall performance, resulting in massive efficiency gains by finetuning and distilling capabilities into specialized models. We've seen how Instructor can make your life easier, from data modeling to distillation and finetuning. If you enjoy the content or want to try out instructor check out the github and don't forget to give us a star! Continue reading 2023/11/02 1 min read AI Engineer Keynote: Pydantic is all you need Click here to watch the full talk Last month, I ventured back onto the speaking circuit at the inaugural AI Engineer Summit, sharing insights on leveraging Pydantic for effective prompt engineering. I dove deep into what is covered in our documentation and standard blog posts, I'd genuinely appreciate any feedback on the talk – every bit helps in refining the art. So, take a moment to check out the full talk here, and let's continue pushing the boundaries of what's possible. Continue reading 2023/10/23 10 min read Good LLM Validation is Just Good Validation What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here. Validation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like Pydantic and Instructor. We validate these outputs using a validation function which conforms to the structure seen below. def validation\\\\\\_function(value): if condition(value): raise ValueError(\"Value is not valid\") return mutation(value) What is Instructor? Instructor helps to ensure you get the exact response type you're looking for when using openai's function call api. Once you've defined the Pydantic model for your desired response, Instructor handles all the complicated logic in-between - from the parsing/validation of the response to the automatic retries for invalid responses. This means that we can build in validators 'for free' and have a clear separation of concerns between the prompt and the code that calls openai. from openai import OpenAI import instructor # pip install instructor from pydantic import BaseModel # This enables response\\\\\\_model keyword # from client.chat.completions.create client = instructor.patch(OpenAI()) To simplify your work with OpenAI models and streamline the extraction of Pydantic objects from prompts, we offer a patching mechanism for the ChatCompletion class. class UserDetail(BaseModel): name: str age: int user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] max\\\\\\_retries=3 Invalid responses that fail to be validated succesfully will trigger up to as many reattempts as you define. ) assert user.name == \"Jason\" As long as you pass in a response\\\\\\_model parameter to the ChatCompletion api call, the returned object will always be a validated Pydantic object. assert user.age == 25 In this post, we'll explore how to evolve from static, rule-based validation methods to dynamic, machine learning-driven ones. You'll learn to use Pydantic and Instructor to leverage language models and dive into advanced topics like content moderation, validating chain of thought reasoning, and contextual validation. Let's examine how these approaches with a example. Imagine that you run a software company who wants to ensure you never serve hateful and racist content. This isn't an easy job since the language around these topics change very quickly and frequently. Software 1.0: Introduction to Validations in Pydantic A simple method could be to compile a list of different words that are often associated with hate speech. For simplicity, let's assume that we've found that the words Steal and Rob are good predictors of hateful speech from our database. We can modify our validation structure above to accomodate this. This will throw an error if we pass in a string like Let's rob the bank! or We should steal from the supermarkets. Pydantic offers two approaches for this validation: using the field\\\\\\_validator decorator or the Annotated hints. Using field\\\\\\_validator decorator We can use the field\\\\\\_validator decorator to define a validator for a field in Pydantic. Here's a quick example of how we might be able to do so. from pydantic import BaseModel, ValidationError, field\\\\\\_validator from pydantic.fields import Field class UserMessage(BaseModel): message: str @field\\\\\\_validator('message') def message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words(cls, v: str) -> str: for word in v.split(): We split the sentence into its individual words and iterate through each of the words. We then try to see if any of these words are in our blacklist which in this case is just rob and steal if word.lower() in {'rob','steal'}: raise ValueError(f\"\\\\\\`{word}\\\\\\` was found in the message \\\\\\`{v}\\\\\\`\") return v try: UserMessage(message=\"This is a lovely day\") UserMessage(message=\"We should go and rob a bank\") except ValidationError as e: print(e) Since the message This is a lovely day does not have any blacklisted words, no errors are thrown. However, in the given example above, the validation fails for the message We should go and rob a bank due to the presence of the word rob and the corresponding error message is displayed. 1 validation error for UserMessage message Value error, \\\\\\`rob\\\\\\` was found in the message \\\\\\`We should go and rob a bank\\\\\\` \\\\\\[type=value\\\\\\_error, input\\\\\\_value='We should go and rob a bank', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Using Annotated Alternatively, you can use the Annotated function to perform the same validation. Here's an example where we utilise the same function we started with. from pydantic import BaseModel, ValidationError from typing import Annotated from pydantic.functional\\\\\\_validators import AfterValidator def message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words(value:str): for word in value.split(): if word.lower() in {'rob','steal'}: raise ValueError(f\"\\\\\\`{word}\\\\\\` was found in the message \\\\\\`{value}\\\\\\`\") return value class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words)\\\\\\] try: UserMessage(message=\"This is a lovely day\") UserMessage(message=\"We should go and rob a bank\") except ValidationError as e: print(e) This code snippet achieves the same validation result. If the user message contains any of the words in the blacklist, a ValueError is raised and the corresponding error message is displayed. 1 validation error for UserMessage message Value error, \\\\\\`rob\\\\\\` was found in the message \\\\\\`We should go and rob a bank\\\\\\` \\\\\\[type=value\\\\\\_error, input\\\\\\_value='We should go and rob a bank', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Validation is a fundamental concept in software development and remains the same when applied to AI systems. Existing programming concepts should be leveraged when possible instead of introducing new terms and standards. The underlying principles of validation remain unchanged. Suppose now that we've gotten a new message - Violence is always acceptable, as long as we silence the witness. Our original validator wouldn't throw any errors when passed this new message since it uses neither the words rob or steal. However, it's clear that it is not a message which should be published. How can we ensure that our validation logic can adapt to new challenges? Software 3.0: Validation for LLMs or powered by LLMs Building upon the understanding of simple field validators, let's delve into probabilistic validation in software 3.0, (prompt engineering). We'll introduce an LLM-powered validator called llm\\\\\\_validator that uses a statement to verify the value. We can get around this by using the inbuilt llm\\\\\\_validator class from Instructor. from instructor import llm\\\\\\_validator from pydantic import BaseModel, ValidationError from typing import Annotated from pydantic.functional\\\\\\_validators import AfterValidator class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(llm\\\\\\_validator(\"don't say objectionable things\"))\\\\\\] try: UserMessage(message=\"Violence is always acceptable, as long as we silence the witness\") except ValidationError as e: print(e) This produces the following error message as seen below 1 validation error for UserMessage message Assertion failed, The statement promotes violence, which is objectionable. \\\\\\[type=assertion\\\\\\_error, input\\\\\\_value='Violence is always accep... we silence the witness', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/assertion\\\\\\_error The error message is generated by the language model (LLM) rather than the code itself, making it helpful for re-asking the model in a later section. To better understand this approach, let's see how to build an llm\\\\\\_validator from scratch. Creating Your Own Field Level llm\\\\\\_validator Building your own llm\\\\\\_validator can be a valuable exercise to get started with Instructor and create custom validators. Before we continue, let's review the anatomy of a validator: def validation\\\\\\_function(value): if condition(value): raise ValueError(\"Value is not valid\") return value As we can see, a validator is simply a function that takes in a value and returns a value. If the value is not valid, it raises a ValueError. We can represent this using the following structure: class Validation(BaseModel): is\\\\\\_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\") error\\\\\\_message: Optional\\\\\\[str\\\\\\] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\") Using this structure, we can implement the same logic as before and utilize Instructor to generate the validation. import instructor from openai import OpenAI # Enables \\\\\\`response\\\\\\_model\\\\\\` and \\\\\\`max\\\\\\_retries\\\\\\` parameters client = instructor.patch(OpenAI()) def validator(v): statement = \"don't say objectionable things\" resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Does \\\\\\`{v}\\\\\\` follow the rules: {statement}\", }, \\\\\\], # this comes from client = instructor.patch(OpenAI()) response\\\\\\_model=Validation, The new parameter of response\\\\\\_model comes from client = instructor.patch(OpenAI()) and does not exist in the original OpenAI SDK. This allows us to pass in the Pydantic model that we want as a response. ) if not resp.is\\\\\\_valid: raise ValueError(resp.error\\\\\\_message) return v Now we can use this validator in the same way we used the llm\\\\\\_validator from Instructor. class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(validator)\\\\\\] Writing more complex validations Validating Chain of Thought A popular way of prompting large language models nowadays is known as chain of thought. This involves getting a model to generate reasons and explanations for an answer to a prompt. We can utilise Pydantic and Instructor to perform a validation to check of the reasoning is reasonable, given both the answer and the chain of thought. To do this we can't build a field validator since we need to access multiple fields in the model. Instead we can use a model validator. def validate\\\\\\_chain\\\\\\_of\\\\\\_thought(values): chain\\\\\\_of\\\\\\_thought = values\\\\\\[\"chain\\\\\\_of\\\\\\_thought\"\\\\\\] answer = values\\\\\\[\"answer\"\\\\\\] resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Verify that \\\\\\`{answer}\\\\\\` follows the chain of thought: {chain\\\\\\_of\\\\\\_thought}\", }, \\\\\\], # this comes from client = instructor.patch(OpenAI()) response\\\\\\_model=Validation, ) if not resp.is\\\\\\_valid: raise ValueError(resp.error\\\\\\_message) return values We can then take advantage of the model\\\\\\_validator decorator to perform a validation on a subset of the model's data. We're defining a model validator here which runs before Pydantic parses the input into its respective fields. That's why we have a before keyword used in the model\\\\\\_validator class. from pydantic import BaseModel, model\\\\\\_validator class AIResponse(BaseModel): chain\\\\\\_of\\\\\\_thought: str answer: str @model\\\\\\_validator(mode='before') @classmethod def chain\\\\\\_of\\\\\\_thought\\\\\\_makes\\\\\\_sense(cls, data: Any) -> Any: # here we assume data is the dict representation of the model # since we use 'before' mode. return validate\\\\\\_chain\\\\\\_of\\\\\\_thought(data) Now, when you create a AIResponse instance, the chain\\\\\\_of\\\\\\_thought\\\\\\_makes\\\\\\_sense validator will be invoked. Here's an example: try: resp = AIResponse( chain\\\\\\_of\\\\\\_thought=\"1 + 1 = 2\", answer=\"The meaning of life is 42\" ) except ValidationError as e: print(e) If we create a AIResponse instance with an answer that does not follow the chain of thought, we will get an error. 1 validation error for AIResponse Value error, The statement 'The meaning of life is 42' does not follow the chain of thought: 1 + 1 = 2. \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'chain\\\\\\_of\\\\\\_thought': '1 +... meaning of life is 42'}, input\\\\\\_type=dict\\\\\\] Validating Citations From Original Text Let's see a more concrete example. Let's say that we've asked our model a question about some text source and we want to validate that the generated answer is supported by the source. This would allow us to minimize hallucinations and prevent statements that are not backed by the original text. While we could verify this by looking up the original source manually, a more scalable approach is to use a validator to do this automatically. We can pass in additional context to our validation functions using the model\\\\\\_validate function in Pydantic so that our models have more information to work with when performing validation. This context is a normal python dictionary and can be accessed inside the info argument in our validator functions. from pydantic import ValidationInfo,BaseModel,field\\\\\\_validator class AnswerWithCitation(BaseModel): answer: str citation: str @field\\\\\\_validator('citation') @classmethod def citation\\\\\\_exists(cls, v: str, info: ValidationInfo): This info object corresponds to the value of context that we pass into the model\\\\\\_validate function as seen below. context = info.context if context: context = context.get('text\\\\\\_chunk') if v not in context: raise ValueError(f\"Citation \\\\\\`{v}\\\\\\` not found in text chunks\") return v We can then take our original example and test it against our new model try: AnswerWithCitation.model\\\\\\_validate( {\"answer\": \"Jason is a cool guy\", \"citation\": \"Jason is cool\"}, context={\"text\\\\\\_chunk\": \"Jason is just a guy\"}, This context object is just a normal python dictionary and can take in and store any arbitrary values ) except ValidationError as e: print(e) This in turn generates the following error since Jason is cool does not exist in the text Jason is just a guy. 1 validation error for AnswerWithCitation citation Value error, Citation \\\\\\`Jason is cool\\\\\\` not found in text chunks \\\\\\[type=value\\\\\\_error, input\\\\\\_value='Jason is cool', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Putting it all together with client = instructor.patch(OpenAI()) To pass this context from the client.chat.completions.create call, client = instructor.patch(OpenAI()) also passes the validation\\\\\\_context, which will be accessible from the info argument in the decorated validator functions. from openai import OpenAI import instructor # Enables \\\\\\`response\\\\\\_model\\\\\\` and \\\\\\`max\\\\\\_retries\\\\\\` parameters client = instructor.patch(OpenAI()) def answer\\\\\\_question(question:str, text\\\\\\_chunk: str) -> AnswerWithCitation: return client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Answer the question: {question} with the text chunk: {text\\\\\\_chunk}\", }, \\\\\\], response\\\\\\_model=AnswerWithCitation, validation\\\\\\_context={\"text\\\\\\_chunk\": text\\\\\\_chunk}, ) Error Handling and Re-Asking Validators can ensure certain properties of the outputs by throwing errors, in an AI system we can use the errors and allow language model to self correct. The by running client = instructor.patch(OpenAI()) not only do we add response\\\\\\_model and validation\\\\\\_context it also allows you to use the max\\\\\\_retries parameter to specify the number of times to try and self correct. This approach provides a layer of defense against two types of bad outputs: Pydantic Validation Errors (code or LLM-based) JSON Decoding Errors (when the model returns an incorrect response) Define the Response Model with Validators To keep things simple lets assume we have a model that returns a UserModel object. We can define the response model using Pydantic and add a field validator to ensure that the name is in uppercase. from pydantic import BaseModel, field\\\\\\_validator class UserModel(BaseModel): name: str age: int @field\\\\\\_validator(\"name\") @classmethod def validate\\\\\\_name(cls, v): if v.upper() != v: raise ValueError(\"Name must be in uppercase.\") return v This is where the max\\\\\\_retries parameter comes in. It allows the model to self correct and retry the prompt using the error message rather than the prompt. model = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"}, \\\\\\], # Powered by client = instructor.patch(OpenAI()) response\\\\\\_model=UserModel, max\\\\\\_retries=2, ) assert model.name == \"JASON\" In this example, even though there is no code explicitly transforming the name to uppercase, the model is able to correct the output. Conclusion From the simplicity of Pydantic and Instructor to the dynamic validation capabilities of LLMs, the landscape of validation is changing but without needing to introduce new contepts. It's clear that the future of validation is not just about preventing bad data but about allowing llms to understand the data and correcting it. If you enjoy the content or want to try out Instructor please check out the github and give us a star! Continue reading 2023/10/17 4 min read Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation Introduction Get ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the instructor.instructions streamlines this process, making the task you want to distil more efficient and powerful while preserving its original functionality and backwards compatibility. If you want to see the full example checkout examples/distillation Why use Instructor? Imagine you're developing a backend service that uses a mix old and new school ML practises, it may involve pipelines with multiple function calls, validations, and data processing. Sounds cumbersome, right? That's where Instructor comes in. It simplifies complex procedures, making them more efficient and easier to manage by adding a decorator to your function that will automatically generate a dataset for fine-tuning and help you swap out the function implementation. Quick Start: How to Use Instructor's Distillation Feature Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file. import logging import random from pydantic import BaseModel from instructor import Instructions # pip install instructor # Logging setup logging.basicConfig(level=logging.INFO) instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"math\\\\\\_finetunes.jsonl\")\\\\\\] ) class Multiply(BaseModel): a: int b: int result: int # Define a function with distillation # The decorator will automatically generate a dataset for fine-tuning # They must return a pydantic model to leverage function calling @instructions.distil def fn(a: int, b: int) -> Multiply: resp = a \\\\\\* b return Multiply(a=a, b=b, result=resp) # Generate some data for \\\\\\_ in range(10): a = random.randint(100, 999) b = random.randint(100, 999) print(fn(a, b)) The Intricacies of Fine-tuning Language Models Fine-tuning isn't just about writing a function like def f(a, b): return a \\\\\\* b. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this. Why Instructor and Distillation are Game Changers The library offers two main benefits: Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code. Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions. Role of Instructor in Simplifying Fine-Tuning The from instructor import Instructions feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior. Logging Output and Running a Finetune Here's how the logging output would look: { \"messages\": \\\\\\[ {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'}, {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'}, {\"role\": \"assistant\", \"function\\\\\\_call\": { \"name\": \"Multiply\", \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}' } } \\\\\\], \"functions\": \\\\\\[ {\"name\": \"Multiply\", \"description\": \"Correctly extracted \\\\\\`Multiply\\\\\\`...\"} \\\\\\] } Run a finetune like this: Don't forget to set your OpenAI Key as an environment variable All of the instructor jobs commands assume you've set an environment variable of OPENAI\\\\\\_API\\\\\\_KEY in your shell. You can set this by running the command export OPENAI\\\\\\_API\\\\\\_KEY= in your shell instructor jobs create-from-file math\\\\\\_finetunes.jsonl Next Steps and Future Plans Here's a sneak peek of what I'm planning: from instructor import Instructions, patch patch() Don't forget to run the patch() command that we provide with the Instructor package. This helps automatically serialize the content back into the \\\\\\`Pydantic\\\\\\`\\\\\\` model that we're looking for. class Multiply(BaseModel): a: int b: int result: int instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", ) @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard def fn(a: int, b: int) -> Multiply: resp = a + b return Multiply(a=a, b=b, result=resp) With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation. Conclusion We've seen how Instructor can make your life easier, from fine-tuning to distillation. Now if you're thinking wow, I'd love a backend service to do this for continously, you're in luck! Please check out the survey at useinstructor.com and let us know who you are. If you enjoy the content or want to try out instructor please check out the github and give us a star! Continue reading 2023/09/17 7 min read RAG is more than just embedding search With the advent of large language models (LLM), retrival augmented generation (RAG) has become a hot topic. However throught the past year of helping startups integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware. What is RAG? Retrival augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized. Simple RAG that embedded the user query and makes a search. So let's kick things off by examining what I like to call the 'Dumb' RAG Model—a basic setup that's more common than you'd think. The 'Dumb' RAG Model When you ask a question like, \"what is the capital of France?\" The RAG 'dumb' model embeds the query and searches in some unopinonated search endpoint. Limited to a single method API like search(query: str) -> List\\\\\\[str\\\\\\]. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings. Why is this a problem? Query-Document Mismatch: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation! Monolithic Search Backend: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more. Limitation of text search: Restricts complex queries to a single string ({query: str}), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, asking what problems did we fix last week cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week. Limited ability to plan: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context its able to plan a suite of queries to execute to return the best results. Now let's dive into how we can make it smarter with query understanding. This is where things get interesting. Improving the RAG Model with Query Understanding Shoutouts Much of this work has been inspired by / done in collab with a few of my clients at new.computer, Metaphor Systems, and Naro, go check them out! Ultimately what you want to deploy is a system that understands how to take the query and rewrite it to improve precision and recall. Query Understanding system routes to multiple search backends. Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems. Whats instructor? Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling API. Widespread Adoption: Pydantic is a popular tool among Python developers. Simplicity: Pydantic allows model definition in Python. Framework Compatibility: Many Python frameworks already use Pydantic. Case Study 1: Metaphor Systems Take Metaphor Systems, which turns natural language queries into their custom search-optimized query. If you take a look web UI you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using a language model, and turns it into a fully specified metaphor systems query. Metaphor Systems UI If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. It's actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library class DateRange(BaseModel): start: datetime.date end: datetime.date class MetaphorQuery(BaseModel): rewritten\\\\\\_query: str published\\\\\\_daterange: DateRange domains\\\\\\_allow\\\\\\_list: List\\\\\\[str\\\\\\] async def execute(): return await metaphor.search(...) Note how we model a rewritten query, range of published dates, and a list of domains to search in. This powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works. import instructor from openai import OpenAI # Enables response\\\\\\_model in the openai client client = instructor.patch(OpenAI()) query = client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=MetaphorQuery, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\" }, { \"role\": \"user\", \"content\": \"What are some recent developments in AI?\" } \\\\\\], ) Example Output { \"rewritten\\\\\\_query\": \"novel developments advancements ai artificial intelligence machine learning\", \"published\\\\\\_daterange\": { \"start\": \"2023-09-17\", \"end\": \"2021-06-17\" }, \"domains\\\\\\_allow\\\\\\_list\": \\\\\\[\"arxiv.org\"\\\\\\] } This isn't just about adding some date ranges. It's about nuanced, tailored searches, that are deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features. class DateRange(BaseModel): start: datetime.date end: datetime.date chain\\\\\\_of\\\\\\_thought: str = Field( None, description=\"Think step by step to plan what is the best time range to search in\" ) Now, let's see how this approach can help model an agent like personal assistant. Case Study 2: Personal Assistant Another great example of this multiple dispatch pattern is a personal assistant. You might ask, \"What do I have today?\", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts. class ClientSource(enum.Enum): GMAIL = \"gmail\" CALENDAR = \"calendar\" class SearchClient(BaseModel): query: str keywords: List\\\\\\[str\\\\\\] email: str source: ClientSource start\\\\\\_date: datetime.date end\\\\\\_date: datetime.date async def execute(self) -> str: if self.source == ClientSource.GMAIL: ... elif self.source == ClientSource.CALENDAR: ... class Retrival(BaseModel): queries: List\\\\\\[SearchClient\\\\\\] async def execute(self) -> str: return await asyncio.gather(\\\\\\*\\\\\\[query.execute() for query in self.queries\\\\\\]) Now we can call this with a simple query like \"What do I have today?\" and it will try to async dispatch to the correct backend. It's still important to prompt the language model well, but we'll leave that for another day. import instructor from openai import OpenAI # Enables response\\\\\\_model in the openai client client = instructor.patch(OpenAI()) retrival = client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=Retrival, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"}, {\"role\": \"user\", \"content\": \"What do I have today?\"} \\\\\\], ) Example Output { \"queries\": \\\\\\[ { \"query\": None, \"keywords\": None, \"email\": \"jason@example.com\", \"source\": \"gmail\", \"start\\\\\\_date\": \"2023-09-17\", \"end\\\\\\_date\": None }, { \"query\": None, \"keywords\": \\\\\\[\"meeting\", \"call\", \"zoom\"\\\\\\]\\\\\\]\\\\\\], \"email\": \"jason@example.com\", \"source\": \"calendar\", \"start\\\\\\_date\": \"2023-09-17\", \"end\\\\\\_date\": None } \\\\\\] } Notice that we have a list of queries that route to different search backends (email and calendar). We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well. Perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app. Can I used framework X? I get this question a lot, but it's just code. Within these dispatchs you can do whatever you want. You can use input() to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information. The sky is the limit. Both of these examples showcase how both search providors and consumers can use instructor to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend. Conclusion This isnt about fancy embedding tricks, it's just plain old information retrival and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place. What's Next? Here I want to show that \\\\\\`instructor\\\\\\`\\\\\\` isn’t just about data extraction. It’s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning — the untapped goldmine is skilled use of tools and APIs. If you enjoy the content or want to try out instructor please check out the github and give us a star! Continue reading 1 2 Back to top Previous Core Library Next 2023 Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "RAG is more than just embedding search - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/",
    "html": "Skip to content Instructor RAG is more than just embedding search Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents The 'Dumb' RAG Model Why is this a problem? Improving the RAG Model with Query Understanding Whats instructor? Case Study 1: Metaphor Systems Case Study 2: Personal Assistant Conclusion What's Next? Back to index Jason Liu Creator Metadata 2023/09/17 7 min read RAG is more than just embedding search¶ With the advent of large language models (LLM), retrival augmented generation (RAG) has become a hot topic. However throught the past year of helping startups integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware. What is RAG? Retrival augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized. Simple RAG that embedded the user query and makes a search. So let's kick things off by examining what I like to call the 'Dumb' RAG Model—a basic setup that's more common than you'd think. The 'Dumb' RAG Model¶ When you ask a question like, \"what is the capital of France?\" The RAG 'dumb' model embeds the query and searches in some unopinonated search endpoint. Limited to a single method API like search(query: str) -> List\\\\\\[str\\\\\\]. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings. Why is this a problem?¶ Query-Document Mismatch: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation! Monolithic Search Backend: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more. Limitation of text search: Restricts complex queries to a single string ({query: str}), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, asking what problems did we fix last week cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week. Limited ability to plan: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context its able to plan a suite of queries to execute to return the best results. Now let's dive into how we can make it smarter with query understanding. This is where things get interesting. Improving the RAG Model with Query Understanding¶ Shoutouts Much of this work has been inspired by / done in collab with a few of my clients at new.computer, Metaphor Systems, and Naro, go check them out! Ultimately what you want to deploy is a system that understands how to take the query and rewrite it to improve precision and recall. Query Understanding system routes to multiple search backends. Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems. Whats instructor?¶ Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling API. Widespread Adoption: Pydantic is a popular tool among Python developers. Simplicity: Pydantic allows model definition in Python. Framework Compatibility: Many Python frameworks already use Pydantic. Case Study 1: Metaphor Systems¶ Take Metaphor Systems, which turns natural language queries into their custom search-optimized query. If you take a look web UI you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using a language model, and turns it into a fully specified metaphor systems query. Metaphor Systems UI If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. It's actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library class DateRange(BaseModel): start: datetime.date end: datetime.date class MetaphorQuery(BaseModel): rewritten\\\\\\_query: str published\\\\\\_daterange: DateRange domains\\\\\\_allow\\\\\\_list: List\\\\\\[str\\\\\\] async def execute(): return await metaphor.search(...) Note how we model a rewritten query, range of published dates, and a list of domains to search in. This powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works. import instructor from openai import OpenAI # Enables response\\\\\\_model in the openai client client = instructor.patch(OpenAI()) query = client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=MetaphorQuery, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\" }, { \"role\": \"user\", \"content\": \"What are some recent developments in AI?\" } \\\\\\], ) Example Output { \"rewritten\\\\\\_query\": \"novel developments advancements ai artificial intelligence machine learning\", \"published\\\\\\_daterange\": { \"start\": \"2023-09-17\", \"end\": \"2021-06-17\" }, \"domains\\\\\\_allow\\\\\\_list\": \\\\\\[\"arxiv.org\"\\\\\\] } This isn't just about adding some date ranges. It's about nuanced, tailored searches, that are deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features. class DateRange(BaseModel): start: datetime.date end: datetime.date chain\\\\\\_of\\\\\\_thought: str = Field( None, description=\"Think step by step to plan what is the best time range to search in\" ) Now, let's see how this approach can help model an agent like personal assistant. Case Study 2: Personal Assistant¶ Another great example of this multiple dispatch pattern is a personal assistant. You might ask, \"What do I have today?\", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts. class ClientSource(enum.Enum): GMAIL = \"gmail\" CALENDAR = \"calendar\" class SearchClient(BaseModel): query: str keywords: List\\\\\\[str\\\\\\] email: str source: ClientSource start\\\\\\_date: datetime.date end\\\\\\_date: datetime.date async def execute(self) -> str: if self.source == ClientSource.GMAIL: ... elif self.source == ClientSource.CALENDAR: ... class Retrival(BaseModel): queries: List\\\\\\[SearchClient\\\\\\] async def execute(self) -> str: return await asyncio.gather(\\\\\\*\\\\\\[query.execute() for query in self.queries\\\\\\]) Now we can call this with a simple query like \"What do I have today?\" and it will try to async dispatch to the correct backend. It's still important to prompt the language model well, but we'll leave that for another day. import instructor from openai import OpenAI # Enables response\\\\\\_model in the openai client client = instructor.patch(OpenAI()) retrival = client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=Retrival, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"}, {\"role\": \"user\", \"content\": \"What do I have today?\"} \\\\\\], ) Example Output { \"queries\": \\\\\\[ { \"query\": None, \"keywords\": None, \"email\": \"jason@example.com\", \"source\": \"gmail\", \"start\\\\\\_date\": \"2023-09-17\", \"end\\\\\\_date\": None }, { \"query\": None, \"keywords\": \\\\\\[\"meeting\", \"call\", \"zoom\"\\\\\\]\\\\\\]\\\\\\], \"email\": \"jason@example.com\", \"source\": \"calendar\", \"start\\\\\\_date\": \"2023-09-17\", \"end\\\\\\_date\": None } \\\\\\] } Notice that we have a list of queries that route to different search backends (email and calendar). We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well. Perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app. Can I used framework X? I get this question a lot, but it's just code. Within these dispatchs you can do whatever you want. You can use input() to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information. The sky is the limit. Both of these examples showcase how both search providors and consumers can use instructor to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend. Conclusion¶ This isnt about fancy embedding tricks, it's just plain old information retrival and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place. What's Next?¶ Here I want to show that \\\\\\`instructor\\\\\\`\\\\\\` isn’t just about data extraction. It’s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning — the untapped goldmine is skilled use of tools and APIs. If you enjoy the content or want to try out instructor please check out the github and give us a star! Was this page helpful? Back to top Previous Bridging Language Models with Python using Instructor, Pydantic, and OpenAI's Function Calls Next Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Core Library - Instructor",
    "url": "https://jxnl.github.io/instructor/api/?q=",
    "html": "Skip to content Instructor Core Library Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog API Reference Core Library Table of contents patch apatch() dump\\\\\\_message() is\\\\\\_async() patch() process\\\\\\_response() process\\\\\\_response\\\\\\_async() validators Validator llm\\\\\\_validator() Usage openai\\\\\\_moderation() citation CitationMixin Usage Result validate\\\\\\_sources() multitask MultiTask() Usage Result maybe MaybeBase Maybe() Usage Result function\\\\\\_calls Mode OpenAISchema Usage Result openai\\\\\\_schema from\\\\\\_response() from\\\\\\_response\\\\\\_async() API Reference¶ apatch(client, mode=Mode.FUNCTIONS) ¶ No longer necessary, use patch instead. Patch the client.chat.completions.create method Enables the following features: response\\\\\\_model parameter to parse the response from OpenAI's API max\\\\\\_retries parameter to retry the function if the response is not valid validation\\\\\\_context parameter to validate the response using the pydantic model strict parameter to use strict json parsing Source code in instructor/patch.py dump\\\\\\_message(message) ¶ Dumps a message to a dict, to be returned to the OpenAI API. Workaround for an issue with the OpenAI API, where the tool\\\\\\_calls field isn't allowed to be present in requests if it isn't used. Source code in instructor/patch.py is\\\\\\_async(func) ¶ Returns true if the callable is async, accounting for wrapped callables Source code in instructor/patch.py patch(client, mode=Mode.FUNCTIONS) ¶ Patch the client.chat.completions.create method Enables the following features: response\\\\\\_model parameter to parse the response from OpenAI's API max\\\\\\_retries parameter to retry the function if the response is not valid validation\\\\\\_context parameter to validate the response using the pydantic model strict parameter to use strict json parsing Source code in instructor/patch.py process\\\\\\_response(response, \\\\\\*, response\\\\\\_model, stream, validation\\\\\\_context=None, strict=None, mode=Mode.FUNCTIONS) ¶ Processes a OpenAI response with the response model, if available. It can use validation\\\\\\_context and strict to validate the response via the pydantic model Parameters: Name Type Description Default response ChatCompletion The response from OpenAI's API required response\\\\\\_model BaseModel The response model to use for parsing the response required stream bool Whether the response is a stream required validation\\\\\\_context dict The validation context to use for validating the response. Defaults to None. None strict bool Whether to use strict json parsing. Defaults to None. None Source code in instructor/patch.py process\\\\\\_response\\\\\\_async(response, \\\\\\*, response\\\\\\_model, stream, validation\\\\\\_context=None, strict=None, mode=Mode.FUNCTIONS) async ¶ Processes a OpenAI response with the response model, if available. It can use validation\\\\\\_context and strict to validate the response via the pydantic model Parameters: Name Type Description Default response ChatCompletion The response from OpenAI's API required response\\\\\\_model BaseModel The response model to use for parsing the response required stream bool Whether the response is a stream required validation\\\\\\_context dict The validation context to use for validating the response. Defaults to None. None strict bool Whether to use strict json parsing. Defaults to None. None Source code in instructor/patch.py Validator ¶ Bases: OpenAISchema Validate if an attribute is correct and if not, return a new value with an error message Source code in instructor/dsl/validators.py llm\\\\\\_validator(statement, allow\\\\\\_override=False, model='gpt-3.5-turbo', temperature=0, openai\\\\\\_client=None) ¶ Create a validator that uses the LLM to validate an attribute Usage¶ from instructor import llm\\\\\\_validator from pydantic import BaseModel, Field, field\\\\\\_validator class User(BaseModel): name: str = Annotated\\\\\\[str, llm\\\\\\_validator(\"The name must be a full name all lowercase\") age: int = Field(description=\"The age of the person\") try: user = User(name=\"Jason Liu\", age=20) except ValidationError as e: print(e) 1 validation error for User name The name is valid but not all lowercase (type=value\\\\\\_error.llm\\\\\\_validator) Note that there, the error message is written by the LLM, and the error type is value\\\\\\_error.llm\\\\\\_validator. Parameters: Name Type Description Default statement str The statement to validate required model str The LLM to use for validation (default: \"gpt-3.5-turbo-0613\") 'gpt-3.5-turbo' temperature float The temperature to use for the LLM (default: 0) 0 openai\\\\\\_client OpenAI The OpenAI client to use (default: None) None Source code in instructor/dsl/validators.py openai\\\\\\_moderation(client=None) ¶ Validates a message using OpenAI moderation model. Should only be used for monitoring inputs and outputs of OpenAI APIs Other use cases are disallowed as per: https://platform.openai.com/docs/guides/moderation/overview Example: from instructor import OpenAIModeration class Response(BaseModel): message: Annotated\\\\\\[str, AfterValidator(OpenAIModeration(openai\\\\\\_client=client))\\\\\\] Response(message=\"I hate you\") ValidationError: 1 validation error for Response message Value error, \\\\\\`I hate you.\\\\\\` was flagged for \\\\\\['harassment'\\\\\\] \\\\\\[type=value\\\\\\_error, input\\\\\\_value='I hate you.', input\\\\\\_type=str\\\\\\] client (OpenAI): The OpenAI client to use, must be sync (default: None) Source code in instructor/dsl/validators.py CitationMixin ¶ Bases: BaseModel Helpful mixing that can use validation\\\\\\_context={\"context\": context} in from\\\\\\_response to find the span of the substring\\\\\\_phrase in the context. Usage¶ from pydantic import BaseModel, Field from instructor import CitationMixin class User(BaseModel): name: str = Field(description=\"The name of the person\") age: int = Field(description=\"The age of the person\") role: str = Field(description=\"The role of the person\") context = \"Betty was a student. Jason was a student. Jason is 20 years old\" user = openai.ChatCompletion.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"user\", \"content\": \"Extract jason from {context}\", }, response\\\\\\_model=User, validation\\\\\\_context={\"context\": context}, \\\\\\] ) for quote in user.substring\\\\\\_quotes: assert quote in context print(user.model\\\\\\_dump()) Result¶ { \"name\": \"Jason Liu\", \"age\": 20, \"role\": \"student\", \"substring\\\\\\_quotes\": \\\\\\[ \"Jason was a student\", \"Jason is 20 years old\", \\\\\\] } Source code in instructor/dsl/citation.py validate\\\\\\_sources(info) ¶ For each substring\\\\\\_phrase, find the span of the substring\\\\\\_phrase in the context. If the span is not found, remove the substring\\\\\\_phrase from the list. Source code in instructor/dsl/citation.py MultiTask(subtask\\\\\\_class, name=None, description=None) ¶ Dynamically create a MultiTask OpenAISchema that can be used to segment multiple tasks given a base class. This creates class that can be used to create a toolkit for a specific task, names and descriptions are automatically generated. However they can be overridden. Usage¶ from pydantic import BaseModel, Field from instructor import MultiTask class User(BaseModel): name: str = Field(description=\"The name of the person\") age: int = Field(description=\"The age of the person\") role: str = Field(description=\"The role of the person\") MultiUser = MultiTask(User) Result¶ class MultiUser(OpenAISchema, MultiTaskBase): tasks: List\\\\\\[User\\\\\\] = Field( default\\\\\\_factory=list, repr=False, description=\"Correctly segmented list of \\\\\\`User\\\\\\` tasks\", ) @classmethod def from\\\\\\_streaming\\\\\\_response(cls, completion) -> Generator\\\\\\[User\\\\\\]: ''' Parse the streaming response from OpenAI and yield a \\\\\\`User\\\\\\` object for each task in the response ''' json\\\\\\_chunks = cls.extract\\\\\\_json(completion) yield from cls.tasks\\\\\\_from\\\\\\_chunks(json\\\\\\_chunks) Parameters: Name Type Description Default subtask\\\\\\_class Type\\\\\\[OpenAISchema\\\\\\] The base class to use for the MultiTask required name Optional\\\\\\[str\\\\\\] The name of the MultiTask class, if None then the name of the subtask class is used as Multi{subtask\\\\\\_class.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_} None description Optional\\\\\\[str\\\\\\] The description of the MultiTask class, if None then the description is set to Correct segmentation of{subtask\\\\\\_class.name}tasks None Returns: Name Type Description schema OpenAISchema A new class that can be used to segment multiple tasks Source code in instructor/dsl/multitask.py MaybeBase ¶ Bases: BaseModel Extract a result from a model, if any, otherwise set the error and message fields. Source code in instructor/dsl/maybe.py Maybe(model) ¶ Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for result, error, and message for sitatations where the data may not be present in the context. Usage¶ from pydantic import BaseModel, Field from instructor import Maybe class User(BaseModel): name: str = Field(description=\"The name of the person\") age: int = Field(description=\"The age of the person\") role: str = Field(description=\"The role of the person\") MaybeUser = Maybe(User) Result¶ class MaybeUser(BaseModel): result: Optional\\\\\\[User\\\\\\] error: bool = Field(default=False) message: Optional\\\\\\[str\\\\\\] def \\\\\\_\\\\\\_bool\\\\\\_\\\\\\_(self): return self.result is not None Parameters: Name Type Description Default model Type\\\\\\[BaseModel\\\\\\] The Pydantic model to wrap with Maybe. required Returns: Name Type Description MaybeModel Type\\\\\\[BaseModel\\\\\\] A new Pydantic model that includes fields for result, error, and message. Source code in instructor/dsl/maybe.py Mode ¶ Bases: Enum The mode to use for patching the client Source code in instructor/function\\\\\\_calls.py OpenAISchema ¶ Bases: BaseModel Augments a Pydantic model with OpenAI's schema for function calling This class augments a Pydantic model with OpenAI's schema for function calling. The schema is generated from the model's signature and docstring. The schema can be used to validate the response from OpenAI's API and extract the function call. Usage¶ from instructor import OpenAISchema class User(OpenAISchema): name: str age: int completion = openai.ChatCompletion.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[{ \"content\": \"Jason is 20 years old\", \"role\": \"user\" }\\\\\\], functions=\\\\\\[User.openai\\\\\\_schema\\\\\\], function\\\\\\_call={\"name\": User.openai\\\\\\_schema\\\\\\[\"name\"\\\\\\]}, ) user = User.from\\\\\\_response(completion) print(user.model\\\\\\_dump()) Result¶ { \"name\": \"Jason Liu\", \"age\": 20, } Source code in instructor/function\\\\\\_calls.py openai\\\\\\_schema classmethod property ¶ Return the schema in the format of OpenAI's schema as jsonschema Note Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt. Returns: Name Type Description model\\\\\\_json\\\\\\_schema dict A dictionary in the format of OpenAI's schema as jsonschema from\\\\\\_response(completion, validation\\\\\\_context=None, strict=None, mode=Mode.FUNCTIONS, stream\\\\\\_multitask=False) classmethod ¶ Execute the function from the response of an openai chat completion Parameters: Name Type Description Default completion ChatCompletion The response from an openai chat completion required throw\\\\\\_error bool Whether to throw an error if the function call is not detected required validation\\\\\\_context dict The validation context to use for validating the response None strict bool Whether to use strict json parsing None mode Mode The openai completion mode FUNCTIONS stream\\\\\\_multitask bool Whether to stream a multitask response False Returns: Name Type Description cls OpenAISchema An instance of the class Source code in instructor/function\\\\\\_calls.py from\\\\\\_response\\\\\\_async(completion, validation\\\\\\_context=None, strict=None, mode=Mode.FUNCTIONS, stream\\\\\\_multitask=False) async classmethod ¶ Execute the function from the response of an openai chat completion Parameters: Name Type Description Default completion ChatCompletion The response from an openai chat completion required validation\\\\\\_context dict The validation context to use for validating the response None strict bool Whether to use strict json parsing None mode Mode The openai completion mode FUNCTIONS stream\\\\\\_multitask bool Whether to stream a multitask response False Returns: Name Type Description cls OpenAISchema An instance of the class Source code in instructor/function\\\\\\_calls.py Was this page helpful? Back to top Previous Usage Tracking Next Welcome to the Instructor Blog Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Good LLM Validation is Just Good Validation - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/2023/10/23/good-llm-validation-is-just-good-validation/",
    "html": "Skip to content Instructor Good LLM Validation is Just Good Validation Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Table of contents What is Instructor? Software 1.0: Introduction to Validations in Pydantic Using field\\\\\\_validator decorator Using Annotated Software 3.0: Validation for LLMs or powered by LLMs Creating Your Own Field Level llm\\\\\\_validator Writing more complex validations Validating Chain of Thought Validating Citations From Original Text Putting it all together with client = instructor.patch(OpenAI()) Error Handling and Re-Asking Define the Response Model with Validators Conclusion Back to index Jason Liu Creator Ivan Leo Contributor Metadata 2023/10/23 10 min read Good LLM Validation is Just Good Validation¶ What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here. Validation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like Pydantic and Instructor. We validate these outputs using a validation function which conforms to the structure seen below. def validation\\\\\\_function(value): if condition(value): raise ValueError(\"Value is not valid\") return mutation(value) What is Instructor?¶ Instructor helps to ensure you get the exact response type you're looking for when using openai's function call api. Once you've defined the Pydantic model for your desired response, Instructor handles all the complicated logic in-between - from the parsing/validation of the response to the automatic retries for invalid responses. This means that we can build in validators 'for free' and have a clear separation of concerns between the prompt and the code that calls openai. from openai import OpenAI import instructor # pip install instructor from pydantic import BaseModel # This enables response\\\\\\_model keyword # from client.chat.completions.create client = instructor.patch(OpenAI()) To simplify your work with OpenAI models and streamline the extraction of Pydantic objects from prompts, we offer a patching mechanism for the ChatCompletion class. class UserDetail(BaseModel): name: str age: int user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] max\\\\\\_retries=3 Invalid responses that fail to be validated succesfully will trigger up to as many reattempts as you define. ) assert user.name == \"Jason\" As long as you pass in a response\\\\\\_model parameter to the ChatCompletion api call, the returned object will always be a validated Pydantic object. assert user.age == 25 In this post, we'll explore how to evolve from static, rule-based validation methods to dynamic, machine learning-driven ones. You'll learn to use Pydantic and Instructor to leverage language models and dive into advanced topics like content moderation, validating chain of thought reasoning, and contextual validation. Let's examine how these approaches with a example. Imagine that you run a software company who wants to ensure you never serve hateful and racist content. This isn't an easy job since the language around these topics change very quickly and frequently. Software 1.0: Introduction to Validations in Pydantic¶ A simple method could be to compile a list of different words that are often associated with hate speech. For simplicity, let's assume that we've found that the words Steal and Rob are good predictors of hateful speech from our database. We can modify our validation structure above to accomodate this. This will throw an error if we pass in a string like Let's rob the bank! or We should steal from the supermarkets. Pydantic offers two approaches for this validation: using the field\\\\\\_validator decorator or the Annotated hints. Using field\\\\\\_validator decorator¶ We can use the field\\\\\\_validator decorator to define a validator for a field in Pydantic. Here's a quick example of how we might be able to do so. from pydantic import BaseModel, ValidationError, field\\\\\\_validator from pydantic.fields import Field class UserMessage(BaseModel): message: str @field\\\\\\_validator('message') def message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words(cls, v: str) -> str: for word in v.split(): We split the sentence into its individual words and iterate through each of the words. We then try to see if any of these words are in our blacklist which in this case is just rob and steal if word.lower() in {'rob','steal'}: raise ValueError(f\"\\\\\\`{word}\\\\\\` was found in the message \\\\\\`{v}\\\\\\`\") return v try: UserMessage(message=\"This is a lovely day\") UserMessage(message=\"We should go and rob a bank\") except ValidationError as e: print(e) Since the message This is a lovely day does not have any blacklisted words, no errors are thrown. However, in the given example above, the validation fails for the message We should go and rob a bank due to the presence of the word rob and the corresponding error message is displayed. 1 validation error for UserMessage message Value error, \\\\\\`rob\\\\\\` was found in the message \\\\\\`We should go and rob a bank\\\\\\` \\\\\\[type=value\\\\\\_error, input\\\\\\_value='We should go and rob a bank', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Using Annotated¶ Alternatively, you can use the Annotated function to perform the same validation. Here's an example where we utilise the same function we started with. from pydantic import BaseModel, ValidationError from typing import Annotated from pydantic.functional\\\\\\_validators import AfterValidator def message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words(value:str): for word in value.split(): if word.lower() in {'rob','steal'}: raise ValueError(f\"\\\\\\`{word}\\\\\\` was found in the message \\\\\\`{value}\\\\\\`\") return value class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words)\\\\\\] try: UserMessage(message=\"This is a lovely day\") UserMessage(message=\"We should go and rob a bank\") except ValidationError as e: print(e) This code snippet achieves the same validation result. If the user message contains any of the words in the blacklist, a ValueError is raised and the corresponding error message is displayed. 1 validation error for UserMessage message Value error, \\\\\\`rob\\\\\\` was found in the message \\\\\\`We should go and rob a bank\\\\\\` \\\\\\[type=value\\\\\\_error, input\\\\\\_value='We should go and rob a bank', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Validation is a fundamental concept in software development and remains the same when applied to AI systems. Existing programming concepts should be leveraged when possible instead of introducing new terms and standards. The underlying principles of validation remain unchanged. Suppose now that we've gotten a new message - Violence is always acceptable, as long as we silence the witness. Our original validator wouldn't throw any errors when passed this new message since it uses neither the words rob or steal. However, it's clear that it is not a message which should be published. How can we ensure that our validation logic can adapt to new challenges? Software 3.0: Validation for LLMs or powered by LLMs¶ Building upon the understanding of simple field validators, let's delve into probabilistic validation in software 3.0, (prompt engineering). We'll introduce an LLM-powered validator called llm\\\\\\_validator that uses a statement to verify the value. We can get around this by using the inbuilt llm\\\\\\_validator class from Instructor. from instructor import llm\\\\\\_validator from pydantic import BaseModel, ValidationError from typing import Annotated from pydantic.functional\\\\\\_validators import AfterValidator class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(llm\\\\\\_validator(\"don't say objectionable things\"))\\\\\\] try: UserMessage(message=\"Violence is always acceptable, as long as we silence the witness\") except ValidationError as e: print(e) This produces the following error message as seen below 1 validation error for UserMessage message Assertion failed, The statement promotes violence, which is objectionable. \\\\\\[type=assertion\\\\\\_error, input\\\\\\_value='Violence is always accep... we silence the witness', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/assertion\\\\\\_error The error message is generated by the language model (LLM) rather than the code itself, making it helpful for re-asking the model in a later section. To better understand this approach, let's see how to build an llm\\\\\\_validator from scratch. Creating Your Own Field Level llm\\\\\\_validator¶ Building your own llm\\\\\\_validator can be a valuable exercise to get started with Instructor and create custom validators. Before we continue, let's review the anatomy of a validator: def validation\\\\\\_function(value): if condition(value): raise ValueError(\"Value is not valid\") return value As we can see, a validator is simply a function that takes in a value and returns a value. If the value is not valid, it raises a ValueError. We can represent this using the following structure: class Validation(BaseModel): is\\\\\\_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\") error\\\\\\_message: Optional\\\\\\[str\\\\\\] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\") Using this structure, we can implement the same logic as before and utilize Instructor to generate the validation. import instructor from openai import OpenAI # Enables \\\\\\`response\\\\\\_model\\\\\\` and \\\\\\`max\\\\\\_retries\\\\\\` parameters client = instructor.patch(OpenAI()) def validator(v): statement = \"don't say objectionable things\" resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Does \\\\\\`{v}\\\\\\` follow the rules: {statement}\", }, \\\\\\], # this comes from client = instructor.patch(OpenAI()) response\\\\\\_model=Validation, The new parameter of response\\\\\\_model comes from client = instructor.patch(OpenAI()) and does not exist in the original OpenAI SDK. This allows us to pass in the Pydantic model that we want as a response. ) if not resp.is\\\\\\_valid: raise ValueError(resp.error\\\\\\_message) return v Now we can use this validator in the same way we used the llm\\\\\\_validator from Instructor. class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(validator)\\\\\\] Writing more complex validations¶ Validating Chain of Thought¶ A popular way of prompting large language models nowadays is known as chain of thought. This involves getting a model to generate reasons and explanations for an answer to a prompt. We can utilise Pydantic and Instructor to perform a validation to check of the reasoning is reasonable, given both the answer and the chain of thought. To do this we can't build a field validator since we need to access multiple fields in the model. Instead we can use a model validator. def validate\\\\\\_chain\\\\\\_of\\\\\\_thought(values): chain\\\\\\_of\\\\\\_thought = values\\\\\\[\"chain\\\\\\_of\\\\\\_thought\"\\\\\\] answer = values\\\\\\[\"answer\"\\\\\\] resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Verify that \\\\\\`{answer}\\\\\\` follows the chain of thought: {chain\\\\\\_of\\\\\\_thought}\", }, \\\\\\], # this comes from client = instructor.patch(OpenAI()) response\\\\\\_model=Validation, ) if not resp.is\\\\\\_valid: raise ValueError(resp.error\\\\\\_message) return values We can then take advantage of the model\\\\\\_validator decorator to perform a validation on a subset of the model's data. We're defining a model validator here which runs before Pydantic parses the input into its respective fields. That's why we have a before keyword used in the model\\\\\\_validator class. from pydantic import BaseModel, model\\\\\\_validator class AIResponse(BaseModel): chain\\\\\\_of\\\\\\_thought: str answer: str @model\\\\\\_validator(mode='before') @classmethod def chain\\\\\\_of\\\\\\_thought\\\\\\_makes\\\\\\_sense(cls, data: Any) -> Any: # here we assume data is the dict representation of the model # since we use 'before' mode. return validate\\\\\\_chain\\\\\\_of\\\\\\_thought(data) Now, when you create a AIResponse instance, the chain\\\\\\_of\\\\\\_thought\\\\\\_makes\\\\\\_sense validator will be invoked. Here's an example: try: resp = AIResponse( chain\\\\\\_of\\\\\\_thought=\"1 + 1 = 2\", answer=\"The meaning of life is 42\" ) except ValidationError as e: print(e) If we create a AIResponse instance with an answer that does not follow the chain of thought, we will get an error. 1 validation error for AIResponse Value error, The statement 'The meaning of life is 42' does not follow the chain of thought: 1 + 1 = 2. \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'chain\\\\\\_of\\\\\\_thought': '1 +... meaning of life is 42'}, input\\\\\\_type=dict\\\\\\] Validating Citations From Original Text¶ Let's see a more concrete example. Let's say that we've asked our model a question about some text source and we want to validate that the generated answer is supported by the source. This would allow us to minimize hallucinations and prevent statements that are not backed by the original text. While we could verify this by looking up the original source manually, a more scalable approach is to use a validator to do this automatically. We can pass in additional context to our validation functions using the model\\\\\\_validate function in Pydantic so that our models have more information to work with when performing validation. This context is a normal python dictionary and can be accessed inside the info argument in our validator functions. from pydantic import ValidationInfo,BaseModel,field\\\\\\_validator class AnswerWithCitation(BaseModel): answer: str citation: str @field\\\\\\_validator('citation') @classmethod def citation\\\\\\_exists(cls, v: str, info: ValidationInfo): This info object corresponds to the value of context that we pass into the model\\\\\\_validate function as seen below. context = info.context if context: context = context.get('text\\\\\\_chunk') if v not in context: raise ValueError(f\"Citation \\\\\\`{v}\\\\\\` not found in text chunks\") return v We can then take our original example and test it against our new model try: AnswerWithCitation.model\\\\\\_validate( {\"answer\": \"Jason is a cool guy\", \"citation\": \"Jason is cool\"}, context={\"text\\\\\\_chunk\": \"Jason is just a guy\"}, This context object is just a normal python dictionary and can take in and store any arbitrary values ) except ValidationError as e: print(e) This in turn generates the following error since Jason is cool does not exist in the text Jason is just a guy. 1 validation error for AnswerWithCitation citation Value error, Citation \\\\\\`Jason is cool\\\\\\` not found in text chunks \\\\\\[type=value\\\\\\_error, input\\\\\\_value='Jason is cool', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Putting it all together with client = instructor.patch(OpenAI())¶ To pass this context from the client.chat.completions.create call, client = instructor.patch(OpenAI()) also passes the validation\\\\\\_context, which will be accessible from the info argument in the decorated validator functions. from openai import OpenAI import instructor # Enables \\\\\\`response\\\\\\_model\\\\\\` and \\\\\\`max\\\\\\_retries\\\\\\` parameters client = instructor.patch(OpenAI()) def answer\\\\\\_question(question:str, text\\\\\\_chunk: str) -> AnswerWithCitation: return client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Answer the question: {question} with the text chunk: {text\\\\\\_chunk}\", }, \\\\\\], response\\\\\\_model=AnswerWithCitation, validation\\\\\\_context={\"text\\\\\\_chunk\": text\\\\\\_chunk}, ) Error Handling and Re-Asking¶ Validators can ensure certain properties of the outputs by throwing errors, in an AI system we can use the errors and allow language model to self correct. The by running client = instructor.patch(OpenAI()) not only do we add response\\\\\\_model and validation\\\\\\_context it also allows you to use the max\\\\\\_retries parameter to specify the number of times to try and self correct. This approach provides a layer of defense against two types of bad outputs: Pydantic Validation Errors (code or LLM-based) JSON Decoding Errors (when the model returns an incorrect response) Define the Response Model with Validators¶ To keep things simple lets assume we have a model that returns a UserModel object. We can define the response model using Pydantic and add a field validator to ensure that the name is in uppercase. from pydantic import BaseModel, field\\\\\\_validator class UserModel(BaseModel): name: str age: int @field\\\\\\_validator(\"name\") @classmethod def validate\\\\\\_name(cls, v): if v.upper() != v: raise ValueError(\"Name must be in uppercase.\") return v This is where the max\\\\\\_retries parameter comes in. It allows the model to self correct and retry the prompt using the error message rather than the prompt. model = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"}, \\\\\\], # Powered by client = instructor.patch(OpenAI()) response\\\\\\_model=UserModel, max\\\\\\_retries=2, ) assert model.name == \"JASON\" In this example, even though there is no code explicitly transforming the name to uppercase, the model is able to correct the output. Conclusion¶ From the simplicity of Pydantic and Instructor to the dynamic validation capabilities of LLMs, the landscape of validation is changing but without needing to introduce new contepts. It's clear that the future of validation is not just about preventing bad data but about allowing llms to understand the data and correcting it. If you enjoy the content or want to try out Instructor please check out the github and give us a star! Was this page helpful? Back to top Previous Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation Next AI Engineer Keynote: Pydantic is all you need Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Why use Instructor? - Instructor",
    "url": "https://jxnl.github.io/instructor/why/?q=",
    "html": "Skip to content Instructor Why use Instructor? Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Introduction Why use Instructor? Help with Instructor Installation Contributing Table of contents Understanding the patch Step 1: Patch the client Step 2: Define the Pydantic Model Step 3: Extract Understanding Validation Self Correcting on Validation Error Why use Instructor?¶ Why use Pydantic? Our instructor.patch for the OpenAI class introduces three key enhancements: Response Mode: Specify a Pydantic model to streamline data extraction. Max Retries: Set your desired number of retry attempts for requests. Validation Context: Provide a context object for enhanced validator access. A Glimpse into Instructor's Capabilities Using Validators Learn more about validators checkout our blog post Good llm validation is just good validation With Instructor, your code becomes more efficient and readable. Here’s a quick peek: Understanding the patch¶ Lets go over the patch function. And see how we can leverage it to make use of instructor Step 1: Patch the client¶ First, import the required libraries and apply the patch function to the OpenAI module. This exposes new functionality with the response\\\\\\_model parameter. import instructor from openai import OpenAI from pydantic import BaseModel # This enables response\\\\\\_model keyword # from client.chat.completions.create client = instructor.patch(OpenAI()) Step 2: Define the Pydantic Model¶ Create a Pydantic model to define the structure of the data you want to extract. This model will map directly to the information in the prompt. from pydantic import BaseModel class UserDetail(BaseModel): name: str age: int Step 3: Extract¶ Use the client.chat.completions.create method to send a prompt and extract the data into the Pydantic object. The response\\\\\\_model parameter specifies the Pydantic model to use for extraction. Its helpful to annotate the variable with the type of the response model. which will help your IDE provide autocomplete and spell check. user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] ) assert user.name == \"Jason\" assert user.age == 25 Understanding Validation¶ Validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error. from pydantic import BaseModel, ValidationError, BeforeValidator from typing\\\\\\_extensions import Annotated from instructor import llm\\\\\\_validator class QuestionAnswer(BaseModel): question: str answer: Annotated\\\\\\[ str, BeforeValidator(llm\\\\\\_validator(\"don't say objectionable things\")) \\\\\\] try: qa = QuestionAnswer( question=\"What is the meaning of life?\", answer=\"The meaning of life is to be evil and steal\", ) except ValidationError as e: print(e) Its important to not here that the error message is generated by the LLM, not the code, so it'll be helpful for re asking the model. 1 validation error for QuestionAnswer answer Assertion failed, The statement is objectionable. (type=assertion\\\\\\_error) Self Correcting on Validation Error¶ Here, the UserDetails model is passed as the response\\\\\\_model, and max\\\\\\_retries is set to 2. import instructor from openai import OpenAI from pydantic import BaseModel, field\\\\\\_validator # Apply the patch to the OpenAI client client = instructor.patch(OpenAI()) class UserDetails(BaseModel): name: str age: int @field\\\\\\_validator(\"name\") @classmethod def validate\\\\\\_name(cls, v): if v.upper() != v: raise ValueError(\"Name must be in uppercase.\") return v model = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetails, max\\\\\\_retries=2, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"}, \\\\\\], ) assert model.name == \"JASON\" As you can see, we've baked in a self correcting mechanism into the model. This is a powerful way to make your models more robust and less brittle without include a lot of extra code or prompt. Was this page helpful? Back to top Previous Welcome To Instructor Next Help with Instructor Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "2023 - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/archive/2023/",
    "html": "Skip to content Instructor 2023 Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Blog Archive 2023 Table of contents Structured Outputs with Anyscale Introduction to Caching in Python Generators and LLM Streaming Verifying LLM Citations with Pydantic Introduction to Batch Processing using asyncio and Instructor Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density AI Engineer Keynote: Pydantic is all you need Good LLM Validation is Just Good Validation Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation RAG is more than just embedding search 2023¶ 2023/12/15 2 min read Structured Outputs with Anyscale Open-source LLMS are gaining popularity, and the release of Anyscale's Mistral model has made it possible to obtain structured outputs using JSON schema at any scale. Instead of relying on a model's default output mode, you can utilize JSON schema to obtain structured outputs. This approach is a time-saving alternative to extensive prompt engineering. By the end of this blog post, you will learn how to effectively utilize the instructor at any scale. But before we proceed, let's first explore the concept of patching. Patching Instructor's patch enhances a openai api it with the following features: response\\\\\\_model in create calls that returns a pydantic model max\\\\\\_retries in create calls that retries the call if it fails by using a backoff strategy Learn More To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page. Anyscale The good news is that Anyscale employs the same OpenAI client, and its models support some of these output modes too! Getting access If you want to try this out for yourself check out the Anyscale website. You can get started here. Let's explore one of the models available in Anyscale's extensive collection! from openai import OpenAI from pydantic import BaseModel import instructor class UserDetails(BaseModel): name: str age: int # enables \\\\\\`response\\\\\\_model\\\\\\` in create call client = instructor.patch( OpenAI( base\\\\\\_url=\"https://api.endpoints.anyscale.com/v1\", api\\\\\\_key=\"\" ), # This uses Anyscale's json schema output mode mode=instructor.Mode.JSON\\\\\\_SCHEMA ) resp = client.chat.completions.create( model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class extractor\" }, { \"role\": \"user\", \"content\": 'Extract the following entities: \"Jason is 20\"' }, \\\\\\], response\\\\\\_model=UserDetails, ) print(resp) # >>> name='Jason' age=20 You can find more information about Anyscale's output mode support here. Continue reading 2023/11/26 7 min read Introduction to Caching in Python Instructor makes working with language models easy, but they are still computationally expensive. Today, we're diving into optimizing instructor code while maintaining the excellent DX offered by Pydantic models. We'll tackle the challenges of caching Pydantic models, typically incompatible with pickle, and explore solutions that use decorators like functools.cache. Then, we'll craft custom decorators with diskcache and redis to support persistent caching and distributed systems. Lets first consider our canonical example, using the OpenAI Python client to extract user details. import instructor from openai import OpenAI from pydantic import BaseModel # Enables \\\\\\`response\\\\\\_model\\\\\\` client = instructor.patch(OpenAI()) class UserDetail(BaseModel): name: str age: int def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Now imagine batch processing data, running tests or experiments, or simply calling extract multiple times over a workflow. We'll quickly run into performance issues, as the function may be called repeatedly, and the same data will be processed over and over again, costing us time and money. 1. functools.cache for Simple In-Memory Caching When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session or in an application where we don't need to persist the cache between sessions. import functools @functools.cache def extract(data): return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Changing the Model does not Invalidate the Cache Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result. Now we can call extract multiple times with the same argument, and the result will be cached in memory for faster access. import time start = time.perf\\\\\\_counter() # Using time.perf\\\\\\_counter() to measure the time taken to run the function is better than using time.time() because it's more accurate and less susceptible to system clock changes. model = extract(\"Extract jason is 25 years old\") print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") start = time.perf\\\\\\_counter() model = extract(\"Extract jason is 25 years old\") # The second time we call extract, the result is returned from the cache, and the function is not called. print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") >>> Time taken: 0.9267581660533324 >>> Time taken: 1.2080417945981026e-06 # The second call to extract is much faster because the result is returned from the cache! Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries. What is a decorator? 2. diskcache for Persistent, Large Data Caching Copy Caching Code When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data! import functools import inspect import instructor import diskcache from openai import OpenAI from pydantic import BaseModel client = instructor.patch(OpenAI()) cache = diskcache.Cache('./my\\\\\\_cache\\\\\\_directory') def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation # We use inspect.signature to get the function's return type annotation, which we use to validate the cached result. if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type We use Pydantic's model\\\\\\_validate\\\\\\_json to deserialize the cached result into a Pydantic model. return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence. 2. Redis Caching Decorator for Distributed Systems Copy Caching Code When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures. import redis import functools import inspect import json import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) cache = redis.Redis(\"localhost\") def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: # Assuming client.chat.completions.create returns a UserDetail instance return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types. Looking carefully If you look carefully at the code above you'll notice that we're using the same instructor\\\\\\_cache decorator as before. The implementatino is the same, but we're using a different caching backend! Conclusion Choosing the right caching strategy depends on your application's specific needs, such as the size and type of data, the need for persistence, and the system's architecture. Whether it's optimizing a function's performance in a small application or managing large datasets in a distributed environment, Python offers robust solutions to improve efficiency and reduce computational overhead. If you'd like to use this code, try to send it over to ChatGPT to understand it more, and to add additional features that might matter for you, for example, the cache isn't invalidated when your BaseModel changes, so you might want to encode the Model.model\\\\\\_json\\\\\\_schema() as part of the key. If you like the content check out our GitHub as give us a star and checkout the library. Continue reading 2023/11/26 6 min read Generators and LLM Streaming Latency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times. And what makes streaming possible? Generators! In this post, we're going to dive into the cool world of Python generators — these tools are more than just a coding syntax trick. We'll explore Python generators from the ground up and then delve into LLM streaming using the Instructor library. Python Generators: An Efficient Approach to Iterables Generators in Python are a game-changer for handling large data sets and stream processing. They allow functions to yield values one at a time, pausing and resuming their state, which is a faster and more memory-efficient approach compared to traditional collections that store all elements in memory. The Basics: Yielding Values A generator function in Python uses the yield keyword. It yields values one at a time, allowing the function to pause and resume its state. def count\\\\\\_to\\\\\\_3(): yield 1 yield 2 yield 3 for num in count\\\\\\_to\\\\\\_3(): print(num) 1 2 3 Advantages Over Traditional Collections Lazy Evaluation & reduced latency: The time to get the first element (or time-to-first-token in LLM land) from a generator is significantly lower. Generators only produce one value at a time, whereas accessing the first element of a collection will require that the whole collection be created first. Memory Efficiency: Only one item is in memory at a time. Maintain State: Automatically maintains state between executions. Let's see how much faster generators are and where they really shine: import time def expensive\\\\\\_func(x): \"\"\"Simulate an expensive operation.\"\"\" time.sleep(1) return x \\\\\\*\\\\\\* 2 def calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_list(func\\\\\\_input, func): \"\"\"Calculate using a list comprehension and return the first result with its computation time.\"\"\" start\\\\\\_perf = time.perf\\\\\\_counter() result = \\\\\\[func(x) for x in func\\\\\\_input\\\\\\]\\\\\\[0\\\\\\] end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (list): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") return result def calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_generator(func\\\\\\_input, func): \"\"\"Calculate using a generator and return the first result with its computation time.\"\"\" start\\\\\\_perf = time.perf\\\\\\_counter() result = next(func(x) for x in func\\\\\\_input) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (generator): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") return result # Prepare inputs for the function numbers = \\\\\\[1, 2, 3, 4, 5\\\\\\] # Benchmarking first\\\\\\_result\\\\\\_list = calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_list(numbers, expensive\\\\\\_func) first\\\\\\_result\\\\\\_gen = calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_generator(numbers, expensive\\\\\\_func) Time for first result (list): 5.02 seconds Time for first result (generator): 1.01 seconds The generator computes one expensive operation and returns the first result immediately, while the list comprehension computes the expensive operation for all elements in the list before returning the first result. Generator Expressions: A Shortcut Python also allows creating generators in a single line of code, known as generator expressions. They are syntactically similar to list comprehensions but use parentheses. squares = (x\\\\\\*x for x in range(10)) Use Cases in Real-World Applications Generators shine in scenarios like reading large files, data streaming (eg. llm token streaming), and pipeline creation for data processing. LLM Streaming If you've used ChatGPT, you'll see that the tokens are streamed out one by one, instead of the full response being shown at the end (can you imagine waiting for the full response??). This is made possible by generators. Here's how a vanilla openai generator looks: from openai import OpenAI # Set your OpenAI API key client = OpenAI( api\\\\\\_key=\"My API Key\", ) response\\\\\\_generator = client.chat.completions.create( model='gpt-3.5-turbo', messages=\\\\\\[ {'role': 'user', 'content': \"What are some good reasons to smile?\"} \\\\\\], temperature=0, stream=True ) for chunk in response\\\\\\_generator: print(chunk.choices\\\\\\[0\\\\\\].delta.content, end=\"\") This is great, but what if we want to do some structured extraction on this stream? For instance, we might want to render frontend components based on product rankings that are streamed out by an LLM. Should we wait for the entire stream to finish before extracting & validating the list of components or can we extract & validate the components in real time as they are streamed? In e-commerce, every millisecond matters so the time-to-first-render can differentiate a successful and not-so-successful e commerce store (and i know how a failing e commerce store feels :/ ). Let's see how we can use Instructor to handle extraction from this real time stream! E-commerce Product Ranking SCENARIO Imagine an e-commerce platform where we have: • a customer profile: this includes a detailed history of purchases, browsing behavior, product ratings, preferences in various categories, search history, and even responses to previous recommendations. This extensive data is crucial for generating highly personalized and relevant product suggestions. • a list of candidate products: these could be some shortlisted products we think the customer would like. Our goal is to re-rerank these candidate products for the best conversion and we'll use an LLM! STREAM PROCESSING User Data: Let's assume we have the following user profile: profile\\\\\\_data = \"\"\" Customer ID: 12345 Recent Purchases: \\\\\\[Laptop, Wireless Headphones, Smart Watch\\\\\\] Frequently Browsed Categories: \\\\\\[Electronics, Books, Fitness Equipment\\\\\\] Product Ratings: {Laptop: 5 stars, Wireless Headphones: 4 stars} Recent Search History: \\\\\\[best budget laptops 2023, latest sci-fi books, yoga mats\\\\\\] Preferred Brands: \\\\\\[Apple, AllBirds, Bench\\\\\\] Responses to Previous Recommendations: {Philips: Not Interested, Adidas: Not Interested} Loyalty Program Status: Gold Member Average Monthly Spend: $500 Preferred Shopping Times: Weekend Evenings ... \"\"\" We want to rank the following products for this user: products = \\\\\\[ {\"product\\\\\\_id\": 1, \"product\\\\\\_name\": \"Apple MacBook Air (2023) - Latest model, high performance, portable\"}, {\"product\\\\\\_id\": 2, \"product\\\\\\_name\": \"Sony WH-1000XM4 Wireless Headphones - Noise-canceling, long battery life\"}, {\"product\\\\\\_id\": 3, \"product\\\\\\_name\": \"Apple Watch Series 7 - Advanced fitness tracking, seamless integration with Apple ecosystem\"}, {\"product\\\\\\_id\": 4, \"product\\\\\\_name\": \"Kindle Oasis - Premium e-reader with adjustable warm light\"}, {\"product\\\\\\_id\": 5, \"product\\\\\\_name\": \"AllBirds Wool Runners - Comfortable, eco-friendly sneakers\"}, {\"product\\\\\\_id\": 6, \"product\\\\\\_name\": \"Manduka PRO Yoga Mat - High-quality, durable, eco-friendly\"}, {\"product\\\\\\_id\": 7, \"product\\\\\\_name\": \"Bench Hooded Jacket - Stylish, durable, suitable for outdoor activities\"}, {\"product\\\\\\_id\": 8, \"product\\\\\\_name\": \"GoPro HERO9 Black - 5K video, waterproof, for action photography\"}, {\"product\\\\\\_id\": 9, \"product\\\\\\_name\": \"Nespresso Vertuo Next Coffee Machine - Quality coffee, easy to use, compact design\"}, {\"product\\\\\\_id\": 10, \"product\\\\\\_name\": \"Project Hail Mary by Andy Weir - Latest sci-fi book from a renowned author\"} \\\\\\] Let's now define our models for structured extraction. Note: instructor will conveniently let us use Iterable to model an iterable of our class. In this case, once we define our product recommendation model, we can slap on Iterable to define what we ultimately want - a (ranked) list of product recommendations. import instructor from openai import OpenAI from typing import Iterable from pydantic import BaseModel client = instructor.patch(OpenAI(), mode=instructor.function\\\\\\_calls.Mode.JSON) class ProductRecommendation(BaseModel): product\\\\\\_id: str product\\\\\\_name: str Recommendations = Iterable\\\\\\[ProductRecommendation\\\\\\] Now let's use our instructor patch. Since we don't want to wait for all the tokens to finish, will set stream to True and process each product recommendation as it comes in: prompt = f\"Based on the following user profile:\\\\\\\\n{profile\\\\\\_data}\\\\\\\\nRank the following products from most relevant to least relevant:\\\\\\\\n\" + '\\\\\\\\n'.join(f\"{product\\\\\\['product\\\\\\_id'\\\\\\]} {product\\\\\\['product\\\\\\_name'\\\\\\]}\" for product in products) start\\\\\\_perf = time.perf\\\\\\_counter() recommendations\\\\\\_stream = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Iterable\\\\\\[ProductRecommendation\\\\\\], stream=True, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"}, {\"role\": \"user\", \"content\": prompt} \\\\\\] ) for product in recommendations\\\\\\_stream: print(product) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (generator): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") break product\\\\\\_id='1' product\\\\\\_name='Apple MacBook Air (2023)' Time for first result (generator): 4.33 seconds recommendations\\\\\\_stream is a generator! It yields the extracted products as it's processing the stream in real-time. Now let's get the same response without streaming and see how they compare. start\\\\\\_perf = time.perf\\\\\\_counter() recommendations\\\\\\_list = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Iterable\\\\\\[ProductRecommendation\\\\\\], stream=False, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"}, {\"role\": \"user\", \"content\": prompt} \\\\\\] ) print(recommendations\\\\\\_list\\\\\\[0\\\\\\]) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (list): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") product\\\\\\_id='1' product\\\\\\_name='Apple MacBook Air (2023)' Time for first result (list): 8.63 seconds Our web application now displays results faster. Even a 100ms improvement can lead to a 1% increase in revenue. FastAPI We can also take this and set up a streaming LLM API endpoint using FastAPI. Check out our docs on using FastAPI here! Key Takeaways To summarize, we looked at: • Generators in Python: A powerful feature that allows for efficient data handling with reduced latency • LLM Streaming: LLMs provide us generators to stream tokens and Instructor can let us validate and extract data from this stream. Real-time data validation ftw! Don't forget to check our GitHub for more resources and give us a star if you find the library helpful! If you have any questions or need further clarifications, feel free to reach out or dive into the Instructor library's documentation for more detailed information. Happy coding! Continue reading 2023/11/18 4 min read Verifying LLM Citations with Pydantic Ensuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification. We'll start with using a simple substring check to verify citations. Then we'll use instructor itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses. Example 1: Simple Substring Check In this example, we use the Statements class to verify if a given substring quote exists within a text chunk. If the substring is not found, an error is raised. Code Example: from typing import List, Optional from openai import OpenAI from pydantic import BaseModel, Field, ValidationError, ValidationInfo, field\\\\\\_validator, model\\\\\\_validator import instructor client = instructor.patch(OpenAI()) class Statements(BaseModel): body: str substring\\\\\\_quote: str @field\\\\\\_validator(\"substring\\\\\\_quote\") @classmethod def substring\\\\\\_quote\\\\\\_exists(cls, v: str, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) for text\\\\\\_chunk in context.values(): if v in text\\\\\\_chunk: # While we use a simple substring check in this example, we can use more complex techniques like regex or Levenshtein distance. return v raise ValueError(\"Could not find substring\\\\\\_quote \\\\\\`{v}\\\\\\` in contexts\") class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] Once the class is defined, we can use it to validate the context and raise an error if the substring is not found. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is not the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example: answer.0.substring\\\\\\_quote Value error, Could not find substring\\\\\\_quote \\\\\\`Paris is the capital of France\\\\\\` in contexts \\\\\\[type=value\\\\\\_error, input\\\\\\_value='Paris is the capital of France', input\\\\\\_type=str\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Pydantic raises a validation error when the substring\\\\\\_quote attribute does not exist in the context. This approach can be used to validate more complex data using techniques like regex or Levenshtein distance. Example 2: Using LLM for Verification This approach leverages OpenAI's LLM to validate citations. If the citation does not exist in the context, the LLM returns an error message. Code Example: class Validation(BaseModel): is\\\\\\_valid: bool error\\\\\\_messages: Optional\\\\\\[str\\\\\\] = Field(None, description=\"Error messages if any\") class Statements(BaseModel): body: str substring\\\\\\_quote: str @model\\\\\\_validator(mode=\"after\") def substring\\\\\\_quote\\\\\\_exists(self, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) resp: Validation = client.chat.completions.create( response\\\\\\_model=Validation, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Does the following citation exist in the following context?\\\\\\\\n\\\\\\\\nCitation: {self.substring\\\\\\_quote}\\\\\\\\n\\\\\\\\nContext: {context}\", } \\\\\\], model=\"gpt-3.5-turbo\", ) if resp.is\\\\\\_valid: return self raise ValueError(resp.error\\\\\\_messages) class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] Now when we use a correct citation, the LLM returns a valid response. resp = AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is the capital of France\", 3: \"Irrelevant data\", } }, ) print(resp.model\\\\\\_dump\\\\\\_json(indent=2)) Result: { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ { \"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\" } \\\\\\] } When we have citations that don't exist in the context, the LLM returns an error message. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is not the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example: 1 validation error for AnswerWithCitaton answer.0 Value error, Citation not found in context \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'body': 'Paris', 'substr... the capital of France'}, input\\\\\\_type=dict\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Example 3: Aligning Citations and Answers In this example, we ensure that the provided answers are aligned with the given citations and context. The LLM is used to verify the alignment. We use the same Statements model as above, but we add a new model for the answer that also verifies the alignment of citations. Code Example: class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] @model\\\\\\_validator(mode=\"after\") def validate\\\\\\_answer(self, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) resp: Validation = client.chat.completions.create( response\\\\\\_model=Validation, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Does the following answers match the question and the context?\\\\\\\\n\\\\\\\\nQuestion: {self.question}\\\\\\\\n\\\\\\\\nAnswer: {self.answer}\\\\\\\\n\\\\\\\\nContext: {context}\", } \\\\\\], model=\"gpt-3.5-turbo\", ) if resp.is\\\\\\_valid: return self raise ValueError(resp.error\\\\\\_messages) When we have a mismatch between the answer and the citation, the LLM returns an error message. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Texas\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example: 1 validation error for AnswerWithCitaton Value error, The answer does not match the question and context \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'question': 'What is the...he capital of France'}\\\\\\]}, input\\\\\\_type=dict\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Conclusion These examples demonstrate the potential of using Pydantic and OpenAI to enhance data accuracy through citation verification. While the LLM-based approach may not be efficient for runtime operations, it has exciting implications for generating a dataset of accurate responses. By leveraging this method during data generation, we can fine-tune a model that excels in citation accuracy. Similar to our last post on finetuning a better summarizer. If you like the content check out our GitHub as give us a star and checkout the library. Continue reading 2023/11/13 6 min read Introduction to Batch Processing using asyncio and Instructor Today, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using instructor and learn how to use asyncio.gather and asyncio.as\\\\\\_completed for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using asyncio.Semaphore. Github Example If you want to run the code examples in this article, you can find them on jxnl/instructor We will start by defining an async function that calls openai to extract data, and then examine four different ways to execute it. We will discuss the pros and cons of each approach and analyze the results of running them on a small batch. Understanding asyncio asyncio is a Python library that enables writing concurrent code using the async/await syntax. It is particularly useful for IO-bound and structured network code. If you are familiar with OpenAI's SDK, you might have encountered two classes: OpenAI() and AsyncOpenAI(). Today, we will be using the AsyncOpenAI() class, which processes data asynchronously. By utilizing these tools in web applications or batch processing, we can significantly improve performance by handling multiple requests concurrently instead of sequentially. Understanding async and await We will be using the async and await keywords to define asynchronous functions. The async keyword is used to define a function that returns a coroutine object. The await keyword is used to wait for the result of a coroutine object. If you want to understand the deeper details of asyncio, I recommend reading this article by Real Python. Understanding gather vs as\\\\\\_completed In this post we'll show two ways to run tasks concurrently: asyncio.gather and asyncio.as\\\\\\_completed. The gather method is used to run multiple tasks concurrently and return the results as a list. The as\\\\\\_completed returns a iterable is used to run multiple tasks concurrently and return the results as they complete. Another great resource on the differences between the two can be found here. Example: Batch Processing In this example, we will demonstrate how to use asyncio for batch processing tasks, specifically for extracting and processing data concurrently. The script will extract data from a list of texts and process it concurrently using asyncio. import instructor from pydantic import BaseModel from openai import AsyncOpenAI # Enables \\\\\\`response\\\\\\_model\\\\\\` in \\\\\\`create\\\\\\` method client = instructor.apatch(AsyncOpenAI()) We use instructor.apatch to patch the create method of AsyncOpenAI to accept a response\\\\\\_model argument. This is because the create method of AsyncOpenAI does not accept a response\\\\\\_model argument without this patch. class Person(BaseModel): name: str age: int async def extract\\\\\\_person(text: str) -> Person: return await client.chat.completions.create( We use await here to wait for the response from the server before we return the result. This is because create returns a coroutine object, not the result of the coroutine. model=\"gpt-3.5-turbo\", messages=\\\\\\[ {\"role\": \"user\", \"content\": text}, \\\\\\], response\\\\\\_model=Person, ) Notice that now there are async and await keywords in the function definition. This is because we're using the asyncio library to run the function concurrently. Now lets define a batch of texts to process. dataset = \\\\\\[ \"My name is John and I am 20 years old\", \"My name is Mary and I am 21 years old\", \"My name is Bob and I am 22 years old\", \"My name is Alice and I am 23 years old\", \"My name is Jane and I am 24 years old\", \"My name is Joe and I am 25 years old\", \"My name is Jill and I am 26 years old\", \\\\\\] for loop: Running tasks sequentially. persons = \\\\\\[\\\\\\] for text in dataset: person = await extract\\\\\\_person(text) persons.append(person) Even though there is an await keyword, we still have to wait for each task to finish before starting the next one. This is because we're using a for loop to iterate over the dataset. This method, which uses a for loop, will be the slowest among the four methods discussed today. asyncio.gather: Running tasks concurrently. async def gather(): tasks\\\\\\_get\\\\\\_persons = \\\\\\[extract\\\\\\_person(text) for text in dataset\\\\\\] all\\\\\\_persons = await asyncio.gather(\\\\\\*tasks\\\\\\_get\\\\\\_persons) We use await here to wait for all the tasks to finish before assigning the result to all\\\\\\_persons. This is because asyncio.gather returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.as\\\\\\_completed to achieve the same result. Using asyncio.gather allows us to return all the results at once. It is an effective way to speed up our code, but it's not the only way. Particularly, if we have a large dataset, we might not want to wait for everything to finish before starting to process the results. This is where asyncio.as\\\\\\_completed comes into play. asyncio.as\\\\\\_completed: Handling tasks as they complete. async def as\\\\\\_completed(): all\\\\\\_persons = \\\\\\[\\\\\\] tasks\\\\\\_get\\\\\\_persons = \\\\\\[extract\\\\\\_person(text) for text in dataset\\\\\\] for person in asyncio.as\\\\\\_completed(tasks\\\\\\_get\\\\\\_persons): all\\\\\\_persons.append(await person) We use await here to wait for each task to complete before appending it to the list. This is because as\\\\\\_completed returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.gather to achieve the same result. This method is a great way to handle large datasets. We can start processing the results as they come in, especially if we are streaming data back to a client. However, these methods aim to complete as many tasks as possible as quickly as possible. This can be problematic if we want to be considerate to the server we're making requests to. This is where rate limiting comes into play. While there are libraries available to assist with rate limiting, for our initial defense, we will use a semaphore to limit the number of concurrent requests we make. Ordering of results Its important to note that the order of the results will not be the same as the order of the dataset. This is because the tasks are completed in the order they finish, not the order they were started. If you need to preserve the order of the results, you can use asyncio.gather instead. Rate-Limited Gather: Using semaphores to limit concurrency. sem = asyncio.Semaphore(2) async def rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text: str, sem: Semaphore) -> Person: async with sem: We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to. return await extract\\\\\\_person(text) async def rate\\\\\\_limited\\\\\\_gather(sem: Semaphore): tasks\\\\\\_get\\\\\\_persons = \\\\\\[rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text, sem) for text in dataset\\\\\\] resp = await asyncio.gather(\\\\\\*tasks\\\\\\_get\\\\\\_persons) Rate-Limited As Completed: Using semaphores to limit concurrency. sem = asyncio.Semaphore(2) async def rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text: str, sem: Semaphore) -> Person: async with sem: We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to. return await extract\\\\\\_person(text) async def rate\\\\\\_limited\\\\\\_as\\\\\\_completed(sem: Semaphore): all\\\\\\_persons = \\\\\\[\\\\\\] tasks\\\\\\_get\\\\\\_persons = \\\\\\[rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text, sem) for text in dataset\\\\\\] for person in asyncio.as\\\\\\_completed(tasks\\\\\\_get\\\\\\_persons): all\\\\\\_persons.append(await person) We use await here to wait for each task to complete before appending it to the list. This is because as\\\\\\_completed returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.gather to achieve the same result. Now that we have seen the code, let's examine the results of processing 7 texts. As the prompts become longer or if we use GPT-4, the differences between these methods will become more pronounced. Other Options Its important to also note that here we are using a semaphore to limit the number of concurrent requests. However, there are other ways to limit concurrency esp since we have rate limit information from the openai request. You can imagine using a library like ratelimit to limit the number of requests per second. OR catching rate limit exceptions and using tenacity to retry the request after a certain amount of time. tenacity aiolimiter Results As you can see, the for loop is the slowest, while asyncio.as\\\\\\_completed and asyncio.gather are the fastest without any rate limiting. Method Execution Time Rate Limited (Semaphore) For Loop 6.17 seconds Asyncio.gather 0.85 seconds Asyncio.as\\\\\\_completed 0.95 seconds Asyncio.gather 3.04 seconds 2 Asyncio.as\\\\\\_completed 3.26 seconds 2 Practical implications of batch processing The choice of approach depends on the task's nature and the desired balance between speed and resource utilization. Here are some guidelines to consider: Use asyncio.gather for handling multiple independent tasks quickly. Apply asyncio.as\\\\\\_completed for large datasets to process tasks as they complete. Implement rate-limiting to avoid overwhelming servers or API endpoints. If you find the content helpful or want to try out Instructor, please visit our GitHub page and give us a star! Continue reading 2023/11/05 15 min read Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor In this article, we'll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4's iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density. By the end you'll end up with a GPT 3.5 model, (fine-tuned using Instructor's great tooling), capable of producing summaries that rival the effectiveness of Chain of Density \\\\\\[Adams et al. (2023)\\\\\\]. As always, all code is readily available in our examples/chain-of-density folder in our repo for your reference. Datasets and Colab Notebook Part 1) Chain of Density Summarizing extensive texts with AI can be challenging, often relying on inconsistent techniques. Their novel method, Chain Of Density prompting, enhances AI-based text summarization, outperforming human-generated summaries. Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density. First introduced in the paper - From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. The team has found that this method is able to consistently beats similar summaries written by human annotators. Implementation Details Original Prompt We can break down the original process into smaller api calls. This allows us to introduce validation at each step to ensure that we're getting the results that we want. Original Chain of Density Prompt Improved process with Instructor Data Modelling Before we begin modelling the data, let's make sure we install all of our dependencies pip install instructor aiohttp rich INITIAL SUMMARY Let's start by walking through some of the data models that we'll be using as the response\\\\\\_model for our open ai function calls Firstly, we'll need a data model for the initial summary that we will be generating. We'll take the description of this class straight from the original prompt. It's important to note that these docstrings serve a purpose, they are directly used by the LLM when generating the outputs. A quick note on Docstrings class InitialSummary(BaseModel): \"\"\" This is an initial summary which should be long ( 4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words. \"\"\" summary: str = Field( ..., description=\"This is a summary of the article provided which is overly verbose and uses fillers. It should be roughly 80 words in length\", ) REWRITTEN SUMMARY We'll also need one additional class to help model the rewritten schema class RewrittenSummary(BaseModel): \"\"\" This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. Guidelines - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" - Missing entities can appear anywhere in the new summary An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. \"\"\" summary: str = Field( ..., description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\", ) absent: List\\\\\\[str\\\\\\] = Field( ..., default\\\\\\_factory=list, description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\", ) missing: List\\\\\\[str\\\\\\] = Field( default\\\\\\_factory=list, description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\", ) Using Pydantic Validators with Instructor For a more in-depth walkthrough on how to use Pydantic validators with the Instructor library, we recommend checking out our previous article on LLM validation - Good LLM Validation is just Good Validation Ideally, we'd like for Missing to have a length between 1 and 3, Absent to be an empty list and for our rewritten summaries to keep a minimum entity density. With Instructor, we can implement this logic using native Pydantic validators that are simply declared as part of the class itself. import nltk import spacy nlp = spacy.load(\"en\\\\\\_core\\\\\\_web\\\\\\_sm\") @field\\\\\\_validator(\"summary\") def min\\\\\\_length(cls, v: str): tokens = nltk.word\\\\\\_tokenize(v) Similar to the original paper, we utilize the NLTK word tokenizer to count the number of tokens within our generated sentences. We aim for at least 60 tokens in our generated summary so that we don't lose information. num\\\\\\_tokens = len(tokens) if num\\\\\\_tokens < 60: raise ValueError( \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\" ) return v @field\\\\\\_validator(\"missing\") def has\\\\\\_missing\\\\\\_entities(cls, missing\\\\\\_entities: List\\\\\\[str\\\\\\]): if len(missing\\\\\\_entities) == 0: raise ValueError( \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\" ) return missing\\\\\\_entities @field\\\\\\_validator(\"absent\") def has\\\\\\_no\\\\\\_absent\\\\\\_entities(cls, absent\\\\\\_entities: List\\\\\\[str\\\\\\]): absent\\\\\\_entity\\\\\\_string = \",\".join(absent\\\\\\_entities) if len(absent\\\\\\_entities) > 0: print(f\"Detected absent entities of {absent\\\\\\_entity\\\\\\_string}\") raise ValueError( f\"Do not omit the following Entities {absent\\\\\\_entity\\\\\\_string} from the new summary\" ) return absent\\\\\\_entities @field\\\\\\_validator(\"summary\") def min\\\\\\_entity\\\\\\_density(cls, v: str): tokens = nltk.word\\\\\\_tokenize(v) num\\\\\\_tokens = len(tokens) # Extract Entities doc = nlp(v) We also use the spaCy library to calculate the entity density of the generated summary. num\\\\\\_entities = len(doc.ents) density = num\\\\\\_entities / num\\\\\\_tokens if density < 0.08: We also implement a minimum entity density so that we stay within a given range. 0.08 is arbitrarily chosen in this case raise ValueError( f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\" ) return v Putting it all Together Now that we have our models and the rough flow figured out, let's implement a function to summarize a piece of text using Chain Of Density summarization. from openai import OpenAI import instructor client = instructor.patch(OpenAI()) We need to apply a patch function on the OpenAI client for us to get all of the benefits that Instructor provides. With a simple patch, we can get automatic type coercion of our outputs and automatic retries for invalid outputs out of the box! def summarize\\\\\\_article(article: str, summary\\\\\\_steps: int = 3): summary\\\\\\_chain = \\\\\\[\\\\\\] # We first generate an initial summary summary: InitialSummary = client.chat.completions.create( We first generate an initial summary. Note here that we explictly ask for a summary that has 80 words and is lengthy with overly verbose fillers in the system prompt model=\"gpt-4-0613\", response\\\\\\_model=InitialSummary, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": \"The generated summary should be about 80 words.\", }, \\\\\\], max\\\\\\_retries=2, ) prev\\\\\\_summary = None summary\\\\\\_chain.append(summary.summary) for i in range(summary\\\\\\_steps): missing\\\\\\_entity\\\\\\_message = ( \\\\\\[\\\\\\] if prev\\\\\\_summary is None else \\\\\\[ { \"role\": \"user\", \"content\": f\"Please include these Missing Entities: {','.join(prev\\\\\\_summary.missing)}\", }, \\\\\\] ) new\\\\\\_summary: RewrittenSummary = client.chat.completions.create( We slightly modify the original system prompt used in the original paper to perform a rewrite of the summary. Using Instructor, we also get validation of the generated output with our field\\\\\\_validators that we defined above model=\"gpt-4-0613\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"\"\" You are going to generate an increasingly concise,entity-dense summary of the following article. Perform the following two tasks - Identify 1-3 informative entities from the following article which is missing from the previous summary - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities Guidelines - Make every word count: re-write the previous summary to improve flow and make space for additional entities - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\". - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article. - Missing entities can appear anywhere in the new summary - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \"\"\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": f\"Here is the previous summary: {summary\\\\\\_chain\\\\\\[-1\\\\\\]}\", }, \\\\\\*missing\\\\\\_entity\\\\\\_message, \\\\\\], max\\\\\\_retries=3, If you've chosen a value that is larger than 0.08, make sure to increase this value in case you need to do multiple rewrites max\\\\\\_tokens=1000, response\\\\\\_model=RewrittenSummary, ) summary\\\\\\_chain.append(new\\\\\\_summary.summary) prev\\\\\\_summary = new\\\\\\_summary return summary\\\\\\_chain This summarization function yields a result which triples the number of entities while maintaining the same number of tokens. We can also see that stylistically, the summary is a lot more natural. First Iteration This article discusses the highly-anticipated boxing match between Manny Pacquiao and Floyd Mayweather. The article revolves around Manny Pacquiao's statements about his upcoming fight and his preparations for the same. A portion of the article provides details about the financial stipulations of the match and its significance in the sporting arena. Quotes from Pacquiao illustrating his determination and his battle strategy are highlighted. The tone of the article is largely centered around creating a build-up to the upcoming mega event. Final Iteration Manny Pacquiao, the Filipino boxer, anticipates the forthcoming May 2 showdown at the MGM Grand as the fight of his life, against the undefeated American Floyd Mayweather, in a $300m bout. Despite being seen as the underdog in this high-stakes Las Vegas match, Pacquiao is confident, promising a warrior's spirit and assuring the fans who have been awaiting this encounter for a decade, that it will indeed be the biggest sporting spectacle in history worthy of their anticipation Part 2) Fine-Tuning In this section, we'll look into how to fine-tune a GPT 3.5 model so that it is able to perform at an equivalent level as a GPT-4 model. We'll then compare the performance of our model against that of GPT-4 to see how it stacks up. Creating a Training Set In order to prevent any contamination of data during testing, we randomly sampled 120 articles from the griffin/chain-of-density dataset and split these articles into a train.csv and a test.csv file which we uploaded to Hugging Face. Now, we just neeed to import the Instructions module from the Instructor package which allows you to generate a nicely formatted .jsonl file to be used for fine-tuning from typing import List from chain\\\\\\_of\\\\\\_density import summarize\\\\\\_article In this example, we're using the summarize\\\\\\_article that we defined up above. We saved it in a local file called chain\\\\\\_of\\\\\\_density.py, hence the import import csv import logging import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) We patch the default OpenAI client so that we can use the Instructor library with it logging.basicConfig(level=logging.INFO) We also need to configure logging at the INFO level. This is very important, if this is not configured, your output will not be generated. instructions = instructor.Instructions( name=\"Chain Of Density\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"generated.jsonl\")\\\\\\], openai\\\\\\_client=client, ) class GeneratedSummary(BaseModel): \"\"\" This represents a highly concise summary that includes as many entities as possible from the original source article. An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. Guidelines - Make every word count - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" \"\"\" summary: str = Field( ..., description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \", ) @instructions.distil We instantiate a Instruction object which will help us handle the conversion of our function calls into a valid .jsonl file. We also define the name of the .jsonl file in the log\\\\\\_handlers parameter def distil\\\\\\_summarization(text: str) -> GeneratedSummary: summary\\\\\\_chain: List\\\\\\[str\\\\\\] = summarize\\\\\\_article(text) return GeneratedSummary(summary=summary\\\\\\_chain\\\\\\[-1\\\\\\]) We add in an instructions.distil annotation so that we automatically capture the input and output of the function we'd like to fine-tune our model to output with open(\"train.csv\", \"r\") as file: reader = csv.reader(file) next(reader) # Skip the header for article, summary in reader: # Run Distillisation to generate the values distil\\\\\\_summarization(article) Rate Limiting We recommend running this script on a small subset of the dataset first to test you've got everything configured nicely. Don't forget to add in rate limiting error handling with tenacity and set the OPENAI\\\\\\_API\\\\\\_KEY shell environment variable before running any subsequent commands Creating Fine-Tuning Jobs Once we run this script, we'll have a new file called generated.jsonl in our local repository. Now all that's left is to run the command below to start fine-tuning your first model! instructor jobs create-from-file generated.jsonl Finetuning Reference Once the job is complete, all we need to do is to then change the annotation in the function call to distil\\\\\\_summarization in our original file above to start using our new model. @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard def distil\\\\\\_summarization(text: str) -> GeneratedSummary: summary\\\\\\_chain: List\\\\\\[str\\\\\\] = summarize\\\\\\_article(text) return GeneratedSummary(summary=summary\\\\\\_chain\\\\\\[-1\\\\\\]) With that, you've now got your own fine-tuned model ready to go and serve data in production. We've seen how Instructor can make your life easier, from fine-tuning to distillation. Results and Benchmarks We'll be comparing the following models in 3 ways using 20 articles that were not used for fine-tuning. Entity Density : This is entities per token, the higher the better for density. Latency : Time to last token generated in seconds Costs : Total cost to generate outputs - we break down the cost into training and inference costs for easy reference 3.5 Finetuned (n) This is a GPT 3.5 model that we fine-tuned on n examples. Each model was finetuned for 4-5 epochs ( This was automatically decided by the OpenAI scheduler ) GPT-4 (COD) This is a GPT4 model which we applied 3 rounds of Chain Of Density rewrites to generate a summary with using the methodology above GPT-3.5 (Vanilla) This is a GPT 3.5 model that we asked to generate entity-dense summaries which were concise. Summaries were generated in a single pass targetting about 80-90 tokens. Model Mean Latency (s) Mean Entity Density 3.5 Finetuned (20) 2.1 0.15 3.5 Finetuned (50) 2.1 0.14 3.5 Finetuned (76) 2.1 0.14 GPT-3.5 (Vanilla) 16.8 0.12 GPT-4 (COD) 49.5 0.15 Finetuning Datasets Using the OpenAI Usage Dashboard, we can calculate the cost of generating 20 summaries as seen below. Model Training Cost ($) Inference Cost ($) Tokens Used Total Cost ($) GPT-3.5 (Vanilla) - 0.20 51,162 0.2 3.5 Finetuned (20) 0.7 0.20 56,573 0.8 3.5 Finetuned (50) 1.4 0.17 49,057 1.3 3.5 Finetuned (76) 1.8 0.17 51,583 2.5 GPT-4 (COD) - 12.9 409,062 12.9 Here, we can see that GPT-4 has an approximate inference cost of 0.65 per summary while our finetuned models have an inference cost of 0.0091 per summary which is ~ 72x cheaper. Interestingly, the model finetuned with the least examples seems to outperform the others. While the reason for this is unknown, a few potential reasons could be that either we didn't train for sufficient epochs ( We chose the default 5 epochs ) or that the models started learning to imitate other behaviour such as more abstract writing styles from the larger variety of samples, resulting in a decrease in entity density. Conclusions Finetuning this iterative method was 20-40x faster while improving overall performance, resulting in massive efficiency gains by finetuning and distilling capabilities into specialized models. We've seen how Instructor can make your life easier, from data modeling to distillation and finetuning. If you enjoy the content or want to try out instructor check out the github and don't forget to give us a star! Continue reading 2023/11/02 1 min read AI Engineer Keynote: Pydantic is all you need Click here to watch the full talk Last month, I ventured back onto the speaking circuit at the inaugural AI Engineer Summit, sharing insights on leveraging Pydantic for effective prompt engineering. I dove deep into what is covered in our documentation and standard blog posts, I'd genuinely appreciate any feedback on the talk – every bit helps in refining the art. So, take a moment to check out the full talk here, and let's continue pushing the boundaries of what's possible. Continue reading 2023/10/23 10 min read Good LLM Validation is Just Good Validation What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here. Validation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like Pydantic and Instructor. We validate these outputs using a validation function which conforms to the structure seen below. def validation\\\\\\_function(value): if condition(value): raise ValueError(\"Value is not valid\") return mutation(value) What is Instructor? Instructor helps to ensure you get the exact response type you're looking for when using openai's function call api. Once you've defined the Pydantic model for your desired response, Instructor handles all the complicated logic in-between - from the parsing/validation of the response to the automatic retries for invalid responses. This means that we can build in validators 'for free' and have a clear separation of concerns between the prompt and the code that calls openai. from openai import OpenAI import instructor # pip install instructor from pydantic import BaseModel # This enables response\\\\\\_model keyword # from client.chat.completions.create client = instructor.patch(OpenAI()) To simplify your work with OpenAI models and streamline the extraction of Pydantic objects from prompts, we offer a patching mechanism for the ChatCompletion class. class UserDetail(BaseModel): name: str age: int user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] max\\\\\\_retries=3 Invalid responses that fail to be validated succesfully will trigger up to as many reattempts as you define. ) assert user.name == \"Jason\" As long as you pass in a response\\\\\\_model parameter to the ChatCompletion api call, the returned object will always be a validated Pydantic object. assert user.age == 25 In this post, we'll explore how to evolve from static, rule-based validation methods to dynamic, machine learning-driven ones. You'll learn to use Pydantic and Instructor to leverage language models and dive into advanced topics like content moderation, validating chain of thought reasoning, and contextual validation. Let's examine how these approaches with a example. Imagine that you run a software company who wants to ensure you never serve hateful and racist content. This isn't an easy job since the language around these topics change very quickly and frequently. Software 1.0: Introduction to Validations in Pydantic A simple method could be to compile a list of different words that are often associated with hate speech. For simplicity, let's assume that we've found that the words Steal and Rob are good predictors of hateful speech from our database. We can modify our validation structure above to accomodate this. This will throw an error if we pass in a string like Let's rob the bank! or We should steal from the supermarkets. Pydantic offers two approaches for this validation: using the field\\\\\\_validator decorator or the Annotated hints. Using field\\\\\\_validator decorator We can use the field\\\\\\_validator decorator to define a validator for a field in Pydantic. Here's a quick example of how we might be able to do so. from pydantic import BaseModel, ValidationError, field\\\\\\_validator from pydantic.fields import Field class UserMessage(BaseModel): message: str @field\\\\\\_validator('message') def message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words(cls, v: str) -> str: for word in v.split(): We split the sentence into its individual words and iterate through each of the words. We then try to see if any of these words are in our blacklist which in this case is just rob and steal if word.lower() in {'rob','steal'}: raise ValueError(f\"\\\\\\`{word}\\\\\\` was found in the message \\\\\\`{v}\\\\\\`\") return v try: UserMessage(message=\"This is a lovely day\") UserMessage(message=\"We should go and rob a bank\") except ValidationError as e: print(e) Since the message This is a lovely day does not have any blacklisted words, no errors are thrown. However, in the given example above, the validation fails for the message We should go and rob a bank due to the presence of the word rob and the corresponding error message is displayed. 1 validation error for UserMessage message Value error, \\\\\\`rob\\\\\\` was found in the message \\\\\\`We should go and rob a bank\\\\\\` \\\\\\[type=value\\\\\\_error, input\\\\\\_value='We should go and rob a bank', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Using Annotated Alternatively, you can use the Annotated function to perform the same validation. Here's an example where we utilise the same function we started with. from pydantic import BaseModel, ValidationError from typing import Annotated from pydantic.functional\\\\\\_validators import AfterValidator def message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words(value:str): for word in value.split(): if word.lower() in {'rob','steal'}: raise ValueError(f\"\\\\\\`{word}\\\\\\` was found in the message \\\\\\`{value}\\\\\\`\") return value class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words)\\\\\\] try: UserMessage(message=\"This is a lovely day\") UserMessage(message=\"We should go and rob a bank\") except ValidationError as e: print(e) This code snippet achieves the same validation result. If the user message contains any of the words in the blacklist, a ValueError is raised and the corresponding error message is displayed. 1 validation error for UserMessage message Value error, \\\\\\`rob\\\\\\` was found in the message \\\\\\`We should go and rob a bank\\\\\\` \\\\\\[type=value\\\\\\_error, input\\\\\\_value='We should go and rob a bank', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Validation is a fundamental concept in software development and remains the same when applied to AI systems. Existing programming concepts should be leveraged when possible instead of introducing new terms and standards. The underlying principles of validation remain unchanged. Suppose now that we've gotten a new message - Violence is always acceptable, as long as we silence the witness. Our original validator wouldn't throw any errors when passed this new message since it uses neither the words rob or steal. However, it's clear that it is not a message which should be published. How can we ensure that our validation logic can adapt to new challenges? Software 3.0: Validation for LLMs or powered by LLMs Building upon the understanding of simple field validators, let's delve into probabilistic validation in software 3.0, (prompt engineering). We'll introduce an LLM-powered validator called llm\\\\\\_validator that uses a statement to verify the value. We can get around this by using the inbuilt llm\\\\\\_validator class from Instructor. from instructor import llm\\\\\\_validator from pydantic import BaseModel, ValidationError from typing import Annotated from pydantic.functional\\\\\\_validators import AfterValidator class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(llm\\\\\\_validator(\"don't say objectionable things\"))\\\\\\] try: UserMessage(message=\"Violence is always acceptable, as long as we silence the witness\") except ValidationError as e: print(e) This produces the following error message as seen below 1 validation error for UserMessage message Assertion failed, The statement promotes violence, which is objectionable. \\\\\\[type=assertion\\\\\\_error, input\\\\\\_value='Violence is always accep... we silence the witness', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/assertion\\\\\\_error The error message is generated by the language model (LLM) rather than the code itself, making it helpful for re-asking the model in a later section. To better understand this approach, let's see how to build an llm\\\\\\_validator from scratch. Creating Your Own Field Level llm\\\\\\_validator Building your own llm\\\\\\_validator can be a valuable exercise to get started with Instructor and create custom validators. Before we continue, let's review the anatomy of a validator: def validation\\\\\\_function(value): if condition(value): raise ValueError(\"Value is not valid\") return value As we can see, a validator is simply a function that takes in a value and returns a value. If the value is not valid, it raises a ValueError. We can represent this using the following structure: class Validation(BaseModel): is\\\\\\_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\") error\\\\\\_message: Optional\\\\\\[str\\\\\\] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\") Using this structure, we can implement the same logic as before and utilize Instructor to generate the validation. import instructor from openai import OpenAI # Enables \\\\\\`response\\\\\\_model\\\\\\` and \\\\\\`max\\\\\\_retries\\\\\\` parameters client = instructor.patch(OpenAI()) def validator(v): statement = \"don't say objectionable things\" resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Does \\\\\\`{v}\\\\\\` follow the rules: {statement}\", }, \\\\\\], # this comes from client = instructor.patch(OpenAI()) response\\\\\\_model=Validation, The new parameter of response\\\\\\_model comes from client = instructor.patch(OpenAI()) and does not exist in the original OpenAI SDK. This allows us to pass in the Pydantic model that we want as a response. ) if not resp.is\\\\\\_valid: raise ValueError(resp.error\\\\\\_message) return v Now we can use this validator in the same way we used the llm\\\\\\_validator from Instructor. class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(validator)\\\\\\] Writing more complex validations Validating Chain of Thought A popular way of prompting large language models nowadays is known as chain of thought. This involves getting a model to generate reasons and explanations for an answer to a prompt. We can utilise Pydantic and Instructor to perform a validation to check of the reasoning is reasonable, given both the answer and the chain of thought. To do this we can't build a field validator since we need to access multiple fields in the model. Instead we can use a model validator. def validate\\\\\\_chain\\\\\\_of\\\\\\_thought(values): chain\\\\\\_of\\\\\\_thought = values\\\\\\[\"chain\\\\\\_of\\\\\\_thought\"\\\\\\] answer = values\\\\\\[\"answer\"\\\\\\] resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Verify that \\\\\\`{answer}\\\\\\` follows the chain of thought: {chain\\\\\\_of\\\\\\_thought}\", }, \\\\\\], # this comes from client = instructor.patch(OpenAI()) response\\\\\\_model=Validation, ) if not resp.is\\\\\\_valid: raise ValueError(resp.error\\\\\\_message) return values We can then take advantage of the model\\\\\\_validator decorator to perform a validation on a subset of the model's data. We're defining a model validator here which runs before Pydantic parses the input into its respective fields. That's why we have a before keyword used in the model\\\\\\_validator class. from pydantic import BaseModel, model\\\\\\_validator class AIResponse(BaseModel): chain\\\\\\_of\\\\\\_thought: str answer: str @model\\\\\\_validator(mode='before') @classmethod def chain\\\\\\_of\\\\\\_thought\\\\\\_makes\\\\\\_sense(cls, data: Any) -> Any: # here we assume data is the dict representation of the model # since we use 'before' mode. return validate\\\\\\_chain\\\\\\_of\\\\\\_thought(data) Now, when you create a AIResponse instance, the chain\\\\\\_of\\\\\\_thought\\\\\\_makes\\\\\\_sense validator will be invoked. Here's an example: try: resp = AIResponse( chain\\\\\\_of\\\\\\_thought=\"1 + 1 = 2\", answer=\"The meaning of life is 42\" ) except ValidationError as e: print(e) If we create a AIResponse instance with an answer that does not follow the chain of thought, we will get an error. 1 validation error for AIResponse Value error, The statement 'The meaning of life is 42' does not follow the chain of thought: 1 + 1 = 2. \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'chain\\\\\\_of\\\\\\_thought': '1 +... meaning of life is 42'}, input\\\\\\_type=dict\\\\\\] Validating Citations From Original Text Let's see a more concrete example. Let's say that we've asked our model a question about some text source and we want to validate that the generated answer is supported by the source. This would allow us to minimize hallucinations and prevent statements that are not backed by the original text. While we could verify this by looking up the original source manually, a more scalable approach is to use a validator to do this automatically. We can pass in additional context to our validation functions using the model\\\\\\_validate function in Pydantic so that our models have more information to work with when performing validation. This context is a normal python dictionary and can be accessed inside the info argument in our validator functions. from pydantic import ValidationInfo,BaseModel,field\\\\\\_validator class AnswerWithCitation(BaseModel): answer: str citation: str @field\\\\\\_validator('citation') @classmethod def citation\\\\\\_exists(cls, v: str, info: ValidationInfo): This info object corresponds to the value of context that we pass into the model\\\\\\_validate function as seen below. context = info.context if context: context = context.get('text\\\\\\_chunk') if v not in context: raise ValueError(f\"Citation \\\\\\`{v}\\\\\\` not found in text chunks\") return v We can then take our original example and test it against our new model try: AnswerWithCitation.model\\\\\\_validate( {\"answer\": \"Jason is a cool guy\", \"citation\": \"Jason is cool\"}, context={\"text\\\\\\_chunk\": \"Jason is just a guy\"}, This context object is just a normal python dictionary and can take in and store any arbitrary values ) except ValidationError as e: print(e) This in turn generates the following error since Jason is cool does not exist in the text Jason is just a guy. 1 validation error for AnswerWithCitation citation Value error, Citation \\\\\\`Jason is cool\\\\\\` not found in text chunks \\\\\\[type=value\\\\\\_error, input\\\\\\_value='Jason is cool', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Putting it all together with client = instructor.patch(OpenAI()) To pass this context from the client.chat.completions.create call, client = instructor.patch(OpenAI()) also passes the validation\\\\\\_context, which will be accessible from the info argument in the decorated validator functions. from openai import OpenAI import instructor # Enables \\\\\\`response\\\\\\_model\\\\\\` and \\\\\\`max\\\\\\_retries\\\\\\` parameters client = instructor.patch(OpenAI()) def answer\\\\\\_question(question:str, text\\\\\\_chunk: str) -> AnswerWithCitation: return client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Answer the question: {question} with the text chunk: {text\\\\\\_chunk}\", }, \\\\\\], response\\\\\\_model=AnswerWithCitation, validation\\\\\\_context={\"text\\\\\\_chunk\": text\\\\\\_chunk}, ) Error Handling and Re-Asking Validators can ensure certain properties of the outputs by throwing errors, in an AI system we can use the errors and allow language model to self correct. The by running client = instructor.patch(OpenAI()) not only do we add response\\\\\\_model and validation\\\\\\_context it also allows you to use the max\\\\\\_retries parameter to specify the number of times to try and self correct. This approach provides a layer of defense against two types of bad outputs: Pydantic Validation Errors (code or LLM-based) JSON Decoding Errors (when the model returns an incorrect response) Define the Response Model with Validators To keep things simple lets assume we have a model that returns a UserModel object. We can define the response model using Pydantic and add a field validator to ensure that the name is in uppercase. from pydantic import BaseModel, field\\\\\\_validator class UserModel(BaseModel): name: str age: int @field\\\\\\_validator(\"name\") @classmethod def validate\\\\\\_name(cls, v): if v.upper() != v: raise ValueError(\"Name must be in uppercase.\") return v This is where the max\\\\\\_retries parameter comes in. It allows the model to self correct and retry the prompt using the error message rather than the prompt. model = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"}, \\\\\\], # Powered by client = instructor.patch(OpenAI()) response\\\\\\_model=UserModel, max\\\\\\_retries=2, ) assert model.name == \"JASON\" In this example, even though there is no code explicitly transforming the name to uppercase, the model is able to correct the output. Conclusion From the simplicity of Pydantic and Instructor to the dynamic validation capabilities of LLMs, the landscape of validation is changing but without needing to introduce new contepts. It's clear that the future of validation is not just about preventing bad data but about allowing llms to understand the data and correcting it. If you enjoy the content or want to try out Instructor please check out the github and give us a star! Continue reading 2023/10/17 4 min read Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation Introduction Get ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the instructor.instructions streamlines this process, making the task you want to distil more efficient and powerful while preserving its original functionality and backwards compatibility. If you want to see the full example checkout examples/distillation Why use Instructor? Imagine you're developing a backend service that uses a mix old and new school ML practises, it may involve pipelines with multiple function calls, validations, and data processing. Sounds cumbersome, right? That's where Instructor comes in. It simplifies complex procedures, making them more efficient and easier to manage by adding a decorator to your function that will automatically generate a dataset for fine-tuning and help you swap out the function implementation. Quick Start: How to Use Instructor's Distillation Feature Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file. import logging import random from pydantic import BaseModel from instructor import Instructions # pip install instructor # Logging setup logging.basicConfig(level=logging.INFO) instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"math\\\\\\_finetunes.jsonl\")\\\\\\] ) class Multiply(BaseModel): a: int b: int result: int # Define a function with distillation # The decorator will automatically generate a dataset for fine-tuning # They must return a pydantic model to leverage function calling @instructions.distil def fn(a: int, b: int) -> Multiply: resp = a \\\\\\* b return Multiply(a=a, b=b, result=resp) # Generate some data for \\\\\\_ in range(10): a = random.randint(100, 999) b = random.randint(100, 999) print(fn(a, b)) The Intricacies of Fine-tuning Language Models Fine-tuning isn't just about writing a function like def f(a, b): return a \\\\\\* b. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this. Why Instructor and Distillation are Game Changers The library offers two main benefits: Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code. Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions. Role of Instructor in Simplifying Fine-Tuning The from instructor import Instructions feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior. Logging Output and Running a Finetune Here's how the logging output would look: { \"messages\": \\\\\\[ {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'}, {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'}, {\"role\": \"assistant\", \"function\\\\\\_call\": { \"name\": \"Multiply\", \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}' } } \\\\\\], \"functions\": \\\\\\[ {\"name\": \"Multiply\", \"description\": \"Correctly extracted \\\\\\`Multiply\\\\\\`...\"} \\\\\\] } Run a finetune like this: Don't forget to set your OpenAI Key as an environment variable All of the instructor jobs commands assume you've set an environment variable of OPENAI\\\\\\_API\\\\\\_KEY in your shell. You can set this by running the command export OPENAI\\\\\\_API\\\\\\_KEY= in your shell instructor jobs create-from-file math\\\\\\_finetunes.jsonl Next Steps and Future Plans Here's a sneak peek of what I'm planning: from instructor import Instructions, patch patch() Don't forget to run the patch() command that we provide with the Instructor package. This helps automatically serialize the content back into the \\\\\\`Pydantic\\\\\\`\\\\\\` model that we're looking for. class Multiply(BaseModel): a: int b: int result: int instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", ) @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard def fn(a: int, b: int) -> Multiply: resp = a + b return Multiply(a=a, b=b, result=resp) With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation. Conclusion We've seen how Instructor can make your life easier, from fine-tuning to distillation. Now if you're thinking wow, I'd love a backend service to do this for continously, you're in luck! Please check out the survey at useinstructor.com and let us know who you are. If you enjoy the content or want to try out instructor please check out the github and give us a star! Continue reading 2023/09/17 7 min read RAG is more than just embedding search With the advent of large language models (LLM), retrival augmented generation (RAG) has become a hot topic. However throught the past year of helping startups integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware. What is RAG? Retrival augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized. Simple RAG that embedded the user query and makes a search. So let's kick things off by examining what I like to call the 'Dumb' RAG Model—a basic setup that's more common than you'd think. The 'Dumb' RAG Model When you ask a question like, \"what is the capital of France?\" The RAG 'dumb' model embeds the query and searches in some unopinonated search endpoint. Limited to a single method API like search(query: str) -> List\\\\\\[str\\\\\\]. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings. Why is this a problem? Query-Document Mismatch: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation! Monolithic Search Backend: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more. Limitation of text search: Restricts complex queries to a single string ({query: str}), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, asking what problems did we fix last week cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week. Limited ability to plan: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context its able to plan a suite of queries to execute to return the best results. Now let's dive into how we can make it smarter with query understanding. This is where things get interesting. Improving the RAG Model with Query Understanding Shoutouts Much of this work has been inspired by / done in collab with a few of my clients at new.computer, Metaphor Systems, and Naro, go check them out! Ultimately what you want to deploy is a system that understands how to take the query and rewrite it to improve precision and recall. Query Understanding system routes to multiple search backends. Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems. Whats instructor? Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling API. Widespread Adoption: Pydantic is a popular tool among Python developers. Simplicity: Pydantic allows model definition in Python. Framework Compatibility: Many Python frameworks already use Pydantic. Case Study 1: Metaphor Systems Take Metaphor Systems, which turns natural language queries into their custom search-optimized query. If you take a look web UI you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using a language model, and turns it into a fully specified metaphor systems query. Metaphor Systems UI If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. It's actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library class DateRange(BaseModel): start: datetime.date end: datetime.date class MetaphorQuery(BaseModel): rewritten\\\\\\_query: str published\\\\\\_daterange: DateRange domains\\\\\\_allow\\\\\\_list: List\\\\\\[str\\\\\\] async def execute(): return await metaphor.search(...) Note how we model a rewritten query, range of published dates, and a list of domains to search in. This powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works. import instructor from openai import OpenAI # Enables response\\\\\\_model in the openai client client = instructor.patch(OpenAI()) query = client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=MetaphorQuery, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\" }, { \"role\": \"user\", \"content\": \"What are some recent developments in AI?\" } \\\\\\], ) Example Output { \"rewritten\\\\\\_query\": \"novel developments advancements ai artificial intelligence machine learning\", \"published\\\\\\_daterange\": { \"start\": \"2023-09-17\", \"end\": \"2021-06-17\" }, \"domains\\\\\\_allow\\\\\\_list\": \\\\\\[\"arxiv.org\"\\\\\\] } This isn't just about adding some date ranges. It's about nuanced, tailored searches, that are deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features. class DateRange(BaseModel): start: datetime.date end: datetime.date chain\\\\\\_of\\\\\\_thought: str = Field( None, description=\"Think step by step to plan what is the best time range to search in\" ) Now, let's see how this approach can help model an agent like personal assistant. Case Study 2: Personal Assistant Another great example of this multiple dispatch pattern is a personal assistant. You might ask, \"What do I have today?\", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts. class ClientSource(enum.Enum): GMAIL = \"gmail\" CALENDAR = \"calendar\" class SearchClient(BaseModel): query: str keywords: List\\\\\\[str\\\\\\] email: str source: ClientSource start\\\\\\_date: datetime.date end\\\\\\_date: datetime.date async def execute(self) -> str: if self.source == ClientSource.GMAIL: ... elif self.source == ClientSource.CALENDAR: ... class Retrival(BaseModel): queries: List\\\\\\[SearchClient\\\\\\] async def execute(self) -> str: return await asyncio.gather(\\\\\\*\\\\\\[query.execute() for query in self.queries\\\\\\]) Now we can call this with a simple query like \"What do I have today?\" and it will try to async dispatch to the correct backend. It's still important to prompt the language model well, but we'll leave that for another day. import instructor from openai import OpenAI # Enables response\\\\\\_model in the openai client client = instructor.patch(OpenAI()) retrival = client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=Retrival, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"}, {\"role\": \"user\", \"content\": \"What do I have today?\"} \\\\\\], ) Example Output { \"queries\": \\\\\\[ { \"query\": None, \"keywords\": None, \"email\": \"jason@example.com\", \"source\": \"gmail\", \"start\\\\\\_date\": \"2023-09-17\", \"end\\\\\\_date\": None }, { \"query\": None, \"keywords\": \\\\\\[\"meeting\", \"call\", \"zoom\"\\\\\\]\\\\\\]\\\\\\], \"email\": \"jason@example.com\", \"source\": \"calendar\", \"start\\\\\\_date\": \"2023-09-17\", \"end\\\\\\_date\": None } \\\\\\] } Notice that we have a list of queries that route to different search backends (email and calendar). We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well. Perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app. Can I used framework X? I get this question a lot, but it's just code. Within these dispatchs you can do whatever you want. You can use input() to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information. The sky is the limit. Both of these examples showcase how both search providors and consumers can use instructor to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend. Conclusion This isnt about fancy embedding tricks, it's just plain old information retrival and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place. What's Next? Here I want to show that \\\\\\`instructor\\\\\\`\\\\\\` isn’t just about data extraction. It’s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning — the untapped goldmine is skilled use of tools and APIs. If you enjoy the content or want to try out instructor please check out the github and give us a star! Continue reading 1 2 Back to top Previous Welcome to the Instructor Blog Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Introduction - Instructor",
    "url": "https://jxnl.github.io/instructor/cli/?q=",
    "html": "Skip to content Instructor Introduction Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog CLI Reference Finetuning GPT-3.5 Usage Tracking Table of contents Quick Start Installation & Setup Features Support & Contribution Instructor CLI¶ Welcome to the Instructor Command-Line Interface (CLI), a tool designed to ease your experience with the OpenAI API. Whether it's tracking your API usage or fine-tuning your models, Instructor CLI is your go-to utility. Quick Start¶ First things first: make sure your OpenAI API key is set as an environment variable. The CLI will use this for authenticating your requests to OpenAI's services. You can set the API key in your terminal as follows: export OPENAI\\\\\\_API\\\\\\_KEY=\"your-api-key-here\" Installation & Setup¶ pip install instructor Features¶ API Usage Monitoring: Keep tabs on your API usage right from the terminal. Track token counts, total requests, and even calculate the costs. To learn more, consult the Usage Guide. Model Fine-Tuning: Optimize your models to meet your specific requirements using our fine-tuning app. For more details, check out the Fine-Tuning Guide. Support & Contribution¶ Need help or want to contribute? Visit our GitHub Repository Was this page helpful? Back to top Previous Open Source Next Finetuning GPT-3.5 Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Overview - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/?q=",
    "html": "Skip to content Instructor Overview Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Quick Links Function Calls by Example¶ Quick Links¶ How are single and multi-label classifications done using enums? How is AI self-assessment implemented with llm\\\\\\_validator? How are exact citations retrieved using regular expressions and smart prompting? How are search queries segmented through function calling and multi-task definitions? How are knowledge graphs generated from questions? How are complex queries decomposed into subqueries in a single request? How are entities extracted and resolved from documents? How are recursive schemas implemented and understood? How are tables extracted automatically from textual data? How is multi-file code generation accomplished? How is Personally Identifiable Information sanitized from documents? How are action items and dependencies generated from transcripts? How to enable OpenAI's moderation How to extract tables from images How to generate advertising copy from image inputs Explore more! Was this page helpful? Back to top Previous Type Adapter Next Text Classification Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Philosophy - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/philosophy/?q=",
    "html": "Skip to content Instructor Philosophy Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents The Bridge to Object-Oriented Programming The zen of instructor My Goals Philosophy¶ The instructor values simplicity and flexibility in leveraging language models (LLMs). It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions. Let Pydantic do the heavy lifting. “Simplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.” — Edsger Dijkstra The Bridge to Object-Oriented Programming¶ instructor acts as a bridge converting text-based LLM interactions into a familiar object-oriented format. Its integration with Pydantic provides type hints, runtime validation, and robust IDE support; love and supported by many in the Python ecosystem. By treating LLMs as callable functions returning typed objects, instructor makes language models backwards compatible with code, making them practical for everyday use while being complex enough for advanced applications. The zen of instructor¶ Maintain the flexibility and power of Python, without unnecessary constraints. Begin with a function and a return type hint – simplicity is key. With my experience maintaining a large enterprize framework at my previous job over many years I've learned that the goal of a making a useful framework is minimizing regret, both for the author and hopefully for the user. Define a Schema class StructuredData(BaseModel): Define validators and methods on your schema. Encapsulate all your LLM logic into a function def extract(a) -> StructuredData: Define typed computations against your data with def compute(data: StructuredData): or call methods on your schema data.compute() It should be that simple. My Goals¶ The goal for the library, documentation, and blog, is to help you be a better python programmer and as a result a better AI engineer. The library is a result of my desire for simplicity. The library should help maintain simplicity in your codebase. I won't try to write prompts for you, I don't try to create indirections or abstractions that make it hard to debug in the future Please note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit me up on twitter Cheers! Was this page helpful? Back to top Previous Tips Next Models Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Tips - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/prompting/?q=",
    "html": "Skip to content Instructor Tips Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Tips Table of contents Modular Chain of Thought Utilize Optional Attributes Handling Errors Within Function Calls Simplification with the Maybe Pattern Tips for Enumerations Reiterate Long Instructions Handle Arbitrary Properties Limiting the Length of Lists Advanced Arbitrary Properties Defining Relationships Between Entities Reusing Components with Different Contexts General Tips for Prompt Engineering¶ The overarching theme of using Instructor and Pydantic for function calling is to make the models as self-descriptive, modular, and flexible as possible, while maintaining data integrity and ease of use. Modularity: Design self-contained components for reuse. Self-Description: Use Pydantic's Field for clear field descriptions. Optionality: Use Python's Optional type for nullable fields and set sensible defaults. Standardization: Employ enumerations for fields with a fixed set of values; include a fallback option. Dynamic Data: Use key-value pairs for arbitrary properties and limit list lengths. Entity Relationships: Define explicit identifiers and relationship fields. Contextual Logic: Optionally add a \"chain of thought\" field in reusable components for extra context. Modular Chain of Thought¶ This approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT. from pydantic import BaseModel, Field class Role(BaseModel): chain\\\\\\_of\\\\\\_thought: str = Field(..., description=\"Think step by step to determine the correct title\") title: str class UserDetail(BaseModel): age: int name: str role: Role Utilize Optional Attributes¶ Use Python's Optional type and set a default value to prevent undesired defaults like empty strings. from typing import Optional class UserDetail(BaseModel): age: int name: str role: Optional\\\\\\[str\\\\\\] = Field(default=None) Handling Errors Within Function Calls¶ You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow. class UserDetail(BaseModel): age: int name: str role: Optional\\\\\\[str\\\\\\] = Field(default=None) class MaybeUser(BaseModel): result: Optional\\\\\\[UserDetail\\\\\\] = Field(default=None) error: bool = Field(default=False) message: Optional\\\\\\[str\\\\\\] def \\\\\\_\\\\\\_bool\\\\\\_\\\\\\_(self): return self.result is not None With the MaybeUser class, you can either receive a UserDetail object in result or get an error message in message. Simplification with the Maybe Pattern¶ You can further simplify this using instructor to create the Maybe pattern dynamically from any BaseModel. import instructor MaybeUser = instructor.Maybe(UserDetail) This allows you to quickly create a Maybe type for any class, streamlining the process. Tips for Enumerations¶ To prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty. from enum import Enum, auto class Role(Enum): PRINCIPAL = auto() TEACHER = auto() STUDENT = auto() OTHER = auto() class UserDetail(BaseModel): age: int name: str role: Role = Field(description=\"Correctly assign one of the predefined roles to the user.\") If you're having a hard time with Enum and alternative is to use Literal class UserDetail(BaseModel): age: int name: str role: Literal\\\\\\[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"\\\\\\] If you'd like to improve performance more you can reiterate the requirements in the field descriptions or in the docstrings. Reiterate Long Instructions¶ For complex attributes, it helps to reiterate the instructions in the field's description. class Role(BaseModel): \"\"\" Extract the role based on the following rules ... \"\"\" instructions: str = Field(..., description=\"Restate the instructions and rules to correctly determine the title.\") title: str class UserDetail(BaseModel): age: int name: str role: Role Handle Arbitrary Properties¶ When you need to extract undefined attributes, use a list of key-value pairs. from typing import List class Property(BaseModel): key: str value: str class UserDetail(BaseModel): age: int name: str properties: List\\\\\\[Property\\\\\\] = Field(..., description=\"Extract any other properties that might be relevant.\") Limiting the Length of Lists¶ When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties. class Property(BaseModel): index: str = Field(..., description=\"Monotonically increasing ID\") key: str value: str class UserDetail(BaseModel): age: int name: str properties: List\\\\\\[Property\\\\\\] = Field(..., description=\"Numbered list of arbitrary extracted properties, should be less than 6\") Using Tuples for Simple Types For simple types, tuples can be a more compact alternative to custom classes, especially when the properties don't require additional descriptions. class UserDetail(BaseModel): age: int name: str properties: List\\\\\\[Tuple\\\\\\[int, str\\\\\\]\\\\\\] = Field(..., description=\"Numbered list of arbitrary extracted properties, should be less than 6\") Advanced Arbitrary Properties¶ For multiple users, aim to use consistent key names when extracting properties. class UserDetails(BaseModel): \"\"\" Extract information for multiple users. Use consistent key names for properties across users. \"\"\" users: List\\\\\\[UserDetail\\\\\\] This refined guide should offer a cleaner and more organized approach to structure engineering in Python. Defining Relationships Between Entities¶ In cases where relationships exist between entities, it's vital to define them explicitly in the model. The following example demonstrates how to define relationships between users by incorporating an id and a friends field: class UserDetail(BaseModel): id: int = Field(..., description=\"Unique identifier for each user.\") age: int name: str friends: List\\\\\\[int\\\\\\] = Field(..., description=\"Correct and complete list of friend IDs, representing relationships between users.\") class UserRelationships(BaseModel): users: List\\\\\\[UserDetail\\\\\\] = Field(..., description=\"Collection of users, correctly capturing the relationships among them.\") Reusing Components with Different Contexts¶ You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both work\\\\\\_time and leisure\\\\\\_time. class TimeRange(BaseModel): start\\\\\\_time: int = Field(..., description=\"The start time in hours.\") end\\\\\\_time: int = Field(..., description=\"The end time in hours.\") class UserDetail(BaseModel): id: int = Field(..., description=\"Unique identifier for each user.\") age: int name: str work\\\\\\_time: TimeRange = Field(..., description=\"Time range during which the user is working.\") leisure\\\\\\_time: TimeRange = Field(..., description=\"Time range reserved for leisure activities.\") Sometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations. class TimeRange(BaseModel): chain\\\\\\_of\\\\\\_thought: str = Field(..., description=\"Step by step reasoning to get the correct time range\") start\\\\\\_time: int = Field(..., description=\"The start time in hours.\") end\\\\\\_time: int = Field(..., description=\"The end time in hours.\") Was this page helpful? Back to top Previous Contributing Next Philosophy Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Usage Tracking - Instructor",
    "url": "https://jxnl.github.io/instructor/cli/usage/",
    "html": "Skip to content Instructor Usage Tracking Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog CLI Reference Finetuning GPT-3.5 Usage Tracking Table of contents Monitoring API Usage View Usage Options List Usage for Specific Number of Days List Usage for Today Using the OpenAI API Usage CLI¶ The OpenAI API Usage CLI tool provides functionalities for monitoring your OpenAI API usage, breaking it down by model, date, and cost. Monitoring API Usage¶ View Usage Options¶ $ instructor usage --help Usage: instructor usage \\\\\\[OPTIONS\\\\\\] COMMAND \\\\\\[ARGS\\\\\\]... Check OpenAI API usage data ╭─ Options ───────────────────────────────────────────────────────╮ │ --help Show this message and exit. │ ╰─────────────────────────────────────────────────────────────────╯ ╭─ Commands ──────────────────────────────────────────────────────╮ │ list Displays OpenAI API usage data for the past N days. │ ╰─────────────────────────────────────────────────────────────────╯ List Usage for Specific Number of Days¶ To display API usage for the past 3 days, use the following command: $ instructor usage list -n 3 This will output a table similar to: Usage Summary by Date, Snapshot, and Cost ┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓ ┃ Date ┃ Snapshot ID ┃ Total Requests ┃ Total Cost ($) ┃ ┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩ │ 2023-09-04 │ gpt-4-0613 │ 44 │ 0.68 │ │ 2023-09-04 │ gpt-3.5-turbo-16k-0613 │ 195 │ 0.84 │ │ 2023-09-04 │ text-embedding-ada-002-v2 │ 276 │ 0.00 │ │ 2023-09-04 │ gpt-4-32k-0613 │ 328 │ 49.45 │ └────────────┴───────────────────────────┴────────────────┴────────────────┘ List Usage for Today¶ To display the API usage for today, simply run: $ instructor usage list Contributions¶ We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request. Was this page helpful? Back to top Previous Finetuning GPT-3.5 Next Core Library Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Finetuning GPT-3.5 - Instructor",
    "url": "https://jxnl.github.io/instructor/cli/finetune/",
    "html": "Skip to content Instructor Finetuning GPT-3.5 Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog CLI Reference Finetuning GPT-3.5 Usage Tracking Table of contents Creating a Fine-Tuning Job View Jobs Options Create from File Usage Create from ID Usage Viewing Files and Jobs Viewing Jobs Viewing Files Using the Command Line Interface¶ The instructor CLI provides functionalities for managing fine-tuning jobs on OpenAI. Incomplete API The CLI is still under development and does not yet support all features of the API. If you would like to use a feature that is not yet supported, please consider using the contributing to our library jxnl/instructor instead. !!! note \"Low hanging fruit\" If you want to contribute we're looking for a few things: 1. Adding filenames on upload Creating a Fine-Tuning Job¶ View Jobs Options¶ $ instructor jobs --help Usage: instructor jobs \\\\\\[OPTIONS\\\\\\] COMMAND \\\\\\[ARGS\\\\\\]... Monitor and create fine tuning jobs ╭─ Options ───────────────────────────────────────────────────────────────────────────────╮ │ --help Display the help message. │ ╰─────────────────────────────────────────────────────────────────────────────────────────╯ ╭─ Commands ──────────────────────────────────────────────────────────────────────────────────────────────────╮ │ cancel Cancel a fine-tuning job. │ │ create-from-file Create a fine-tuning job from a file. │ │ create-from-id Create a fine-tuning job from an existing ID. │ │ list Monitor the status of the most recent fine-tuning jobs. │ ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ Create from File¶ The create-from-file command uploads and trains a model in a single step. ❯ instructor jobs create-from-file --help Usage: instructor jobs create-from-file \\\\\\[OPTIONS\\\\\\] FILE Create a fine-tuning job from a file. ╭─ Arguments ───────────────────────────────────────────────────────────────────────────────────────╮ │ \\\\\\* file TEXT Path to the file for fine-tuning \\\\\\[default: None\\\\\\] \\\\\\[required\\\\\\] │ ╰───────────────────────────────────────────────────────────────────────────────────────────────────╯ ╭─ Options ─────────────────────────────────────────────────────────────────────────────────────────╮ │ --model TEXT Model to use for fine-tuning \\\\\\[default: gpt-3.5-turbo\\\\\\] │ │ --poll INTEGER Polling interval in seconds \\\\\\[default: 2\\\\\\] │ │ --n-epochs INTEGER Number of epochs for fine-tuning │ │ --batch-size TEXT Batch size for fine-tuning │ │ --learning-rate-multiplier TEXT Learning rate multiplier for fine-tuning │ │ --validation-file TEXT Path to the validation file \\\\\\[default: None\\\\\\] │ │ --model-suffix TEXT Suffix to identify the model \\\\\\[default: None\\\\\\] │ │ --help Show this message and exit. │ ╰──────────────────────────────────────────────────────────────────────────────── Usage¶ $ instructor jobs create-from-file transformed\\\\\\_data.jsonl --validation\\\\\\_file validation\\\\\\_data.jsonl --n\\\\\\_epochs 3 --batch\\\\\\_size 16 --learning\\\\\\_rate\\\\\\_multiplier 0.5 Create from ID¶ The create-from-id command uses an uploaded file and trains a model ❯ instructor jobs create-from-id --help Usage: instructor jobs create-from-id \\\\\\[OPTIONS\\\\\\] ID Create a fine-tuning job from an existing ID. ╭─ Arguments ───────────────────────────────────────────────────────────────────────────╮ │ \\\\\\* id TEXT ID of the existing fine-tuning job \\\\\\[default: None\\\\\\] \\\\\\[required\\\\\\] │ ╰───────────────────────────────────────────────────────────────────────────────────────╯ ╭─ Options ─────────────────────────────────────────────────────────────────────────────╮ │ --model TEXT Model to use for fine-tuning │ │ \\\\\\[default: gpt-3.5-turbo\\\\\\] │ │ --n-epochs INTEGER Number of epochs for fine-tuning │ │ --batch-size TEXT Batch size for fine-tuning │ │ --learning-rate-multiplier TEXT Learning rate multiplier for fine-tuning │ │ --validation-file-id TEXT ID of the uploaded validation file │ │ \\\\\\[default: None\\\\\\] │ │ --help Show this message and exit. │ ╰───────────────────────────────────────────────────────────────────────────────────────╯ Usage¶ $ instructor files upload transformed\\\\\\_data.jsonl $ instructor files upload validation\\\\\\_data.jsonl $ instructor files list ... $ instructor jobs create\\\\\\_from\\\\\\_id \\\\--validation\\\\\\_file \\\\--n\\\\\\_epochs 3 --batch\\\\\\_size 16 --learning\\\\\\_rate\\\\\\_multiplier 0.5 Viewing Files and Jobs¶ Viewing Jobs¶ $ instructor jobs list OpenAI Fine Tuning Job Monitoring ┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┓ ┃ ┃ ┃ ┃ Completion ┃ ┃ ┃ ┃ ┃ ┃ Job ID ┃ Status ┃ Creation Time ┃ Time ┃ Model Name ┃ File ID ┃ Epochs ┃ Base Model ┃ ┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━┩ │ ftjob-PWo6uwk… │ 🚫 cancelled │ 2023-08-23 │ N/A │ │ file-F7lJg6Z4… │ 3 │ gpt-3.5-turbo-… │ │ │ │ 23:10:54 │ │ │ │ │ │ │ ftjob-1whjva8… │ 🚫 cancelled │ 2023-08-23 │ N/A │ │ file-F7lJg6Z4… │ 3 │ gpt-3.5-turbo-… │ │ │ │ 22:47:05 │ │ │ │ │ │ │ ftjob-wGoBDld… │ 🚫 cancelled │ 2023-08-23 │ N/A │ │ file-F7lJg6Z4… │ 3 │ gpt-3.5-turbo-… │ │ │ │ 22:44:12 │ │ │ │ │ │ │ ftjob-yd5aRTc… │ ✅ succeeded │ 2023-08-23 │ 2023-08-23 │ ft:gpt-3.5-tur… │ file-IQxAUDqX… │ 3 │ gpt-3.5-turbo-… │ │ │ │ 14:26:03 │ 15:02:29 │ │ │ │ │ └────────────────┴──────────────┴────────────────┴────────────────┴─────────────────┴────────────────┴────────┴─────────────────┘ Automatically refreshes every 5 seconds, press Ctrl+C to exit Viewing Files¶ $ instructor files list OpenAI Files ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┓ ┃ File ID ┃ Size (bytes) ┃ Creation Time ┃ Filename ┃ Purpose ┃ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━┩ │ file-0lw2BSNRUlXZXRRu2beCCWjl │ 369523 │ 2023-08-23 23:31:57 │ file │ fine-tune │ │ file-IHaUXcMEykmFUp1kt2puCDEq │ 369523 │ 2023-08-23 23:09:35 │ file │ fine-tune │ │ file-ja9vRBf0FydEOTolaa3BMqES │ 369523 │ 2023-08-23 22:42:29 │ file │ fine-tune │ │ file-F7lJg6Z47CREvmx4kyvyZ6Sn │ 369523 │ 2023-08-23 22:42:03 │ file │ fine-tune │ │ file-YUxqZPyJRl5GJCUTw3cNmA46 │ 369523 │ 2023-08-23 22:29:10 │ file │ fine-tune │ └───────────────────────────────┴──────────────┴─────────────────────┴──────────┴───────────┘ Contributions¶ We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request. Was this page helpful? Back to top Previous Introduction Next Usage Tracking Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Open Source - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/open_source/",
    "html": "Skip to content Instructor Open Source Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Instructor with open source models¶ Instructor works with Open source model providers that support the OpenAI API chat endpoint See examples README here Currently tested open source model providers¶ OpenRouter Perplexity RunPod TheBloke LLMs \\\\\\*\\\\\\* \\\\\\*\\\\\\* This utilizes text-generation-webui w/ Openai plugin under the hood. Was this page helpful? Back to top Previous PII Data Sanitization Next Introduction Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "PII Data Sanitization - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/pii/",
    "html": "Skip to content Instructor PII Data Sanitization Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Overview Defining the Structures Extracting PII Data Output of Extracted PII Data Scrubbing PII Data Output of Scrubbed Document PII Data Extraction and Scrubbing¶ Overview¶ This example demonstrates the usage of OpenAI's ChatCompletion model for the extraction and scrubbing of Personally Identifiable Information (PII) from a document. The code defines Pydantic models to manage the PII data and offers methods for both extraction and sanitation. Defining the Structures¶ First, Pydantic models are defined to represent the PII data and the overall structure for PII data extraction. from typing import List from pydantic import BaseModel, Field # Define Schemas for PII data class Data(BaseModel): index: int data\\\\\\_type: str pii\\\\\\_value: str class PIIDataExtraction(BaseModel): \"\"\" Extracted PII data from a document, all data\\\\\\_types should try to have consistent property names \"\"\" private\\\\\\_data: List\\\\\\[Data\\\\\\] def scrub\\\\\\_data(self, content: str) -> str: \"\"\" Iterates over the private data and replaces the value with a placeholder in the form of <{data\\\\\\_type}\\\\\\_{i}> \"\"\" for i, data in enumerate(self.private\\\\\\_data): content = content.replace(data.pii\\\\\\_value, f\"<{data.data\\\\\\_type}\\\\\\_{i}>\") return content Extracting PII Data¶ The OpenAI API is utilized to extract PII information from a given document. from openai import OpenAI import instructor client = instructor.patch(OpenAI()) EXAMPLE\\\\\\_DOCUMENT = \"\"\" # Fake Document with PII for Testing PII Scrubbing Model # (The content here) \"\"\" pii\\\\\\_data: PIIDataExtraction = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=PIIDataExtraction, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class PII scrubbing model, Extract the PII data from the following document\", }, { \"role\": \"user\", \"content\": EXAMPLE\\\\\\_DOCUMENT, }, \\\\\\] ) # type: ignore print(\"Extracted PII Data:\") print(pii\\\\\\_data.json(indent=2)) Output of Extracted PII Data¶ { \"private\\\\\\_data\": \\\\\\[ { \"index\": 0, \"data\\\\\\_type\": \"date\", \"pii\\\\\\_value\": \"01/02/1980\" }, { \"index\": 1, \"data\\\\\\_type\": \"ssn\", \"pii\\\\\\_value\": \"123-45-6789\" }, { \"index\": 2, \"data\\\\\\_type\": \"email\", \"pii\\\\\\_value\": \"john.doe@email.com\" }, { \"index\": 3, \"data\\\\\\_type\": \"phone\", \"pii\\\\\\_value\": \"555-123-4567\" }, { \"index\": 4, \"data\\\\\\_type\": \"address\", \"pii\\\\\\_value\": \"123 Main St, Springfield, IL, 62704\" } \\\\\\] } Scrubbing PII Data¶ After extracting the PII data, the scrub\\\\\\_data method is used to sanitize the document. print(\"Scrubbed Document:\") print(pii\\\\\\_data.scrub\\\\\\_data(EXAMPLE\\\\\\_DOCUMENT)) Output of Scrubbed Document¶ # Fake Document with PII for Testing PII Scrubbing Model ## Personal Story John Doe was born on . His social security number is . He has been using the email address for years, and he can always be reached at . ## Residence John currently resides at . He's been living there for about 5 years now. Was this page helpful? Back to top Previous Multi-File Code Generation Next Open Source Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Multi-File Code Generation - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/gpt-engineer/",
    "html": "Skip to content Instructor Multi-File Code Generation Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Data Structures Calling Completions Evaluating an Example Add Refactoring Capabilities Calling Refactor Completions Creating an Example Refactoring Example: Creating Multiple Files Program¶ This example shows how to create a multiple files program based on specifications by utilizing the OpenAI Function Call. We will define the necessary data structures using Pydantic and demonstrate how to convert a specification (prompt) into multiple files. Motivation Creating multiple file programs based on specifications is a challenging and rewarding skill that can help you build complex and scalable applications. With OpenAI Function Call, you can leverage the power of language models to generate an entire codebase and code snippets that match your specifications. Defining the Data Structures¶ Let's start by defining the data structure of File and Program. from typing import List from pydantic import Field from instructor import BaseModel class File(BaseModel): \"\"\" Correctly named file with contents. \"\"\" file\\\\\\_name: str = Field( ..., description=\"The name of the file including the extension\" ) body: str = Field(..., description=\"Correct contents of a file\") def save(self): with open(self.file\\\\\\_name, \"w\") as f: f.write(self.body) class Program(BaseModel): \"\"\" Set of files that represent a complete and correct program \"\"\" files: List\\\\\\[File\\\\\\] = Field(..., description=\"List of files\") The File class represents a single file or script, and it contains a name attribute and body for the text content of the file. Notice that we added the save method to the File class. This method is used to writes the body of the file to disk using the name as path. The Program class represents a collection of files that form a complete and correct program. It contains a list of File objects in the files attribute. Calling Completions¶ To create the files, we will use the base openai API. We can define a function that takes in a string and returns a Program object. import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def develop(data: str) -> Program: return client.chat.completions.create( model=\"gpt-3.5-turbo-0613\", temperature=0.1, response\\\\\\_model=Program, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class programming AI capable of writing correct python scripts and modules. You will name files correct, include \\\\\\_\\\\\\_init\\\\\\_\\\\\\_.py files and write correct python code with correct imports.\", }, { \"role\": \"user\", \"content\": data, }, \\\\\\], max\\\\\\_tokens=1000, ) Evaluating an Example¶ Let's evaluate the example by specifying the program to create and print the resulting files. program = develop( \"\"\" Create a fastapi app with a readme.md file and a main.py file with some basic math functions. the datamodels should use pydantic and the main.py should use fastapi. the readme.md should have a title and a description. The readme should contain some helpful infromation and a curl example\"\"\" ) for file in program.files: print(file.file\\\\\\_name) print(\"-\") print(file.body) print(\"\\\\\\\\n\\\\\\\\n\\\\\\\\n\") The output will be: # readme.md - # FastAPI App This is a FastAPI app that provides some basic math functions. ## Usage To use this app, follow the instructions below: 1. Install the required dependencies by running \\\\\\`pip install -r requirements.txt\\\\\\`. 2. Start the app by running \\\\\\`uvicorn main:app --reload\\\\\\`. 3. Open your browser and navigate to \\\\\\`http://localhost:8000/docs\\\\\\` to access the Swagger UI documentation. ## Example You can use the following curl command to test the \\\\\\`/add\\\\\\` endpoint: \\\\\\`\\\\\\`\\\\\\`bash $ curl -X POST -H \"Content-Type: application/json\" -d '{\"a\": 2, \"b\": 3}' http://localhost:8000/add \\\\\\`\\\\\\`\\\\\\` # main.py - from fastapi import FastAPI from pydantic import BaseModel app = FastAPI() class Numbers(BaseModel): a: int b: int @app.post('/add') def add\\\\\\_numbers(numbers: Numbers): return {'result': numbers.a + numbers.b} @app.post('/subtract') def subtract\\\\\\_numbers(numbers: Numbers): return {'result': numbers.a - numbers.b} @app.post('/multiply') def multiply\\\\\\_numbers(numbers: Numbers): return {'result': numbers.a \\\\\\* numbers.b} @app.post('/divide') def divide\\\\\\_numbers(numbers: Numbers): if numbers.b == 0: return {'error': 'Cannot divide by zero'} return {'result': numbers.a / numbers.b} # requirements.txt - fastapi uvicorn pydantic Add Refactoring Capabilities¶ This second part of the example shows how OpenAI API can be used to update the multiples files previously created, based on new specifications. In order to do that, we'll rely on the standard unidiff format. This will be our definition for a change in our code base: from pydantic import Field from instructor import BaseModel class Diff(BaseModel): \"\"\" Changes that must be correctly made in a program's code repository defined as a complete diff (Unified Format) file which will be used to \\\\\\`patch\\\\\\` the repository. Example: --- /path/to/original timestamp +++ /path/to/new timestamp @@ -1,3 +1,9 @@ +This is an important +notice! It should +therefore be located at +the beginning of this +document! + This part of the document has stayed the same from version to @@ -8,13 +14,8 @@ compress the size of the changes. -This paragraph contains -text that is outdated. -It will be deleted in the -near future. - It is important to spell -check this dokument. On +check this document. On the other hand, a misspelled word isn't the end of the world. @@ -22,3 +23,7 @@ this paragraph needs to be changed. Things can be added after it. + +This paragraph contains +important new additions +to this document. \"\"\" diff: str = Field( ..., description=( \"Changes in a code repository correctly represented in 'diff' format, \" \"correctly escaped so it could be used in a JSON\" ), ) The diff class represents a diff file, with a set of changes that can be applied to our program using a tool like patch or Git. Calling Refactor Completions¶ We'll define a function that will pass the program and the new specifications to the OpenAI API: from generate import Program def refactor(new\\\\\\_requirements: str, program: Program) -> Diff: program\\\\\\_description = \"\\\\\\\\n\".join( \\\\\\[f\"{code.file\\\\\\_name}\\\\\\\\n\\\\\\[\\\\\\[\\\\\\[\\\\\\\\n{code.body}\\\\\\\\n\\\\\\]\\\\\\]\\\\\\]\\\\\\\\n\" for code in program.files\\\\\\] ) return client.chat.completions.create( # model=\"gpt-3.5-turbo-0613\", model=\"gpt-4\", temperature=0, response\\\\\\_model=Diff, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class programming AI capable of refactor \" \"existing python repositories. You will name files correct, include \" \"\\\\\\_\\\\\\_init\\\\\\_\\\\\\_.py files and write correct python code, with correct imports. \" \"You'll deliver your changes in valid 'diff' format so that they could \" \"be applied using the 'patch' command. \" \"Make sure you put the correct line numbers, \" \"and that all lines that must be changed are correctly marked.\", }, { \"role\": \"user\", \"content\": new\\\\\\_requirements, }, { \"role\": \"user\", \"content\": program\\\\\\_description, }, \\\\\\], max\\\\\\_tokens=1000, ) Notice we're using here the version gpt-4 of the model, which is more powerful but, also, more expensive. Creating an Example Refactoring¶ To tests these refactoring, we'll use the program object, generated in the first part of this example. changes = refactor( new\\\\\\_requirements=\"Refactor this code to use flask instead.\", program=program, ) print(changes.diff) The output will be this: --- readme.md +++ readme.md @@ -1,9 +1,9 @@ # FastAPI App -This is a FastAPI app that provides some basic math functions. +This is a Flask app that provides some basic math functions. ## Usage To use this app, follow the instructions below: 1. Install the required dependencies by running \\\\\\`pip install -r requirements.txt\\\\\\`. -2. Start the app by running \\\\\\`uvicorn main:app --reload\\\\\\`. +2. Start the app by running \\\\\\`flask run\\\\\\`. 3. Open your browser and navigate to \\\\\\`http://localhost:5000/docs\\\\\\` to access the Swagger UI documentation. ## Example To perform a basic math operation, you can use the following curl command: \\\\\\`\\\\\\`\\\\\\`bash -curl -X POST -H \"Content-Type: application/json\" -d '{\"operation\": \"add\", \"operands\": \\\\\\[2, 3\\\\\\]}' http://localhost:8000/calculate +curl -X POST -H \"Content-Type: application/json\" -d '{\"operation\": \"add\", \"operands\": \\\\\\[2, 3\\\\\\]}' http://localhost:5000/calculate --- main.py +++ main.py @@ -1,29 +1,29 @@ -from fastapi import FastAPI -from pydantic import BaseModel +from flask import Flask, request, jsonify -app = FastAPI() +app = Flask(name) -class Operation(BaseModel): operation: str operands: list +@app.route('/calculate', methods=\\\\\\['POST'\\\\\\]) +def calculate(): data = request.get\\\\\\_json() operation = data.get('operation') operands = data.get('operands') -@app.post('/calculate') -async def calculate(operation: Operation): if operation.operation == 'add': result = sum(operation.operands) elif operation.operation == 'subtract': result = operation.operands\\\\\\[0\\\\\\] - sum(operation.operands\\\\\\[1:\\\\\\]) elif operation.operation == 'multiply': if operation == 'add': result = sum(operands) elif operation == 'subtract': result = operands\\\\\\[0\\\\\\] - sum(operands\\\\\\[1:\\\\\\]) elif operation == 'multiply': result = 1 for operand in operation.operands: for operand in operands: result \\\\\\*= operand elif operation.operation == 'divide': result = operation.operands\\\\\\[0\\\\\\] for operand in operation.operands\\\\\\[1:\\\\\\]: elif operation == 'divide': result = operands\\\\\\[0\\\\\\] for operand in operands\\\\\\[1:\\\\\\]: result /= operand else: result = None return {'result': result} return jsonify({'result': result}) --- requirements.txt +++ requirements.txt @@ -1,3 +1,2 @@ -fastapi -uvicorn -pydantic +flask +flask-cors Was this page helpful? Back to top Previous Action Item and Dependency Mapping Next PII Data Sanitization Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Action Item and Dependency Mapping - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/action_items/",
    "html": "Skip to content Instructor Action Item and Dependency Mapping Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Structures Extracting Action Items Evaluation and Testing Visualizing the tasks Example: Extracting Action Items from Meeting Transcripts¶ In this guide, we'll walk through how to extract action items from meeting transcripts using OpenAI's API and Pydantic. This use case is essential for automating project management tasks, such as task assignment and priority setting. Motivation Significant amount of time is dedicated to meetings, where action items are generated as the actionable outcomes of these discussions. Automating the extraction of action items can save time and guarantee that no critical tasks are overlooked. Defining the Structures¶ We'll model a meeting transcript as a collection of Ticket objects, each representing an action item. Every Ticket can have multiple Subtask objects, representing smaller, manageable pieces of the main task. from enum import Enum from pydantic import BaseModel, Field from typing import List, Optional class PriorityEnum(str, Enum): high = \"High\" medium = \"Medium\" low = \"Low\" class Subtask(BaseModel): \"\"\"Correctly resolved subtask from the given transcript\"\"\" id: int name: str class Ticket(BaseModel): \"\"\"Correctly resolved ticket from the given transcript\"\"\" id: int name: str description: str priority: PriorityEnum assignees: List\\\\\\[str\\\\\\] subtasks: Optional\\\\\\[List\\\\\\[Subtask\\\\\\]\\\\\\] dependencies: Optional\\\\\\[List\\\\\\[int\\\\\\]\\\\\\] class ActionItems(BaseModel): \"\"\"Correctly resolved set of action items from the given transcript\"\"\" items: List\\\\\\[Ticket\\\\\\] Extracting Action Items¶ To extract action items from a meeting transcript, we use the generate function. It calls OpenAI's API, processes the text, and returns a set of action items modeled as ActionItems. import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def generate(data: str) -> ActionItems: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=ActionItems, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"The following is a transcript of a meeting...\", }, { \"role\": \"user\", \"content\": f\"Create the action items for the following transcript: {data}\", }, \\\\\\], ) # type: ignore Evaluation and Testing¶ To test the generate function, we provide it with a sample transcript, and then print the JSON representation of the extracted action items. prediction = generate( \"\"\" Alice: Hey team, we have several critical tasks we need to tackle for the upcoming release. First, we need to work on improving the authentication system. It's a top priority. Bob: Got it, Alice. I can take the lead on the authentication improvements. Are there any specific areas you want me to focus on? Alice: Good question, Bob. We need both a front-end revamp and back-end optimization. So basically, two sub-tasks. Carol: I can help with the front-end part of the authentication system. Bob: Great, Carol. I'll handle the back-end optimization then. Alice: Perfect. Now, after the authentication system is improved, we have to integrate it with our new billing system. That's a medium priority task. Carol: Is the new billing system already in place? Alice: No, it's actually another task. So it's a dependency for the integration task. Bob, can you also handle the billing system? Bob: Sure, but I'll need to complete the back-end optimization of the authentication system first, so it's dependent on that. Alice: Understood. Lastly, we also need to update our user documentation to reflect all these changes. It's a low-priority task but still important. Carol: I can take that on once the front-end changes for the authentication system are done. So, it would be dependent on that. Alice: Sounds like a plan. Let's get these tasks modeled out and get started.\"\"\" ) Visualizing the tasks¶ In order to quickly visualize the data we used code interpreter to create a graphviz export of the json version of the ActionItems array. { \"items\": \\\\\\[ { \"id\": 1, \"name\": \"Improve Authentication System\", \"description\": \"Revamp the front-end and optimize the back-end of the authentication system\", \"priority\": \"High\", \"assignees\": \\\\\\[\"Bob\", \"Carol\"\\\\\\], \"subtasks\": \\\\\\[ { \"id\": 2, \"name\": \"Front-end Revamp\" }, { \"id\": 3, \"name\": \"Back-end Optimization\" } \\\\\\], \"dependencies\": \\\\\\[\\\\\\] }, { \"id\": 4, \"name\": \"Integrate Authentication System with Billing System\", \"description\": \"Integrate the improved authentication system with the new billing system\", \"priority\": \"Medium\", \"assignees\": \\\\\\[\"Bob\"\\\\\\], \"subtasks\": \\\\\\[\\\\\\], \"dependencies\": \\\\\\[1\\\\\\] }, { \"id\": 5, \"name\": \"Update User Documentation\", \"description\": \"Update the user documentation to reflect the changes in the authentication system\", \"priority\": \"Low\", \"assignees\": \\\\\\[\"Carol\"\\\\\\], \"subtasks\": \\\\\\[\\\\\\], \"dependencies\": \\\\\\[2\\\\\\] } \\\\\\] } In this example, the generate function successfully identifies and segments the action items, assigning them priorities, assignees, subtasks, and dependencies as discussed in the meeting. By automating this process, you can ensure that important tasks and details are not lost in the sea of meeting minutes, making project management more efficient and effective. Was this page helpful? Back to top Previous Table Extraction Next Multi-File Code Generation Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Table Extraction - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/autodataframe/",
    "html": "Skip to content Instructor Table Extraction Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Data Structures Using the Prompt Pipeline Evaluating an Example Example: Converting Text into Dataframes¶ In this example, we'll demonstrate how to convert a text into dataframes using OpenAI Function Call. We will define the necessary data structures using Pydantic and show how to convert the text into dataframes. Motivation Often times when we parse data we have an opportunity to extract structured data, what if we could extract an arbitrary number of tables with arbitrary schemas? By pulling out dataframes we could write tables or .csv files and attach them to our retrieved data. Defining the Data Structures¶ Let's start by defining the data structures required for this task: RowData, Dataframe, and Database. from pydantic import Field, BaseModel from typing import List, Any class RowData(BaseModel): row: List\\\\\\[Any\\\\\\] = Field(..., description=\"The values for each row\") citation: str = Field( ..., description=\"The citation for this row from the original source data\" ) class Dataframe(BaseModel): \"\"\" Class representing a dataframe. This class is used to convert data into a frame that can be used by pandas. \"\"\" name: str = Field(..., description=\"The name of the dataframe\") data: List\\\\\\[RowData\\\\\\] = Field( ..., description=\"Correct rows of data aligned to column names, Nones are allowed\", ) columns: List\\\\\\[str\\\\\\] = Field( ..., description=\"Column names relevant from source data, should be in snake\\\\\\_case\", ) def to\\\\\\_pandas(self): import pandas as pd columns = self.columns + \\\\\\[\"citation\"\\\\\\] data = \\\\\\[row.row + \\\\\\[row.citation\\\\\\] for row in self.data\\\\\\] return pd.DataFrame(data=data, columns=columns) class Database(BaseModel): \"\"\" A set of correct named and defined tables as dataframes \"\"\" tables: List\\\\\\[Dataframe\\\\\\] = Field( ..., description=\"List of tables in the database\", ) The RowData class represents a single row of data in the dataframe. It contains a row attribute for the values in each row and a citation attribute for the citation from the original source data. The Dataframe class represents a dataframe and consists of a name attribute, a list of RowData objects in the data attribute, and a list of column names in the columns attribute. It also provides a to\\\\\\_pandas method to convert the dataframe into a Pandas DataFrame. The Database class represents a set of tables in a database. It contains a list of Dataframe objects in the tables attribute. Using the Prompt Pipeline¶ To convert a text into dataframes, we'll use the Prompt Pipeline in OpenAI Function Call. We can define a function dataframe that takes a text as input and returns a Database object. import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def dataframe(data: str) -> Database: return client.chat.completions.create( model=\"gpt-4-0613\", temperature=0.1, response\\\\\\_model=Database, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"\"\"Map this data into a dataframe a nd correctly define the correct columns and rows\"\"\", }, { \"role\": \"user\", \"content\": f\"{data}\", }, \\\\\\], max\\\\\\_tokens=1000, ) The dataframe function takes a string data as input and creates a completion using the Prompt Pipeline. It prompts the model to map the data into a dataframe and define the correct columns and rows. The resulting completion is then converted into a Database object. Evaluating an Example¶ Let's evaluate the example by converting a text into dataframes using the dataframe function and print the resulting dataframes. dfs = dataframe(\"\"\"My name is John and I am 25 years old. I live in New York and I like to play basketball. His name is Mike and he is 30 years old. He lives in San Francisco and he likes to play baseball. Sarah is 20 years old and she lives in Los Angeles. She likes to play tennis. Her name is Mary and she is 35 years old. She lives in Chicago. On one team 'Tigers' the captain is John and there are 12 players. On the other team 'Lions' the captain is Mike and there are 10 players. \"\"\") for df in dfs.tables: print(df.name) print(df.to\\\\\\_pandas()) The output will be: People Name Age City Favorite Sport 0 John 25 New York Basketball 1 Mike 30 San Francisco Baseball 2 Sarah 20 Los Angeles Tennis 3 Mary 35 Chicago None Teams Team Name Captain Number of Players 0 Tigers John 12 1 Lions Mike 10 Was this page helpful? Back to top Previous Recursive Schemas Next Action Item and Dependency Mapping Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Recursive Schemas - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/recursive/",
    "html": "Skip to content Instructor Recursive Schemas Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Structures Parsing the Tree Example Usage Example: Parsing a Directory Tree¶ In this example, we will demonstrate how define and use a recursive class definition to convert a string representing a directory tree into a filesystem structure using OpenAI's function call api. We will define the necessary structures using Pydantic, create a function to parse the tree, and provide an example of how to use it. Defining the Structures¶ We will use Pydantic to define the necessary data structures representing the directory tree and its nodes. We have two classes, Node and DirectoryTree, which are used to model individual nodes and the entire directory tree, respectively. Flat is better than nested While it's easier to model things as nested, returning flat items with dependencies tends to yield better results. For a flat example, check out planning tasks where we model a query plan as a dag. import enum from typing import List from pydantic import Field class NodeType(str, enum.Enum): \"\"\"Enumeration representing the types of nodes in a filesystem.\"\"\" FILE = \"file\" FOLDER = \"folder\" class Node(BaseModel): \"\"\" Class representing a single node in a filesystem. Can be either a file or a folder. Note that a file cannot have children, but a folder can. Args: name (str): The name of the node. children (List\\\\\\[Node\\\\\\]): The list of child nodes (if any). node\\\\\\_type (NodeType): The type of the node, either a file or a folder. Methods: print\\\\\\_paths: Prints the path of the node and its children. \"\"\" name: str = Field(..., description=\"Name of the folder\") children: List\\\\\\[\"Node\"\\\\\\] = Field( default\\\\\\_factory=list, description=\"List of children nodes, only applicable for folders, files cannot have children\", ) node\\\\\\_type: NodeType = Field( default=NodeType.FILE, description=\"Either a file or folder, use the name to determine which it could be\", ) def print\\\\\\_paths(self, parent\\\\\\_path=\"\"): \"\"\"Prints the path of the node and its children.\"\"\" if self.node\\\\\\_type == NodeType.FOLDER: path = f\"{parent\\\\\\_path}/{self.name}\" if parent\\\\\\_path != \"\" else self.name print(path, self.node\\\\\\_type) if self.children is not None: for child in self.children: child.print\\\\\\_paths(path) else: print(f\"{parent\\\\\\_path}/{self.name}\", self.node\\\\\\_type) class DirectoryTree(BaseModel): \"\"\" Container class representing a directory tree. Args: root (Node): The root node of the tree. Methods: print\\\\\\_paths: Prints the paths of the root node and its children. \"\"\" root: Node = Field(..., description=\"Root folder of the directory tree\") def print\\\\\\_paths(self): \"\"\"Prints the paths of the root node and its children.\"\"\" self.root.print\\\\\\_paths() Node.update\\\\\\_forward\\\\\\_refs() DirectoryTree.update\\\\\\_forward\\\\\\_refs() The Node class represents a single node in the directory tree. It has a name, a list of children nodes (applicable only to folders), and a node type (either a file or a folder). The print\\\\\\_paths method can be used to print the path of the node and its children. The DirectoryTree class represents the entire directory tree. It has a single attribute, root, which is the root node of the tree. The print\\\\\\_paths method can be used to print the paths of the root node and its children. Parsing the Tree¶ We define a function parse\\\\\\_tree\\\\\\_to\\\\\\_filesystem to convert a string representing a directory tree into a filesystem structure using OpenAI. import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def parse\\\\\\_tree\\\\\\_to\\\\\\_filesystem(data: str) -> DirectoryTree: \"\"\" Convert a string representing a directory tree into a filesystem structure using OpenAI's GPT-3 model. Args: data (str): The string to convert into a filesystem. Returns: DirectoryTree: The directory tree representing the filesystem. \"\"\" return client.chat.completions.create( model=\"gpt-3.5-turbo-0613\", response\\\\\\_model=DirectoryTree, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a perfect file system parsing algorithm. You are given a string representing a directory tree. You must return the correct filesystem structure.\", }, { \"role\": \"user\", \"content\": f\"Consider the data below:\\\\\\\\n{data} and return the correctly labeled filesystem\", }, \\\\\\], max\\\\\\_tokens=1000, ) The parse\\\\\\_tree\\\\\\_to\\\\\\_filesystem function takes a string data representing the directory tree and returns a DirectoryTree object representing the filesystem structure. It uses the OpenAI Chat API to complete the prompt and extract the directory tree. Example Usage¶ Let's demonstrate how to use the parse\\\\\\_tree\\\\\\_to\\\\\\_filesystem function with an example: root = parse\\\\\\_tree\\\\\\_to\\\\\\_filesystem( \"\"\" root ├── folder1 │ ├── file1.txt │ └── file2.txt └── folder2 ├── file3.txt └── subfolder1 └── file4.txt \"\"\" ) root.print\\\\\\_paths() In this example, we call parse\\\\\\_tree\\\\\\_to\\\\\\_filesystem with a string representing a directory tree. After parsing the string into a DirectoryTree object, we call root.print\\\\\\_paths() to print the paths of the root node and its children. The output of this example will be: root NodeType.FOLDER root/folder1 NodeType.FOLDER root/folder1/file1.txt NodeType.FILE root/folder1/file2.txt NodeType.FILE root/folder2 NodeType.FOLDER root/folder2/file3.txt NodeType.FILE root/folder2/subfolder1 NodeType.FOLDER root/folder2/subfolder1/file4.txt NodeType.FILE This demonstrates how to use OpenAI's GPT-3 model to parse a string representing a directory tree and obtain the correct filesystem structure. I hope this example helps you understand how to leverage OpenAI Function Call for parsing recursive trees. If you have any further questions, feel free to ask! Was this page helpful? Back to top Previous Query Decomposition Next Table Extraction Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Query Decomposition - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/planning-tasks/",
    "html": "Skip to content Instructor Query Decomposition Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Structures Planning a Query Plan Conclusion Example: Planning and Executing a Query Plan¶ This example demonstrates how to use the OpenAI Function Call ChatCompletion model to plan and execute a query plan in a question-answering system. By breaking down a complex question into smaller sub-questions with defined dependencies, the system can systematically gather the necessary information to answer the main question. Motivation The goal of this example is to showcase how query planning can be used to handle complex questions, facilitate iterative information gathering, automate workflows, and optimize processes. By leveraging the OpenAI Function Call model, you can design and execute a structured plan to find answers effectively. Use Cases: Complex question answering Iterative information gathering Workflow automation Process optimization With the OpenAI Function Call model, you can customize the planning process and integrate it into your specific application to meet your unique requirements. Defining the Structures¶ Let's define the necessary Pydantic models to represent the query plan and the queries. import enum from typing import List from pydantic import Field class QueryType(str, enum.Enum): \"\"\"Enumeration representing the types of queries that can be asked to a question answer system.\"\"\" SINGLE\\\\\\_QUESTION = \"SINGLE\" MERGE\\\\\\_MULTIPLE\\\\\\_RESPONSES = \"MERGE\\\\\\_MULTIPLE\\\\\\_RESPONSES\" class Query(BaseModel): \"\"\"Class representing a single question in a query plan.\"\"\" id: int = Field(..., description=\"Unique id of the query\") question: str = Field( ..., description=\"Question asked using a question answering system\", ) dependencies: List\\\\\\[int\\\\\\] = Field( default\\\\\\_factory=list, description=\"List of sub questions that need to be answered before asking this question\", ) node\\\\\\_type: QueryType = Field( default=QueryType.SINGLE\\\\\\_QUESTION, description=\"Type of question, either a single question or a multi-question merge\", ) class QueryPlan(BaseModel): \"\"\"Container class representing a tree of questions to ask a question answering system.\"\"\" query\\\\\\_graph: List\\\\\\[Query\\\\\\] = Field( ..., description=\"The query graph representing the plan\" ) def \\\\\\_dependencies(self, ids: List\\\\\\[int\\\\\\]) -> List\\\\\\[Query\\\\\\]: \"\"\"Returns the dependencies of a query given their ids.\"\"\" return \\\\\\[q for q in self.query\\\\\\_graph if q.id in ids\\\\\\] Graph Generation Notice that this example produces a flat list of items with dependencies that resemble a graph, while pydantic allows for recursive definitions, it's much easier and less confusing for the model to generate flat schemas rather than recursive schemas. If you want to see a recursive example, see recursive schemas Planning a Query Plan¶ Now, let's demonstrate how to plan and execute a query plan using the defined models and the OpenAI API. import asyncio import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def query\\\\\\_planner(question: str) -> QueryPlan: PLANNING\\\\\\_MODEL = \"gpt-4-0613\" messages = \\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class query planning algorithm capable ofbreaking apart questions into its dependency queries such that the answers can be used to inform the parent question. Do not answer the questions, simply provide a correct compute graph with good specific questions to ask and relevant dependencies. Before you call the function, think step-by-step to get a better understanding of the problem.\", }, { \"role\": \"user\", \"content\": f\"Consider: {question}\\\\\\\\nGenerate the correct query plan.\", }, \\\\\\] root = client.chat.completions.create( model=PLANNING\\\\\\_MODEL, temperature=0, response\\\\\\_model=QueryPlan, messages=messages, max\\\\\\_tokens=1000, ) return root plan = query\\\\\\_planner( \"What is the difference in populations of Canada and the Jason's home country?\" ) plan.model\\\\\\_dump() No RAG While we build the query plan in this example, we do not propose a method to actually answer the question. You can implement your own answer function that perhaps makes a retrival and calls openai for retrival augmented generation. That step would also make use of function calls but goes beyond the scope of this example. {'query\\\\\\_graph': \\\\\\[{'dependencies': \\\\\\[\\\\\\], 'id': 1, 'node\\\\\\_type': , 'question': \"Identify Jason's home country\"}, {'dependencies': \\\\\\[\\\\\\], 'id': 2, 'node\\\\\\_type': , 'question': 'Find the population of Canada'}, {'dependencies': \\\\\\[1\\\\\\], 'id': 3, 'node\\\\\\_type': , 'question': \"Find the population of Jason's home country\"}, {'dependencies': \\\\\\[2, 3\\\\\\], 'id': 4, 'node\\\\\\_type': , 'question': 'Calculate the difference in populations between Canada and Jason's home country\"}\\\\\\]} In the above code, we define a query\\\\\\_planner function that takes a question as input and generates a query plan using the OpenAI API. Conclusion¶ In this example, we demonstrated how to use the OpenAI Function Call ChatCompletion model to plan and execute a query plan using a question-answering system. We defined the necessary structures using Pydantic, created a query planner function. If you want to see multiple versions of this style of code, please visit: query planning example task planning with topo sort Feel free to modify the code to fit your specific use case and explore other possibilities of using the OpenAI Function Call model to plan and execute complex workflows. Was this page helpful? Back to top Previous Search Queries Next Recursive Schemas Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Entity Resolution - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/entity_resolution/",
    "html": "Skip to content Instructor Entity Resolution Type to start searching instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Data Structures Entity Extraction and Resolution Graph Visualization Execution Entity Resolution and Visualization for Legal Documents¶ In this guide, we demonstrate how to extract and resolve entities from a sample legal contract. Then, we visualize these entities and their dependencies as an entity graph. This approach can be invaluable for legal tech applications, aiding in the understanding of complex documents. Motivation Legal contracts are full of intricate details and interconnected clauses. Automatically extracting and visualizing these elements can make it easier to understand the document's overall structure and terms. Defining the Data Structures¶ The Entity and Property classes model extracted entities and their attributes. DocumentExtraction encapsulates a list of these entities. from pydantic import BaseModel, Field from typing import List class Property(BaseModel): key: str value: str resolved\\\\\\_absolute\\\\\\_value: str class Entity(BaseModel): id: int = Field( ..., description=\"Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities\", ) subquote\\\\\\_string: List\\\\\\[str\\\\\\] = Field( ..., description=\"Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution\", ) entity\\\\\\_title: str properties: List\\\\\\[Property\\\\\\] = Field( ..., description=\"List of properties of the entity\" ) dependencies: List\\\\\\[int\\\\\\] = Field( ..., description=\"List of entity ids that this entity depends or relies on to resolve it\", ) class DocumentExtraction(BaseModel): entities: List\\\\\\[Entity\\\\\\] = Field( ..., description=\"Body of the answer, each fact should be a separate object with a body and a list of sources\", ) Entity Extraction and Resolution¶ The ask\\\\\\_ai function utilizes OpenAI's API to extract and resolve entities from the input content. import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def ask\\\\\\_ai(content) -> DocumentExtraction: return client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=DocumentExtraction, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"Extract and resolve a list of entities from the following document:\", }, { \"role\": \"user\", \"content\": content, }, \\\\\\], ) # type: ignore Graph Visualization¶ generate\\\\\\_graph takes the extracted entities and visualizes them using Graphviz. It creates nodes for each entity and edges for their dependencies. from graphviz import Digraph def generate\\\\\\_html\\\\\\_label(entity: Entity) -> str: rows = \\\\\\[f\"{prop.key}{prop.resolved\\\\\\_absolute\\\\\\_value}\" for prop in entity.properties\\\\\\] table\\\\\\_rows = \"\".join(rows) return f\"<{table\\\\\\_rows} \\*\\*{entity.entity\\\\\\_title}\\*\\* \\\\>\" def generate\\\\\\_graph(data: DocumentExtraction): dot = Digraph(comment=\"Entity Graph\", node\\\\\\_attr={\"shape\": \"plaintext\"}) for entity in data.entities: label = generate\\\\\\_html\\\\\\_label(entity) dot.node(str(entity.id), label) for entity in data.entities: for dep\\\\\\_id in entity.dependencies: dot.edge(str(entity.id), str(dep\\\\\\_id)) dot.render(\"entity.gv\", view=True) Execution¶ Finally, execute the code to visualize the entity graph for the sample legal contract. content = \"\"\" Sample Legal Contract Agreement Contract This Agreement is made and entered into on 2020-01-01 by and between Company A (\"the Client\") and Company B (\"the Service Provider\"). Article 1: Scope of Work The Service Provider will deliver the software product to the Client 30 days after the agreement date. Article 2: Payment Terms The total payment for the service is $50,000. An initial payment of $10,000 will be made within 7 days of the the signed date. The final payment will be due 45 days after \\\\\\[SignDate\\\\\\]. Article 3: Confidentiality The parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date. Article 4: Termination The contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the \\\\\\[DeliveryDate\\\\\\]. \"\"\" # Your legal contract here model = ask\\\\\\_ai(content) generate\\\\\\_graph(model) This will produce a graphical representation of the entities and their dependencies, stored as \"entity.gv\". Was this page helpful? Back to top Previous Knowledge Graph Next Search Queries Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Search Queries - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/search/",
    "html": "Skip to content Instructor Search Queries Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Structures Calling Completions Evaluating an Example Example: Segmenting Search Queries¶ In this example, we will demonstrate how to leverage the MultiTask and enum.Enum features of OpenAI Function Call to segment search queries. We will define the necessary structures using Pydantic and demonstrate how segment queries into multiple sub queries and execute them in parallel with asyncio. Motivation Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use OpenAI Function Call to segment search queries and execute them in parallel. Defining the Structures¶ Let's model the problem as breaking down a search request into a list of search queries. We will use an enum to represent different types of searches and take advantage of Python objects to add additional query logic. import enum from pydantic import Field class SearchType(str, enum.Enum): \"\"\"Enumeration representing the types of searches that can be performed.\"\"\" VIDEO = \"video\" EMAIL = \"email\" class Search(BaseModel): \"\"\" Class representing a single search query. \"\"\" title: str = Field(..., description=\"Title of the request\") query: str = Field(..., description=\"Query to search for relevant content\") type: SearchType = Field(..., description=\"Type of search\") async def execute(self): print(f\"Searching for \\\\\\`{self.title}\\\\\\` with query \\\\\\`{self.query}\\\\\\` using \\\\\\`{self.type}\\\\\\`\") Notice that we have added the execute method to the Search class. This method can be used to route the search query based on the enum type. You can add logic specific to each search type in the execute method. Next, let's define a class to represent multiple search queries. from typing import List class MultiSearch(BaseModel): \"Correctly segmented set of search results\" tasks: List\\\\\\[Search\\\\\\] The MultiSearch class has a single attribute, tasks, which is a list of Search objects. This pattern is so common that we've added a helper function MultiTask to makes this simpler from instructor.dsl import MultiTask MultiSearch = MultiTask(Search) Calling Completions¶ To segment a search query, we will use the base openai api. We can define a function that takes a string and returns segmented search queries using the MultiSearch class. import instructor from openai import OpenAI # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def segment(data: str) -> MultiSearch: return client.chat.completions.create( model=\"gpt-3.5-turbo-0613\", temperature=0.1, functions=\\\\\\[MultiSearch.openai\\\\\\_schema\\\\\\], function\\\\\\_call={\"name\": MultiSearch.openai\\\\\\_schema\\\\\\[\"name\"\\\\\\]}, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Consider the data below: '\\\\\\\\n{data}' and segment it into multiple search queries\", }, \\\\\\], max\\\\\\_tokens=1000, ) The segment function takes a string data and creates a completion. It prompts the model to segment the data into multiple search queries and returns the result as a MultiSearch object. Evaluating an Example¶ Let's evaluate an example by segmenting a search query and executing the segmented queries. import asyncio queries = segment(\"Please send me the video from last week about the investment case study and also documents about your GDPR policy?\") async def execute\\\\\\_queries(queries: Multisearch): await asyncio.gather(\\\\\\*\\\\\\[q.execute() for q in queries.tasks\\\\\\]) loop = asyncio.get\\\\\\_event\\\\\\_loop() loop.run\\\\\\_until\\\\\\_complete(execute\\\\\\_queries()) loop.close() In this example, we use the segment function to segment the search query. We then use asyncio to asynchronously execute the queries using the execute method defined in the Search class. The output will be: Searching for \\\\\\`Please send me the video from last week about the investment case study\\\\\\` with query \\\\\\`Please send me the video from last week about the investment case study\\\\\\` using \\\\\\`SearchType.VIDEO\\\\\\` Searching for \\\\\\`also documents about your GDPR policy?\\\\\\` with query \\\\\\`also documents about your GDPR policy?\\\\\\` using \\\\\\`SearchType.EMAIL\\\\\\` Was this page helpful? Back to top Previous Entity Resolution Next Query Decomposition Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Knowledge Graph - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/knowledge_graph/",
    "html": "Skip to content Instructor Knowledge Graph Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Defining the Structures Generating Knowledge Graphs Visualizing the Graph Putting It All Together Visualizing Knowledge Graphs for Complex Topics¶ In this guide, you'll discover how to visualize a detailed knowledge graph for understanding complex topics, in this case, quantum mechanics. We leverage OpenAI's API and the Graphviz library to bring structure to intricate subjects. Motivation Knowledge graphs offer a visually appealing and coherent way to understand complicated topics like quantum mechanics. By generating these graphs automatically, you can accelerate the learning process and make it easier to digest complex information. Defining the Structures¶ Let's model a knowledge graph with Node and Edge objects. Node objects represent key concepts or entities, while Edge objects indicate the relationships between them. from pydantic import BaseModel, Field from typing import List class Node(BaseModel): id: int label: str color: str class Edge(BaseModel): source: int target: int label: str color: str = \"black\" class KnowledgeGraph(BaseModel): nodes: List\\\\\\[Node\\\\\\] = Field(..., default\\\\\\_factory=list) edges: List\\\\\\[Edge\\\\\\] = Field(..., default\\\\\\_factory=list) Generating Knowledge Graphs¶ The generate\\\\\\_graph function leverages OpenAI's API to generate a knowledge graph based on the input query. from openai import OpenAI import instructor # Adds response\\\\\\_model to ChatCompletion # Allows the return of Pydantic model rather than raw JSON client = instructor.patch(OpenAI()) def generate\\\\\\_graph(input) -> KnowledgeGraph: return client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Help me understand the following by describing it as a detailed knowledge graph: {input}\", } \\\\\\], response\\\\\\_model=KnowledgeGraph, ) # type: ignore Visualizing the Graph¶ The visualize\\\\\\_knowledge\\\\\\_graph function uses the Graphviz library to render the generated knowledge graph. from graphviz import Digraph def visualize\\\\\\_knowledge\\\\\\_graph(kg: KnowledgeGraph): dot = Digraph(comment=\"Knowledge Graph\") # Add nodes for node in kg.nodes: dot.node(str(node.id), node.label, color=node.color) # Add edges for edge in kg.edges: dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color) # Render the graph dot.render(\"knowledge\\\\\\_graph.gv\", view=True) Putting It All Together¶ Execute the code to generate and visualize a knowledge graph for understanding quantum mechanics. graph: KnowledgeGraph = generate\\\\\\_graph(\"Teach me about quantum mechanics\") visualize\\\\\\_knowledge\\\\\\_graph(graph) This will produce a visual representation of the knowledge graph, stored as \"knowledge\\\\\\_graph.gv\". You can open this file to explore the key concepts and their relationships in quantum mechanics. By leveraging automated knowledge graphs, you can dissect complex topics into digestible pieces, making the learning journey less daunting and more effective. Was this page helpful? Back to top Previous Citations Next Entity Resolution Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Image to Ad Copy - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/image_to_ad_copy/",
    "html": "Skip to content Instructor Image to Ad Copy Type to start searching instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Building the models Product Identified Product Advertising Copy Calling the API Product Detection Generate advertising copy Putting it all together Input file Use Vision API to detect products and generate advertising copy¶ This post demonstrates how to use GPT-4 Vision API and the Chat API to automatically generate advertising copy from product images. This method can be useful for marketing and advertising teams, as well as for e-commerce platforms. The full code is available on GitHub. Building the models¶ Product¶ For the Product model, we define a class that represents a product extracted from an image and store the name, key features, and description. The product attributes are dynamically determined based on the content of the image. Note that it is easy to add Validators and other Pydantic features to the model to ensure that the data is valid and consistent. class Product(BaseModel): \"\"\" Represents a product extracted from an image using AI. The product attributes are dynamically determined based on the content of the image and the AI's interpretation. This class serves as a structured representation of the identified product characteristics. \"\"\" name: str = Field( description=\"A generic name for the product.\", example=\"Headphones\" ) key\\\\\\_features: Optional\\\\\\[List\\\\\\[str\\\\\\]\\\\\\] = Field( description=\"A list of key features of the product that stand out.\", default=None, ) description: Optional\\\\\\[str\\\\\\] = Field( description=\"A description of the product.\", default=None, ) # Can be customized and automatically generated def generate\\\\\\_prompt(self): prompt = f\"Product: {self.name}\\\\\\\\n\" if self.description: prompt += f\"Description: {self.description}\\\\\\\\n\" if self.key\\\\\\_features: prompt += f\"Key Features: {', '.join(self.key\\\\\\_features)}\\\\\\\\n\" return prompt Identified Product¶ We also define a class that represents a list of products identified in the images. We also add an error flag and message to indicate if there was an error in the processing of the image. class IdentifiedProduct(BaseModel): \"\"\" Represents a list of products identified in the images. \"\"\" products: Optional\\\\\\[List\\\\\\[Product\\\\\\]\\\\\\] = Field( description=\"A list of products identified by the AI.\", example=\\\\\\[ Product( name=\"Headphones\", description=\"Wireless headphones with noise cancellation.\", key\\\\\\_features=\\\\\\[\"Wireless\", \"Noise Cancellation\"\\\\\\], ) \\\\\\], default=None, ) error: bool = Field(default=False) message: Optional\\\\\\[str\\\\\\] = Field(default=None) Advertising Copy¶ Finally, the AdCopy models stores the output in a structured format with a headline and the text. class AdCopy(BaseModel): \"\"\" Represents a generated ad copy. \"\"\" headline: str = Field( description=\"A short, catchy, and memorable headline for the given product. The headline should invoke curiosity and interest in the product.\", ) ad\\\\\\_copy: str = Field( description=\"A long-form advertisement copy for the given product. This will be used in campaigns to promote the product with a persuasive message and a call-to-action with the objective of driving sales.\", ) name: str = Field( description=\"The name of the product being advertised.\" ) Calling the API¶ Product Detection¶ The read\\\\\\_images function uses OpenAI's vision model to process a list of image URLs and identify products in each of them. We utilize the instructor library to patch the OpenAI client for this purpose. def read\\\\\\_images(image\\\\\\_urls: List\\\\\\[str\\\\\\]) -> IdentifiedProduct: \"\"\" Given a list of image URLs, identify the products in the images. \"\"\" logger.info(f\"Identifying products in images... {len(image\\\\\\_urls)} images\") return client\\\\\\_image.chat.completions.create( model=\"gpt-4-vision-preview\", response\\\\\\_model=IdentifiedProduct, max\\\\\\_tokens=1024, # can be changed temperature=0, messages=\\\\\\[ { \"role\": \"user\", \"content\": \\\\\\[ { \"type\": \"text\", \"text\": \"Identify products using the given images and generate key features for each product.\", }, \\\\\\*\\\\\\[ {\"type\": \"image\\\\\\_url\", \"image\\\\\\_url\": {\"url\": url}} for url in image\\\\\\_urls \\\\\\], \\\\\\], } \\\\\\], ) This gives us a list of products identified in all the images. Generate advertising copy¶ Then, we can use the generate\\\\\\_ad\\\\\\_copy function to generate advertising copy for each of the products identified in the images. Two clients are defined for the two different models. This is because the gpt-4-vision-preview model is not compatible with the gpt-4-1106-preview model in terms of their response format. def generate\\\\\\_ad\\\\\\_copy(product: Product) -> AdCopy: \"\"\" Given a product, generate an ad copy for the product. \"\"\" logger.info(f\"Generating ad copy for product: {product.name}\") return client\\\\\\_copy.chat.completions.create( model=\"gpt-4-1106-preview\", response\\\\\\_model=AdCopy, temperature=0.3, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are an expert marketing assistant for all products. Your task is to generate an advertisement copy for a product using the name, description, and key features.\", }, {\"role\": \"user\", \"content\": product.generate\\\\\\_prompt()}, \\\\\\], ) Putting it all together¶ Finally, we can put it all together in a single function that takes a list of image URLs and generates advertising copy for the products identified in the images. Please refer to the full code for the complete implementation. Input file¶ The input file is currently a list of image URLs, but this trivial to change to any required format. https://contents.mediadecathlon.com/p1279823/9a1c59ad97a4084a346c014740ae4d3ff860ea70b485ee65f34017ff5e9ae5f7/recreational-ice-skates-fit-50-black.jpg?format=auto https://contents.mediadecathlon.com/p1279822/a730505231dbd6747c14ee93e8f89e824d3fa2a5b885ec26de8d7feb5626638a/recreational-ice-skates-fit-50-black.jpg?format=auto https://contents.mediadecathlon.com/p2329893/1ed75517602a5e00245b89ab6a1c6be6d8968a5a227c932b10599f857f3ed4cd/mens-hiking-leather-boots-sh-100-x-warm.jpg?format=auto https://contents.mediadecathlon.com/p2047870/8712c55568dd9928c83b19c6a4067bf161811a469433dc89244f0ff96a50e3e9/men-s-winter-hiking-boots-sh-100-x-warm-grey.jpg?format=auto Expand to see the output Was this page helpful? Back to top Previous Image Extracting Tables Next Moderation Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Citations - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/exact_citations/",
    "html": "Skip to content Instructor Citations Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Overview Data Structures The Fact Class Validation Method: validate\\\\\\_sources The QuestionAnswer Class Validation Method: validate\\\\\\_sources Function to Ask AI a Question The ask\\\\\\_ai Function Example Example: Answering Questions with Validated Citations¶ For the full code example check out examples/citation\\\\\\_fuzzy\\\\\\_match.py Overview¶ This example shows how to use Instructor with validators to not only add citations to answers generated but also prevent hallucinations by ensuring that every statement made by the LLM is backed up by a direct quote from the context provided, and that those quotes exist!.Two Python classes, Fact and QuestionAnswer, are defined to encapsulate the information of individual facts and the entire answer, respectively. Data Structures¶ The Fact Class¶ The Fact class encapsulates a single statement or fact. It contains two fields: fact: A string representing the body of the fact or statement. substring\\\\\\_quote: A list of strings. Each string is a direct quote from the context that supports the fact. Validation Method: validate\\\\\\_sources¶ This method validates the sources (substring\\\\\\_quote) in the context. It utilizes regex to find the span of each substring quote in the given context. If the span is not found, the quote is removed from the list. from pydantic import Field, BaseModel, model\\\\\\_validator, FieldValidationInfo from typing import List class Fact(BaseModel): fact: str = Field(...) substring\\\\\\_quote: List\\\\\\[str\\\\\\] = Field(...) @model\\\\\\_validator(mode=\"after\") def validate\\\\\\_sources(self, info: FieldValidationInfo) -> \"Fact\": text\\\\\\_chunks = info.context.get(\"text\\\\\\_chunk\", None) spans = list(self.get\\\\\\_spans(text\\\\\\_chunks)) self.substring\\\\\\_quote = \\\\\\[text\\\\\\_chunks\\\\\\[span\\\\\\[0\\\\\\] : span\\\\\\[1\\\\\\]\\\\\\] for span in spans\\\\\\] return self def get\\\\\\_spans(self, context): for quote in self.substring\\\\\\_quote: yield from self.\\\\\\_get\\\\\\_span(quote, context) def \\\\\\_get\\\\\\_span(self, quote, context): for match in re.finditer(re.escape(quote), context): yield match.span() The QuestionAnswer Class¶ This class encapsulates the question and its corresponding answer. It contains two fields: question: The question asked. answer: A list of Fact objects that make up the answer. Validation Method: validate\\\\\\_sources¶ This method checks that each Fact object in the answer list has at least one valid source. If a Fact object has no valid sources, it is removed from the answer list. class QuestionAnswer(BaseModel): question: str = Field(...) answer: List\\\\\\[Fact\\\\\\] = Field(...) @model\\\\\\_validator(mode=\"after\") def validate\\\\\\_sources(self) -> \"QuestionAnswer\": self.answer = \\\\\\[fact for fact in self.answer if len(fact.substring\\\\\\_quote) > 0\\\\\\] return self Function to Ask AI a Question¶ The ask\\\\\\_ai Function¶ This function takes a string question and a string context and returns a QuestionAnswer object. It uses the OpenAI API to fetch the answer and then validates the sources using the defined classes. To understand the validation context work from pydantic check out pydantic's docs from openai import OpenAI import instructor # Apply the patch to the OpenAI client # enables response\\\\\\_model, validation\\\\\\_context keyword client = instructor.patch(OpenAI()) def ask\\\\\\_ai(question: str, context: str) -> QuestionAnswer: return client.chat.completions.create( model=\"gpt-3.5-turbo-0613\", temperature=0, response\\\\\\_model=QuestionAnswer, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"You are a world class algorithm to answer questions with correct and exact citations.\"}, {\"role\": \"user\", \"content\": f\"{context}\"}, {\"role\": \"user\", \"content\": f\"Question: {question}\"} \\\\\\], validation\\\\\\_context={\"text\\\\\\_chunk\": context}, ) Example¶ dd Here's an example of using these classes and functions to ask a question and validate the answer. question = \"What did the author do during college?\" context = \"\"\" My name is Jason Liu, and I grew up in Toronto Canada but I was born in China. I went to an arts high school but in university I studied Computational Mathematics and physics. As part of coop I worked at many companies including Stitchfix, Facebook. I also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years. \"\"\" The output would be a QuestionAnswer object containing validated facts and their sources. { \"question\": \"where did he go to school?\", \"answer\": \\\\\\[ { \"statement\": \"Jason Liu went to an arts highschool.\", \"substring\\\\\\_phrase\": \\\\\\[ \"arts highschool\" \\\\\\] }, { \"statement\": \"Jason Liu studied Computational Mathematics and physics in university.\", \"substring\\\\\\_phrase\": \\\\\\[ \"university\" \\\\\\] } \\\\\\] } This ensures that every piece of information in the answer has been validated against the context. Was this page helpful? Back to top Previous Moderation Next Knowledge Graph Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Moderation - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/moderation/",
    "html": "Skip to content Instructor Moderation Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Overview Incorporating OpenAI moderation validator Testing OpenAI moderation validator OpenAI Moderation¶ Overview¶ This example uses OpenAI's moderation endpoint to check content compliance with OpenAI's usage policies. It can identify and filter harmful content that violates the policies. The model flags content and classifies it into categories including hate, harassment, self-harm, sexual content, and violence. Each category has subcategories for detailed classification. This validator is to be used for monitoring OpenAI API inputs and outputs, other use cases are currently not allowed. Incorporating OpenAI moderation validator¶ The following code defines a function to validate content using OpenAI's Moderation endpoint. The AfterValidator is used to apply OpenAI's moderation after the compute. This moderation checks if the content complies with OpenAI's usage policies and flags any harmful content. Here's how it works: Generate the OpenAI client and patch it with the instructor. Patching is not strictly necessary for this example but its a good idea to always patch the client to leverage the full instructor functionality. Annotate our message field with AfterValidator(openai\\\\\\_moderation(client=client)). This means that after the message is computed, it will be passed to the openai\\\\\\_moderation function for validation. import instructor from instructor import openai\\\\\\_moderation from typing\\\\\\_extensions import Annotated from pydantic import BaseModel, AfterValidator, ValidationError from openai import OpenAI client = instructor.patch(OpenAI()) class Response(BaseModel): message: Annotated\\\\\\[str, AfterValidator(openai\\\\\\_moderation(client=client))\\\\\\] Testing OpenAI moderation validator¶ Now, let's test our class with a piece of content that violates OpenAI's usage policies. Response(message=\"I want to make them suffer the consequences\") The validator will raise a ValidationError if the content violates the policies, like so: ValidationError: 1 validation error for Response message Value error, \\\\\\`I want to make them suffer the consequences\\\\\\` was flagged for harassment, harassment\\\\\\_threatening, violence, harassment/threatening \\\\\\[type=value\\\\\\_error, input\\\\\\_value='I want to make them suffer the consequences', input\\\\\\_type=str\\\\\\] Let's try another example which violates a different policy: self-harm. Response(message=\"I want to hurt myself.\") In this case, our validator will flag the output but return a different error message in the trace, clarifying the specific policies that were violated: ValidationError: 1 validation error for Response message Value error, I want to hurt myself was flagged for self\\\\\\_harm, self\\\\\\_harm\\\\\\_intent, violence, self-harm, self-harm/intent \\\\\\[type=value\\\\\\_error, input\\\\\\_value='I want to hurt myself', input\\\\\\_type=str\\\\\\] Was this page helpful? Back to top Previous Image to Ad Copy Next Citations Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Image Extracting Tables - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/extracting_tables/",
    "html": "Skip to content Instructor Image Extracting Tables Type to start searching instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Building the Custom Type for Markdown Tables Defining the Table Class Extracting Tables from Images Practical Example Top 10 Grossing Apps in October 2023 (Ireland) for Android Platforms Top 10 Grossing Apps in October 2023 (Ireland) for iOS Platforms Extracting Tables From Images¶ This post demonstrates how to use Python's type annotations and OpenAI's new vision model to extract tables from images and convert them into markdown format. This method is particularly useful for data analysis and automation tasks. The full code is available on GitHub Building the Custom Type for Markdown Tables¶ First, we define a custom type, MarkdownDataFrame, to handle pandas DataFrames formatted in markdown. This type uses Python's Annotated and InstanceOf types, along with decorators BeforeValidator and PlainSerializer, to process and serialize the data. from io import StringIO from typing import Annotated, Any from pydantic import BaseModel, Field, BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema import pandas as pd def md\\\\\\_to\\\\\\_df(data: Any) -> Any: # Convert markdown to DataFrame if isinstance(data, str): return ( pd.read\\\\\\_csv( StringIO(data), # Process data sep=\"|\", index\\\\\\_col=1, ) .dropna(axis=1, how=\"all\") .iloc\\\\\\[1:\\\\\\] .applymap(lambda x: x.strip()) ) return data MarkdownDataFrame = Annotated\\\\\\[ InstanceOf(pd.DataFrame), BeforeValidator(md\\\\\\_to\\\\\\_df), PlainSerializer(lambda df: df.to\\\\\\_markdown()), WithJsonSchema( { \"type\": \"string\", \"description\": \"The markdown representation of the table, each one should be tidy, do not try to join tables that should be seperate\", } ) \\\\\\] Defining the Table Class¶ The Table class is essential for organizing the extracted data. It includes a caption and a dataframe, processed as a markdown table. Since most of the complexity is handled by the MarkdownDataFrame type, the Table class is straightforward! class Table(BaseModel): caption: str dataframe: MarkdownDataFrame Extracting Tables from Images¶ The extract\\\\\\_table function uses OpenAI's vision model to process an image URL and extract tables in markdown format. We utilize the instructor library to patch the OpenAI client for this purpose. import instructor from openai import OpenAI # Apply the patch to the OpenAI client to support response\\\\\\_model # Also use MD\\\\\\_JSON mode since the visino model does not support any special structured output mode client = instructor.patch(OpenAI(), mode=instructor.function\\\\\\_calls.Mode.MD\\\\\\_JSON) def extract\\\\\\_table(url: str) -> Iterable\\\\\\[Table\\\\\\]: return client.chat.completions.create( model=\"gpt-4-vision-preview\", response\\\\\\_model=Iterable\\\\\\[Table\\\\\\], max\\\\\\_tokens=1800, messages=\\\\\\[ { \"role\": \"user\", \"content\": \\\\\\[ {\"type\": \"text\", \"text\": \"Extract table from image.\"}, {\"type\": \"image\\\\\\_url\", \"image\\\\\\_url\": {\"url\": url}} \\\\\\], } \\\\\\], ) Practical Example¶ In this example, we apply the method to extract data from an image showing the top grossing apps in Ireland for October 2023. url = \"https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031\\\\\\_uk-ireland-in-three-charts\\\\\\_table\\\\\\_v01\\\\\\_b.png\" tables = extract\\\\\\_table(url) for table in tables: print(table.caption, end=\"\\\\\\\\n\") print(table.dataframe) Expand to see the output Was this page helpful? Back to top Previous Self Critique Next Image to Ad Copy Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Self Critique - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/self_critique/",
    "html": "Skip to content Instructor Self Critique Type to start searching instructor Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Introduction Setup Defining Models Generating a Response Output Before Validation Adding Custom Validation Output After Validation Retrying with Corrections Final Output Self-Correction with llm\\\\\\_validator¶ Introduction¶ This guide demonstrates how to use llm\\\\\\_validator for implementing self-healing. The objective is to showcase how an instructor can self-correct by using validation errors and helpful error messages. Setup¶ Import required modules and apply compatibility patches. from typing\\\\\\_extensions import Annotated from pydantic import BaseModel, BeforeValidator Defining Models¶ Before building validation logic, define a basic Pydantic model named QuestionAnswer. We'll use this model to generate a response without validation to see the output. class QuestionAnswer(BaseModel): question: str answer: str Generating a Response¶ Here we coerce the model to generate a response that is objectionable. from openai import OpenAI import instructor # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) question = \"What is the meaning of life?\" context = \"The according to the devil the meaning of live is to live a life of sin and debauchery.\" qa: QuestionAnswer = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=QuestionAnswer, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: {context}\\\\\\\\n\\\\\\\\nAnswer the following question: {question}\", }, \\\\\\], ) Output Before Validation¶ While it calls out the objectionable content, it doesn't provide any details on how to correct it. { \"question\": \"What is the meaning of life?\", \"answer\": \"The meaning of life, according to the context, is to live a life of sin and debauchery.\" } Adding Custom Validation¶ By adding a validator to the answer field, we can try to catch the issue and correct it. Lets integrate llm\\\\\\_validator into the model and see the error message. Its important to note that you can use all of pydantic's validators as you would normally as long as you raise a ValidationError with a helpful error message as it will be used as part of the self correction prompt. class QuestionAnswerNoEvil(BaseModel): question: str answer: Annotated\\\\\\[ str, BeforeValidator( llm\\\\\\_validator(\"don't say objectionable things\", allow\\\\\\_override=True) ), \\\\\\] try: qa: QuestionAnswerNoEvil = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=QuestionAnswerNoEvil, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: {context}\\\\\\\\n\\\\\\\\nAnswer the following question: {question}\", }, \\\\\\], ) except Exception as e: print(e) Output After Validation¶ Now, we throw validation error that its objectionable and provide a helpful error message. 1 validation error for QuestionAnswerNoEvil answer Assertion failed, The statement promotes sin and debauchery, which is objectionable. Retrying with Corrections¶ By adding the max\\\\\\_retries parameter, we can retry the request with corrections. and use the error message to correct the output. qa: QuestionAnswerNoEvil = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=QuestionAnswerNoEvil, max\\\\\\_retries=1, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\", }, { \"role\": \"user\", \"content\": f\"using the context: {context}\\\\\\\\n\\\\\\\\nAnswer the following question: {question}\", }, \\\\\\], ) Final Output¶ Now, we get a valid response that is not objectionable! { \"question\": \"What is the meaning of life?\", \"answer\": \"The meaning of life is subjective and can vary depending on individual beliefs and philosophies.\" } Was this page helpful? Back to top Previous Text Classification Next Image Extracting Tables Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Text Classification - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/classification/",
    "html": "Skip to content Instructor Text Classification Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Single-Label Classification Defining the Structures Classifying Text Testing and Evaluation Multi-Label Classification Defining the Structures Classifying Text Testing and Evaluation Example: Text Classification using OpenAI and Pydantic¶ This tutorial showcases how to implement text classification tasks—specifically, single-label and multi-label classifications—using the OpenAI API, Python's enum module, and Pydantic models. Motivation Text classification is a common problem in many NLP applications, such as spam detection or support ticket categorization. The goal is to provide a systematic way to handle these cases using OpenAI's GPT models in combination with Python data structures. Single-Label Classification¶ Defining the Structures¶ For single-label classification, we first define an enum for possible labels and a Pydantic model for the output. import enum from pydantic import BaseModel class Labels(str, enum.Enum): \"\"\"Enumeration for single-label text classification.\"\"\" SPAM = \"spam\" NOT\\\\\\_SPAM = \"not\\\\\\_spam\" class SinglePrediction(BaseModel): \"\"\" Class for a single class label prediction. \"\"\" class\\\\\\_label: Labels Classifying Text¶ The function classify will perform the single-label classification. from openai import OpenAI import instructor # Apply the patch to the OpenAI client # enables response\\\\\\_model keyword client = instructor.patch(OpenAI()) def classify(data: str) -> SinglePrediction: \"\"\"Perform single-label classification on the input text.\"\"\" return client.chat.completions.create( model=\"gpt-3.5-turbo-0613\", response\\\\\\_model=SinglePrediction, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Classify the following text: {data}\", }, \\\\\\], ) # type: ignore Testing and Evaluation¶ Let's run an example to see if it correctly identifies a spam message. # Test single-label classification prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\") assert prediction.class\\\\\\_label == Labels.SPAM Multi-Label Classification¶ Defining the Structures¶ For multi-label classification, we introduce a new enum class and a different Pydantic model to handle multiple labels. # Define Enum class for multiple labels class MultiLabels(str, enum.Enum): TECH\\\\\\_ISSUE = \"tech\\\\\\_issue\" BILLING = \"billing\" GENERAL\\\\\\_QUERY = \"general\\\\\\_query\" # Define the multi-class prediction model class MultiClassPrediction(BaseModel): \"\"\" Class for a multi-class label prediction. \"\"\" class\\\\\\_labels: List\\\\\\[MultiLabels\\\\\\] Classifying Text¶ The function multi\\\\\\_classify is responsible for multi-label classification. def multi\\\\\\_classify(data: str) -> MultiClassPrediction: \"\"\"Perform multi-label classification on the input text.\"\"\" return client.chat.completions.create( model=\"gpt-3.5-turbo-0613\", response\\\\\\_model=MultiClassPrediction, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Classify the following support ticket: {data}\", }, \\\\\\], ) # type: ignore Testing and Evaluation¶ Finally, we test the multi-label classification function using a sample support ticket. # Test multi-label classification ticket = \"My account is locked and I can't access my billing info.\" prediction = multi\\\\\\_classify(ticket) assert MultiLabels.TECH\\\\\\_ISSUE in prediction.class\\\\\\_labels assert MultiLabels.BILLING in prediction.class\\\\\\_labels Was this page helpful? Back to top Previous Overview Next Self Critique Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Type Adapter - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/typeadapter/",
    "html": "Instructor Type Adapter Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Type Adapter This page is a work in progress This page is a work in progress. Check out Pydantic's documentation Was this page helpful? Back to top Previous Alias Next Overview Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Alias - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/alias/",
    "html": "Instructor Alias Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Alias This page is a work in progress This page is a work in progress. Check out Pydantic's documentation Was this page helpful? Back to top Previous Union Next Type Adapter Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Union - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/union/",
    "html": "Skip to content Instructor Union Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Unions for Multiple Types Union Pydantic models also support Union types, which are used to represent a value that can be one of several types. While many libraries support multiple function calls, and tool calls support multiple returns, the goal is to provide only one way to do things. Unions for Multiple Types¶ You can use Union types to write agents that can dynamically choose actions - by choosing an output class. For example, in a search and lookup function, the LLM can determine whether to execute another search, lookup or other action. class Search(BaseModel): query: str def execute(self): return ... class Lookup(BaseModel): key: str def execute(self): return ... class Action(BaseModel): action: Union\\\\\\[Search, Lookup\\\\\\] def execute(self): return self.action.execute() See 'examples/union/run.py' for a working example. Was this page helpful? Back to top Previous Types Next Alias Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Types - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/types/",
    "html": "Instructor Types Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Types This page is a work in progress This page is a work in progress. Check out Pydantic's documentation Was this page helpful? Back to top Previous Distillation Next Union Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Distillation - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/distillation/",
    "html": "Skip to content Instructor Distillation Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents The Challenges in Function-Level Fine-Tuning The Role of Instructions in Simplifying the Fine-Tuning Process How to Implement Instructions in Your Code Quick Start: How to Use Instructor's Distillation Feature The Intricacies of Fine-tuning Language Models Why Instructor and Distillation are Game Changers Role of Instructor in Simplifying Fine-Tuning Logging Output and Running a Finetune Distilling python functions into LLM¶ Instructions from the Instructor library offers a seamless way to make language models backward compatible with existing Python functions. By employing Pydantic type hints, it not only ensures compatibility but also facilitates fine-tuning gpt-3.5-turbo to emulate these functions end-to-end. If you want to see the full example checkout examples/distillation The Challenges in Function-Level Fine-Tuning¶ Replicating the behavior of a Python function in a language model involves intricate data preparation. For instance, teaching a model to execute three-digit multiplication is not as trivial as implementing def f(a, b): return a \\\\\\* b. OpenAI's fine-tuning script coupled with their function calling utility provides a structured output, thereby simplifying the data collection process. Additionally, this eliminates the need for passing the schema to the model, thus conserving tokens. The Role of Instructions in Simplifying the Fine-Tuning Process¶ By using Instructions, you can annotate a Python function that returns a Pydantic object, thereby automating the dataset creation for fine-tuning. A handler for logging is all that's needed to build this dataset. How to Implement Instructions in Your Code¶ Quick Start: How to Use Instructor's Distillation Feature¶ Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file. import logging import random from pydantic import BaseModel from instructor import Instructions # pip install instructor # Logging setup logging.basicConfig(level=logging.INFO) instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"math\\\\\\_finetunes.jsonl\")\\\\\\] ) class Multiply(BaseModel): a: int b: int result: int # Define a function with distillation # The decorator will automatically generate a dataset for fine-tuning # They must return a pydantic model to leverage function calling @instructions.distil def fn(a: int, b: int) -> Multiply: resp = a \\\\\\* b return Multiply(a=a, b=b, result=resp) # Generate some data for \\\\\\_ in range(10): a = random.randint(100, 999) b = random.randint(100, 999) print(fn(a, b)) The Intricacies of Fine-tuning Language Models¶ Fine-tuning isn't just about writing a function like def f(a, b): return a \\\\\\* b. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this. Why Instructor and Distillation are Game Changers¶ The library offers two main benefits: Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code. Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions. Role of Instructor in Simplifying Fine-Tuning¶ The from instructor import Instructions feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior. Logging Output and Running a Finetune¶ Here's how the logging output would look: { \"messages\": \\\\\\[ {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'}, {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'}, {\"role\": \"assistant\", \"function\\\\\\_call\": { \"name\": \"Multiply\", \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}' } } \\\\\\], \"functions\": \\\\\\[ {\"name\": \"Multiply\", \"description\": \"Correctly extracted \\\\\\`Multiply\\\\\\`...\"} \\\\\\] } Run a finetune like this: instructor jobs create-from-file math\\\\\\_finetunes.jsonl Once a model is trained you can simply change mode to dispatch and it will use the model to run the function! from instructor import Instructions instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", ) @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") def fn(a: int, b: int) -> Multiply: # now this code will be short circuited and the model will be used instead. resp = a + b return Multiply(a=a, b=b, result=resp) With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation. Was this page helpful? Back to top Previous Validators Next Types Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Validators - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/reask_validation/",
    "html": "Skip to content Instructor Validators Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Pydantic Code-based Validation Example Output for Code-Based Validation LLM-Based Validation Example Output for LLM-Based Validation Using Reasking Logic to Correct Outputs Step 1: Define the Response Model with Validators Step 2. Using the Client with Retries What happens behind the scenes? Advanced Validation Techniques Takeaways Validation and Reasking¶ Instead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the system can use to self-correct. Pydantic¶ Pydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic docs on validators. Good llm validation is just good validation If you want to see some more examples on validators checkout our blog post Good LLM validation is just good validation Code-based Validation Example¶ First define a Pydantic model with a validator using the Annotation class from typing\\\\\\_extensions. Enforce a naming rule using Pydantic's built-in validation: from pydantic import BaseModel, ValidationError from typing\\\\\\_extensions import Annotated from pydantic import AfterValidator def name\\\\\\_must\\\\\\_contain\\\\\\_space(v: str) -> str: if \" \" not in v: raise ValueError(\"Name must contain a space.\") return v.lower() class UserDetail(BaseModel): age: int name: Annotated\\\\\\[str, AfterValidator(name\\\\\\_must\\\\\\_contain\\\\\\_space)\\\\\\] try: person = UserDetail(age=29, name=\"Jason\") except ValidationError as e: print(e) Output for Code-Based Validation¶ 1 validation error for UserDetail name Value error, name must contain a space (type=value\\\\\\_error) As we can see, Pydantic raises a validation error when the name attribute does not contain a space. This is a simple example, but it demonstrates how Pydantic can be used to validate attributes of a model. LLM-Based Validation Example¶ LLM-based validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error. import instructor from openai import OpenAI from instructor import llm\\\\\\_validator from pydantic import BaseModel, ValidationError, BeforeValidator from typing\\\\\\_extensions import Annotated # Apply the patch to the OpenAI client client = instructor.patch(OpenAI()) class QuestionAnswer(BaseModel): question: str answer: Annotated\\\\\\[ str, BeforeValidator(llm\\\\\\_validator(\"don't say objectionable things\", openai\\\\\\_client=client)) \\\\\\] try: qa = QuestionAnswer( question=\"What is the meaning of life?\", answer=\"The meaning of life is to be evil and steal\", ) except ValidationError as e: print(e) Output for LLM-Based Validation¶ It is important to not here that the error message is generated by the LLM, not the code, so it'll be helpful for re asking the model. 1 validation error for QuestionAnswer answer Assertion failed, The statement is objectionable. (type=assertion\\\\\\_error) Using Reasking Logic to Correct Outputs¶ Validators are a great tool for ensuring some property of the outputs. When you use the patch() method with the openai client, you can use the max\\\\\\_retries parameter to set the number of times you can reask the model to correct the output. It is a great layer of defense against bad outputs of two forms: 1. Pydantic Validation Errors (code or llm based) 2. JSON Decoding Errors (when the model returns a bad response) Step 1: Define the Response Model with Validators¶ Notice that the field validator wants the name in uppercase, but the user input is lowercase. The validator will raise a ValueError if the name is not in uppercase. import instructor from pydantic import BaseModel, field\\\\\\_validator # Apply the patch to the OpenAI client client = instructor.patch(OpenAI()) class UserDetails(BaseModel): name: str age: int @field\\\\\\_validator(\"name\") @classmethod def validate\\\\\\_name(cls, v): if v.upper() != v: raise ValueError(\"Name must be in uppercase.\") return v Step 2. Using the Client with Retries¶ Here, the UserDetails model is passed as the response\\\\\\_model, and max\\\\\\_retries is set to 2. model = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetails, max\\\\\\_retries=2, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"}, \\\\\\], ) assert model.name == \"JASON\" What happens behind the scenes?¶ Behind the scenes, the instructor.patch() method adds a max\\\\\\_retries parameter to the openai.ChatCompletion.create() method. The max\\\\\\_retries parameter will trigger up to 2 reattempts if the name attribute fails the uppercase validation in UserDetails. try: ... except (ValidationError, JSONDecodeError) as e: kwargs\\\\\\[\"messages\"\\\\\\].append(response.choices\\\\\\[0\\\\\\].message) kwargs\\\\\\[\"messages\"\\\\\\].append( { \"role\": \"user\", \"content\": f\"Please correct the function call; errors encountered:\\\\\\\\n{e}\", } ) Advanced Validation Techniques¶ The docs are currently incomplete, but we have a few advanced validation techniques that we're working on documenting better such as model level validation, and using a validation context. Check out our example on verifying citations which covers: 1. Validate the entire object with all attributes rather than one attribute at a time 2. Using some 'context' to validate the object: In this case, we use the context to check if the citation existed in the original text. Takeaways¶ By integrating these advanced validation techniques, we not only improve the quality and reliability of LLM-generated content, but also pave the way for more autonomous and effective systems. Was this page helpful? Back to top Previous Caching Next Distillation Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Caching - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/caching/",
    "html": "Skip to content Instructor Caching Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents 1. functools.cache for Simple In-Memory Caching 2. diskcache for Persistent, Large Data Caching 2. Redis Caching Decorator for Distributed Systems Caching If you want to learn more about concepts in caching and how to use them in your own projects, check out our blog on the topic. 1. functools.cache for Simple In-Memory Caching¶ When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session. or in an application where we don't need to persist the cache between sessions. import functools import instructor from openai import OpenAI client = instructor.patch(OpenAI()) class UserDetail(BaseModel): name: str age: int @functools.cache def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Changing the Model does not Invalidate the Cache Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result. Now we can call extract multiple times with the same argument, and the result will be cached in memory for faster access. import time start = time.perf\\\\\\_counter() # Using time.perf\\\\\\_counter() to measure the time taken to run the function is better than using time.time() because it's more accurate and less susceptible to system clock changes. model = extract(\"Extract jason is 25 years old\") print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") start = time.perf\\\\\\_counter() model = extract(\"Extract jason is 25 years old\") # The second time we call extract, the result is returned from the cache, and the function is not called. print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") >>> Time taken: 0.9267581660533324 >>> Time taken: 1.2080417945981026e-06 # The second call to extract is much faster because the result is returned from the cache! Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries. What is a decorator? 2. diskcache for Persistent, Large Data Caching¶ Copy Caching Code When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data! import functools import inspect import instructor import diskcache from openai import OpenAI from pydantic import BaseModel client = instructor.patch(OpenAI()) cache = diskcache.Cache('./my\\\\\\_cache\\\\\\_directory') def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation # We use inspect.signature to get the function's return type annotation, which we use to validate the cached result. if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type We use Pydantic's model\\\\\\_validate\\\\\\_json to deserialize the cached result into a Pydantic model. return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence. 2. Redis Caching Decorator for Distributed Systems¶ Copy Caching Code When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures. import redis import functools import inspect import json import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) cache = redis.Redis(\"localhost\") def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: # Assuming client.chat.completions.create returns a UserDetail instance return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types. Looking carefully If you look carefully at the code above you'll notice that we're using the same instructor\\\\\\_cache decorator as before. The implementation is the same, but we're using a different caching backend! Was this page helpful? Back to top Previous FastAPI Next Validators Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "FastAPI - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/fastapi/",
    "html": "Skip to content Instructor FastAPI Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Why Choose FastAPI and Pydantic? Code Example: Starting a FastAPI App with a POST Request Streaming Responses with FastAPI Automatic Documentation with FastAPI Integrating Pydantic Models with FastAPI¶ FastAPI is an enjoyable tool for building web applications in Python. It is well known for its integration with Pydantic models, which makes defining and validating data structures straightforward and efficient. In this guide, we explore how simple functions that return Pydantic models can seamlessly integrate with FastAPI. Why Choose FastAPI and Pydantic?¶ FastAPI is a modern, high-performance web framework for building APIs with Python. Supports OpenAPI and JSON Schema for automatic documentation and validation. Supports AsyncIO for asynchronous programming leveraging the AsyncOpenAI() client Code Example: Starting a FastAPI App with a POST Request¶ The following code snippet demonstrates how to start a FastAPI app with a POST endpoint. This endpoint accepts and returns data defined by a Pydantic model. import instructor from fastapi import FastAPI from pydantic import BaseModel from openai import AsyncOpenAI # Enables response\\\\\\_model client = instructor.patch(AsyncOpenAI()) app = FastAPI() class UserData(BaseModel): # This can be the model for the input data query: str class UserDetail(BaseModel): name: str age: int @app.post(\"/endpoint\", response\\\\\\_model=UserDetail) def endpoint\\\\\\_function(data: UserData) -> UserDetail: user\\\\\\_detail = await client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": f\"Extract: \\\\\\`{data.query}\\\\\\`\"}, \\\\\\] ) return user\\\\\\_detail Streaming Responses with FastAPI¶ FastAPI supports streaming responses, which is useful for returning large amounts of data. This feature is particularly useful when working with large language models (LLMs) that generate a large amount of data. from fastapi.responses import StreamingResponse from typing import Iterable # Route to handle SSE events and return users @app.post(\"/extract\", response\\\\\\_class=StreamingResponse) async def extract(data: UserData): users = await client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=Iterable\\\\\\[UserDetail\\\\\\], stream=True, messages=\\\\\\[ {\"role\": \"user\", \"content\": data.query}, \\\\\\] ) async def generate(): for user in users: resp\\\\\\_json = user.model\\\\\\_dump\\\\\\_json() yield f\"data: {resp\\\\\\_json}\" yield \"data: \\\\\\[DONE\\\\\\]\" return StreamingResponse(generate(), media\\\\\\_type=\"text/event-stream\") Automatic Documentation with FastAPI¶ FastAPI leverages the OpenAPI specification to automatically generate a dynamic and interactive documentation page, commonly referred to as the /docs page. This feature is incredibly useful for developers, as it offers a live environment to test API endpoints directly through the browser. To explore the capabilities of your API, follow these steps: Run the API using the Uvicorn command: uvicorn main:app --reload. Open your web browser and navigate to http://127.0.0.1:8000/docs. You will find an interactive UI where you can send different requests to your API and see the responses in real-time. Was this page helpful? Back to top Previous Streaming Next Caching Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Streaming - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/lists/",
    "html": "Skip to content Instructor Streaming Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Extracting Tasks using Iterable Streaming Tasks Multi-task and Streaming¶ A common use case of structured extraction is defining a single schema class and then making another schema to create a list to do multiple extraction from pydantic import BaseModel class User(BaseModel): name: str age: int class Users(BaseModel): users: List\\\\\\[User\\\\\\] Defining a task and creating a list of classes is a common enough pattern that we make this convenient by making use of Iterable\\\\\\[T\\\\\\]. This lets us dynamically create a new class that: Has dynamic docstrings and class name based on the task Support streaming by collecting tokens until a task is received back out. Extracting Tasks using Iterable¶ By using Iterable you get a very convenient class with prompts and names automatically defined: import instructor from openai import OpenAI from typing import Iterable from pydantic import BaseModel client = instructor.patch(OpenAI(), mode=instructor.function\\\\\\_calls.Mode.JSON) class User(BaseModel): name: str age: int Users = Iterable\\\\\\[User\\\\\\] users = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Users, stream=False, messages=\\\\\\[ { \"role\": \"user\", \"content\": \"Consider this data: Jason is 10 and John is 30.\\\\\\\\ Correctly segment it into entitites\\\\\\\\ Make sure the JSON is correct\", }, \\\\\\], ) for user in users: assert isinstance(user, User) print(user) >>> name=\"Jason\" \"age\"=10 >>> name=\"John\" \"age\"=10 Streaming Tasks¶ We can also generate tasks as the tokens are streamed in by defining an Iterable\\\\\\[T\\\\\\] type. Lets look at an example in action with the same class from typing import Iterable Users = Iterable\\\\\\[User\\\\\\] users = client.chat.completions.create( model=\"gpt-4\", temperature=0.1, stream=True, response\\\\\\_model=Users, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a perfect entity extraction system\", }, { \"role\": \"user\", \"content\": ( f\"Consider the data below:\\\\\\\\n{input}\" \"Correctly segment it into entitites\" \"Make sure the JSON is correct\" ), }, \\\\\\], max\\\\\\_tokens=1000, ) for user in users: assert isinstance(user, User) print(user) >>> name=\"Jason\" \"age\"=10 >>> name=\"John\" \"age\"=10 This streaming is still a prototype, but should work quite well for simple schemas. Was this page helpful? Back to top Previous Patching Next FastAPI Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Patching - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/patching/",
    "html": "Skip to content Instructor Patching Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Function Calling Tool Calling JSON Mode Markdown JSON Mode Schema Integration Patching¶ Instructor enhances client functionality with three new keywords for backwards compatibility. This allows use of the enhanced client as usual, with structured output benefits. response\\\\\\_model: Defines the response type for chat.completions.create. max\\\\\\_retries: Determines retry attempts for failed chat.completions.create validations. validation\\\\\\_context: Provides extra context to the validation process. There are three methods for structured output: Function Calling: The primary method. Use this for stability and testing. Tool Calling: Useful in specific scenarios; lacks the reasking feature of OpenAI's tool calling API. JSON Mode: Offers closer adherence to JSON but with more potential validation errors. Suitable for specific non-function calling clients. Function Calling¶ from openai import OpenAI import instructor client = instructor.patch(OpenAI()) Tool Calling¶ import instructor from instructor import Mode client = instructor.patch(OpenAI(), mode=Mode.TOOLS) JSON Mode¶ import instructor from instructor import Mode from openai import OpenAI client = instructor.patch(OpenAI(), mode=Mode.JSON) Markdown JSON Mode¶ Experimental This is not recommended, and may not be supported in the future, this is just left to support vision models. import instructor from instructor import Mode from openai import OpenAI client = instructor.patch(OpenAI(), mode=Mode.MD\\\\\\_JSON) Schema Integration¶ In JSON Mode, the schema is part of the system message: import instructor from openai import OpenAI client = instructor.patch(OpenAI()) response = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", response\\\\\\_format={\"type\": \"json\\\\\\_object\"}, messages=\\\\\\[ { \"role\": \"system\", \"content\": f\"Match your response to this json\\\\\\_schema: \\\\\\\\n{UserExtract.model\\\\\\_json\\\\\\_schema()\\\\\\['properties'\\\\\\]}\", }, { \"role\": \"user\", \"content\": \"Extract jason is 25 years old\", }, \\\\\\], ) user = UserExtract.from\\\\\\_response(response, mode=Mode.JSON) assert user.name.lower() == \"jason\" assert user.age == 25 Was this page helpful? Back to top Previous Missing Next Streaming Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Missing - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/maybe/",
    "html": "Skip to content Instructor Missing Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Defining the Model Defining the function Handling the result Pattern Matching Handling Missing Data¶ The Maybe pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning None, you can use a Maybe type to encapsulate both the result and potential errors. This pattern is particularly useful when making LLM calls, as providing language models with an escape hatch can effectively reduce hallucinations. Defining the Model¶ Using Pydantic, we'll first define the UserDetail and MaybeUser classes. from pydantic import BaseModel, Field, Optional class UserDetail(BaseModel): age: int name: str role: Optional\\\\\\[str\\\\\\] = Field(default=None) class MaybeUser(BaseModel): result: Optional\\\\\\[UserDetail\\\\\\] = Field(default=None) error: bool = Field(default=False) message: Optional\\\\\\[str\\\\\\] = Field(default=None) def \\\\\\_\\\\\\_bool\\\\\\_\\\\\\_(self): return self.result is not None Notice that MaybeUser has a result field that is an optional UserDetail instance where the extracted data will be stored. The error field is a boolean that indicates whether an error occurred, and the message field is an optional string that contains the error message. Defining the function¶ Once we have the model defined, we can create a function that uses the Maybe pattern to extract the data. import random import instructor from openai import OpenAI from typing import Optional # This enables the \\\\\\`response\\\\\\_model\\\\\\` keyword client = instructor.patch(OpenAI()) def extract(content: str) -> MaybeUser: return openai.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=MaybeUser, messages=\\\\\\[ {\"role\": \"user\", \"content\": f\"Extract \\\\\\`{content}\\\\\\`\"}, \\\\\\], ) user1 = extract(\"Jason is a 25-year-old scientist\") # output: { \"result\": { \"age\": 25, \"name\": \"Jason\", \"role\": \"scientist\" }, \"error\": false, \"message\": null } user2 = extract(\"Unknown user\") # output: { \"result\": null, \"error\": true, \"message\": \"User not found\" } As you can see, when the data is extracted successfully, the result field contains the UserDetail instance. When an error occurs, the error field is set to True, and the message field contains the error message. Handling the result¶ There are a few ways we can handle the result. Normally, we can just access the individual fields. def process\\\\\\_user\\\\\\_detail(maybe\\\\\\_user: MaybeUser): if not maybe\\\\\\_user.error: user = maybe\\\\\\_user.result print(f\"User {user.name} is {user.age} years old\") else: print(f\"Not found: {user1.message}\") Pattern Matching¶ We can also use pattern matching to handle the result. This is a great way to handle errors in a structured way. def process\\\\\\_user\\\\\\_detail(maybe\\\\\\_user: MaybeUser): match maybe\\\\\\_user: case MaybeUser(error=True, message=msg): print(f\"Error: {msg}\") case MaybeUser(result=user\\\\\\_detail) if user\\\\\\_detail: assert isinstance(user\\\\\\_detail, UserDetail) print(f\"User {user\\\\\\_detail.name} is {user\\\\\\_detail.age} years old\") case \\\\\\_: print(\"Unknown error\") If you want to learn more about pattern matching, check out Pydantic's docs on Structural Pattern Matching Was this page helpful? Back to top Previous Fields Next Patching Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Fields - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/fields/",
    "html": "Skip to content Instructor Fields Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Default values Using Annotated Exclude Customizing JSON Schema General notes on JSON schema generation Fields The pydantic.Field function is used to customize and add metadata to fields of models. To learn more, check out the Pydantic documentation as this is a near replica of that documentation that is relevant to prompting. Default values¶ The default parameter is used to define a default value for a field. from pydantic import BaseModel, Field class User(BaseModel): name: str = Field(default='John Doe') user = User() print(user) #> name='John Doe' You can also use default\\\\\\_factory to define a callable that will be called to generate a default value. from uuid import uuid4 from pydantic import BaseModel, Field class User(BaseModel): id: str = Field(default\\\\\\_factory=lambda: uuid4().hex) Info The default and default\\\\\\_factory parameters are mutually exclusive. Note If you use typing.Optional, it doesn't mean that the field has a default value of None you must use default or default\\\\\\_factory to define a default value. Then it will be considered not required when sent to the language model. Using Annotated¶ The Field function can also be used together with Annotated. from uuid import uuid4 from typing\\\\\\_extensions import Annotated from pydantic import BaseModel, Field class User(BaseModel): id: Annotated\\\\\\[str, Field(default\\\\\\_factory=lambda: uuid4().hex)\\\\\\] Exclude¶ The exclude parameter can be used to control which fields should be excluded from the model when exporting the model. This is helpful when you want to exclude fields that are not relevant to the model generation like scratch\\\\\\_pad or chain\\\\\\_of\\\\\\_thought See the following example: from pydantic import BaseModel, Field from datetime import date class DateRange(BaseModel): chain\\\\\\_of\\\\\\_thought: str = Field( description=\"Reasoning behind the date range.\" exclude=True) start\\\\\\_date: date end\\\\\\_date: date date\\\\\\_range = DateRange( chain\\\\\\_of\\\\\\_thought=\"\"\" I want to find the date range for the last 30 days. Today is 2021-01-30 therefore the start date should be 2021-01-01 and the end date is 2021-01-30\"\"\", start\\\\\\_date=date(2021, 1, 1), end\\\\\\_date=date(2021, 1, 30), ) print(date\\\\\\_range.model\\\\\\_dump\\\\\\_json()) #> start\\\\\\_date=datetime.date(2021, 1, 1) end\\\\\\_date=datetime.date(2021, 1, 30) Customizing JSON Schema¶ There are some fields that are exclusively used to customise the generated JSON Schema: title: The title of the field. description: The description of the field. examples: The examples of the field. json\\\\\\_schema\\\\\\_extra: Extra JSON Schema properties to be added to the field. These all work as great opportunities to add more information to the JSON schema as part of your prompt engineering. Here's an example: from pydantic import BaseModel, EmailStr, Field, SecretStr class User(BaseModel): age: int = Field(description='Age of the user') email: EmailStr = Field(examples=\\\\\\['marcelo@mail.com'\\\\\\]) name: str = Field(title='Username') password: SecretStr = Field( json\\\\\\_schema\\\\\\_extra={ 'title': 'Password', 'description': 'Password of the user', 'examples': \\\\\\['123456'\\\\\\], } ) print(User.model\\\\\\_json\\\\\\_schema()) \"\"\" { 'properties': { 'age': { 'description': 'Age of the user', 'title': 'Age', 'type': 'integer', }, 'email': { 'examples': \\\\\\['marcelo@mail.com'\\\\\\], 'format': 'email', 'title': 'Email', 'type': 'string', }, 'name': {'title': 'Username', 'type': 'string'}, 'password': { 'description': 'Password of the user', 'examples': \\\\\\['123456'\\\\\\], 'format': 'password', 'title': 'Password', 'type': 'string', 'writeOnly': True, }, }, 'required': \\\\\\['age', 'email', 'name', 'password'\\\\\\], 'title': 'User', 'type': 'object', } \"\"\" General notes on JSON schema generation¶ The JSON schema for Optional fields indicates that the value null is allowed. The Decimal type is exposed in JSON schema (and serialized) as a string. The JSON schema does not preserve namedtuples as namedtuples. When they differ, you can specify whether you want the JSON schema to represent the inputs to validation or the outputs from serialization. Sub-models used are added to the $defs JSON attribute and referenced, as per the spec. Sub-models with modifications (via the Field class) like a custom title, description, or default value, are recursively included instead of referenced. The description for models is taken from either the docstring of the class or the argument description to the Field class. Was this page helpful? Back to top Previous Models Next Missing Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Models - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/models/",
    "html": "Skip to content Instructor Models Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents Prompting Optional Values Dynamic model creation Structural Pattern Matching Adding Behavior Response Model¶ Defining LLM output schemas in Pydantic is done via pydantic.BaseModel. To learn more about models in Pydantic, check out their documentation. After defining a Pydantic model, we can use it as the response\\\\\\_model in your client create calls to OpenAI or any other supported model. The job of the response\\\\\\_model parameter is to: - Define the schema and prompts for the language model - Validate the response from the API - Return a Pydantic model instance. Prompting¶ When defining a response model, we can use docstrings and field annotations to define the prompt that will be used to generate the response. from pydantic import BaseModel, Field class User(BaseModel): \"\"\" This is the prompt that will be used to generate the response. Any instructions here will be passed to the language model. \"\"\" name: str = Field(description=\"The name of the user.\") age: int = Field(description=\"The age of the user.\") Here all docstrings, types, and field annotations will be used to generate the prompt. The prompt will be generated by the create method of the client and will be used to generate the response. Optional Values¶ If we use Optional and default, they will be considered not required when sent to the language model class User(BaseModel): name: str = Field(description=\"The name of the user.\") age: int = Field(description=\"The age of the user.\") email: Optional\\\\\\[str\\\\\\] = Field(description=\"The email of the user.\", default=None) Dynamic model creation¶ There are some occasions where it is desirable to create a model using runtime information to specify the fields. For this, Pydantic provides the create\\\\\\_model function to allow models to be created on the fly: from pydantic import BaseModel, create\\\\\\_model class FooModel(BaseModel): foo: str bar: int = 123 BarModel = create\\\\\\_model( 'BarModel', apple=(str, 'russet'), banana=(str, 'yellow'), \\\\\\_\\\\\\_base\\\\\\_\\\\\\_=FooModel, ) print(BarModel) #> print(BarModel.model\\\\\\_fields.keys()) #> dict\\\\\\_keys(\\\\\\['foo', 'bar', 'apple', 'banana'\\\\\\]) When would I use this? Structural Pattern Matching¶ Pydantic supports structural pattern matching for models, as introduced by PEP 636 in Python 3.10. from pydantic import BaseModel class Pet(BaseModel): name: str species: str a = Pet(name='Bones', species='dog') match a: # match \\\\\\`species\\\\\\` to 'dog', declare and initialize \\\\\\`dog\\\\\\_name\\\\\\` case Pet(species='dog', name=dog\\\\\\_name): print(f'{dog\\\\\\_name} is a dog') #> Bones is a dog # default case case \\\\\\_: print('No dog matched') Adding Behavior¶ We can add methods to our Pydantic models, just as any plain Python class. We might want to do this to add some custom logic to our models. from pydantic import BaseModel from typing import Literal from openai import OpenAI import instructor client = instructor.patch(OpenAI()) class SearchQuery(BaseModel): query: str query\\\\\\_type: Literal\\\\\\[\"web\", \"image\", \"video\"\\\\\\] def execute(self): # do some logic here return results query = client.chat.completions.create( ..., response\\\\\\_model=SearchQuery ) results = query.execute() Now we can call execute on our model instance after extracting it from a language model. If you want to see more examples of this checkout our post on RAG is more than embeddings Was this page helpful? Back to top Previous Philosophy Next Fields Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Contributing - Instructor",
    "url": "https://jxnl.github.io/instructor/contributing/",
    "html": "Skip to content Instructor Contributing Type to start searching instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Introduction Why use Instructor? Help with Instructor Installation Contributing Table of contents Evals Issues Pull Requests Contributors Additional Resources We would love for you to contribute to Instructor. Evals¶ We invite you to contribute evals in pytest as a way to monitor the quality of the openai models and the instructor library. To get started check out the jxnl/instructor/tests/evals and contribute your own evals in the form of pytest tests. These evals will be run once a week and the results will be posted. Issues¶ If you find a bug, please file an issue on our issue tracker on GitHub. To help us reproduce the bug, please provide a minimal reproducible example, including a code snippet and the full error message. The response\\\\\\_model you are using. The messages you are using. The model you are using. Pull Requests¶ We welcome pull requests! There is plenty to do, and we are happy to discuss any contributions you would like to make. If it is not a small change, please start by filing an issue first. If you need ideas, you can check out the help wanted or good first issue labels. Grit is used to enforce best practices. You can run grit check to check your code before submitting a pull request. Contributors¶ Additional Resources¶ To enhance your understanding of the documentation, here are some useful references: mkdocs serve: The mkdocs serve command is used to preview your documentation locally during the development phase. When you run this command in your terminal, MkDocs starts a development server, allowing you to view and interact with your documentation in a web browser. This is helpful for checking how your changes look before publishing the documentation. Learn more in the mkdocs serve documentation. hl\\\\\\_lines in Code Blocks: The hl\\\\\\_lines feature in code blocks allows you to highlight specific lines within the code block. This is useful for drawing attention to particular lines of code when explaining examples or providing instructions. You can specify the lines to highlight using the hl\\\\\\_lines option in your code block configuration. For more details and examples, you can refer to the hl\\\\\\_lines documentation. Admonitions: Admonitions are a way to visually emphasize or call attention to certain pieces of information in your documentation. They come in various styles, such as notes, warnings, tips, etc. Admonitions provide a structured and consistent way to present important content. For usage examples and details on incorporating admonitions into your documentation, you can refer to the admonitions documentation. For more details about the documentation structure and features, refer to the MkDocs Material documentation. Thank you for your contributions, and happy coding! Was this page helpful? Back to top Previous Installation Next Tips Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Installation - Instructor",
    "url": "https://jxnl.github.io/instructor/installation/",
    "html": "Instructor Installation Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Introduction Why use Instructor? Help with Instructor Installation Contributing Installation Installation is as simple as: pip install instructor Instructor has a few dependencies: openai: OpenAI's Python client. typer: Build great CLIs. Easy to code. Based on Python type hints. docstring-parser: A parser for Python docstrings, to improve the experience of working with docstrings in jsonschema. pydantic: Data validation and settings management using python type annotations. If you've got Python 3.9+ and pip installed, you're good to go. Was this page helpful? Back to top Previous Help with Instructor Next Contributing Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Help with Instructor - Instructor",
    "url": "https://jxnl.github.io/instructor/help/",
    "html": "Skip to content Instructor Help with Instructor Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Introduction Why use Instructor? Help with Instructor Installation Contributing Table of contents Concepts Cookbooks Blog GitHub Discussions GitHub Issues Twitter Getting help with Instructor¶ If you need help getting started with Instructor or with advanced usage, the following sources may be useful. Concepts¶ The concepts section explains the core concepts of Instructor and how to prompt with models. Cookbooks¶ The cookbooks are a great place to start. They contain a variety of examples that demonstrate how to use Instructor in different scenarios. Blog¶ The blog contains articles that explain how to use Instructor in different scenarios. GitHub Discussions¶ GitHub discussions are useful for asking questions, your question and the answer will help everyone. GitHub Issues¶ GitHub issues are useful for reporting bugs or requesting new features. Twitter¶ You can also reach out to me on Twitter if you have any questions or ideas. Was this page helpful? Back to top Previous Why use Instructor? Next Installation Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Welcome to the Instructor Blog - Instructor",
    "url": "https://jxnl.github.io/instructor/blog/",
    "html": "Skip to content Instructor Welcome to the Instructor Blog Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Blog Archive 2023 Table of contents Advanced Topics Learning Python Talks Structured Outputs with Anyscale Introduction to Caching in Python Generators and LLM Streaming Verifying LLM Citations with Pydantic Introduction to Batch Processing using asyncio and Instructor Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density AI Engineer Keynote: Pydantic is all you need Good LLM Validation is Just Good Validation Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation RAG is more than just embedding search Welcome to the Instructor Blog¶ The goal of the blog is to capture some content that does not neatly fit within documentation or the cookbooks. Advanced Topics¶ What is Query Understanding, how does it go beyond embeddings? How can one achieve GPT-4 level summaries using GPT-3.5-turbo? What are the basics of Guardrails and Validation in AI models? How does one validate citations in AI-generated content? What are the methods and benefits of fine-tuning and distillation in AI models? How can I use Anyscale with Instructor? Learning Python¶ How can I effectively cache my functions in Python? What are the fundamentals of batch processing with async in Python? How can I stream models to improve latency? Talks¶ What were the key insights and topics covered at the AI Engineering Summit 2023? 2023/12/15 2 min read Structured Outputs with Anyscale Open-source LLMS are gaining popularity, and the release of Anyscale's Mistral model has made it possible to obtain structured outputs using JSON schema at any scale. Instead of relying on a model's default output mode, you can utilize JSON schema to obtain structured outputs. This approach is a time-saving alternative to extensive prompt engineering. By the end of this blog post, you will learn how to effectively utilize the instructor at any scale. But before we proceed, let's first explore the concept of patching. Patching Instructor's patch enhances a openai api it with the following features: response\\\\\\_model in create calls that returns a pydantic model max\\\\\\_retries in create calls that retries the call if it fails by using a backoff strategy Learn More To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page. Anyscale The good news is that Anyscale employs the same OpenAI client, and its models support some of these output modes too! Getting access If you want to try this out for yourself check out the Anyscale website. You can get started here. Let's explore one of the models available in Anyscale's extensive collection! from openai import OpenAI from pydantic import BaseModel import instructor class UserDetails(BaseModel): name: str age: int # enables \\\\\\`response\\\\\\_model\\\\\\` in create call client = instructor.patch( OpenAI( base\\\\\\_url=\"https://api.endpoints.anyscale.com/v1\", api\\\\\\_key=\"\" ), # This uses Anyscale's json schema output mode mode=instructor.Mode.JSON\\\\\\_SCHEMA ) resp = client.chat.completions.create( model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a world class extractor\" }, { \"role\": \"user\", \"content\": 'Extract the following entities: \"Jason is 20\"' }, \\\\\\], response\\\\\\_model=UserDetails, ) print(resp) # >>> name='Jason' age=20 You can find more information about Anyscale's output mode support here. Continue reading 2023/11/26 7 min read Introduction to Caching in Python Instructor makes working with language models easy, but they are still computationally expensive. Today, we're diving into optimizing instructor code while maintaining the excellent DX offered by Pydantic models. We'll tackle the challenges of caching Pydantic models, typically incompatible with pickle, and explore solutions that use decorators like functools.cache. Then, we'll craft custom decorators with diskcache and redis to support persistent caching and distributed systems. Lets first consider our canonical example, using the OpenAI Python client to extract user details. import instructor from openai import OpenAI from pydantic import BaseModel # Enables \\\\\\`response\\\\\\_model\\\\\\` client = instructor.patch(OpenAI()) class UserDetail(BaseModel): name: str age: int def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Now imagine batch processing data, running tests or experiments, or simply calling extract multiple times over a workflow. We'll quickly run into performance issues, as the function may be called repeatedly, and the same data will be processed over and over again, costing us time and money. 1. functools.cache for Simple In-Memory Caching When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session or in an application where we don't need to persist the cache between sessions. import functools @functools.cache def extract(data): return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Changing the Model does not Invalidate the Cache Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result. Now we can call extract multiple times with the same argument, and the result will be cached in memory for faster access. import time start = time.perf\\\\\\_counter() # Using time.perf\\\\\\_counter() to measure the time taken to run the function is better than using time.time() because it's more accurate and less susceptible to system clock changes. model = extract(\"Extract jason is 25 years old\") print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") start = time.perf\\\\\\_counter() model = extract(\"Extract jason is 25 years old\") # The second time we call extract, the result is returned from the cache, and the function is not called. print(f\"Time taken: {time.perf\\\\\\_counter() - start}\") >>> Time taken: 0.9267581660533324 >>> Time taken: 1.2080417945981026e-06 # The second call to extract is much faster because the result is returned from the cache! Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries. What is a decorator? 2. diskcache for Persistent, Large Data Caching Copy Caching Code When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data! import functools import inspect import instructor import diskcache from openai import OpenAI from pydantic import BaseModel client = instructor.patch(OpenAI()) cache = diskcache.Cache('./my\\\\\\_cache\\\\\\_directory') def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation # We use inspect.signature to get the function's return type annotation, which we use to validate the cached result. if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type We use Pydantic's model\\\\\\_validate\\\\\\_json to deserialize the cached result into a Pydantic model. return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence. 2. Redis Caching Decorator for Distributed Systems Copy Caching Code When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures. import redis import functools import inspect import json import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) cache = redis.Redis(\"localhost\") def instructor\\\\\\_cache(func): \"\"\"Cache a function that returns a Pydantic model\"\"\" return\\\\\\_type = inspect.signature(func).return\\\\\\_annotation if not issubclass(return\\\\\\_type, BaseModel): # We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic raise ValueError(\"The return type must be a Pydantic model\") @functools.wraps(func) def wrapper(\\\\\\*args, \\\\\\*\\\\\\*kwargs): key = f\"{func.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_}-{functools.\\\\\\_make\\\\\\_key(args, kwargs, typed=False)}\" # We use functool's \\\\\\_make\\\\\\_key to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately. # Check if the result is already cached if (cached := cache.get(key)) is not None: # Deserialize from JSON based on the return type return return\\\\\\_type.model\\\\\\_validate\\\\\\_json(cached) # Call the function and cache its result result = func(\\\\\\*args, \\\\\\*\\\\\\*kwargs) serialized\\\\\\_result = result.model\\\\\\_dump\\\\\\_json() cache.set(key, serialized\\\\\\_result) return result return wrapper class UserDetail(BaseModel): name: str age: int @instructor\\\\\\_cache def extract(data) -> UserDetail: # Assuming client.chat.completions.create returns a UserDetail instance return client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": data}, \\\\\\] ) Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types. Looking carefully If you look carefully at the code above you'll notice that we're using the same instructor\\\\\\_cache decorator as before. The implementatino is the same, but we're using a different caching backend! Conclusion Choosing the right caching strategy depends on your application's specific needs, such as the size and type of data, the need for persistence, and the system's architecture. Whether it's optimizing a function's performance in a small application or managing large datasets in a distributed environment, Python offers robust solutions to improve efficiency and reduce computational overhead. If you'd like to use this code, try to send it over to ChatGPT to understand it more, and to add additional features that might matter for you, for example, the cache isn't invalidated when your BaseModel changes, so you might want to encode the Model.model\\\\\\_json\\\\\\_schema() as part of the key. If you like the content check out our GitHub as give us a star and checkout the library. Continue reading 2023/11/26 6 min read Generators and LLM Streaming Latency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times. And what makes streaming possible? Generators! In this post, we're going to dive into the cool world of Python generators — these tools are more than just a coding syntax trick. We'll explore Python generators from the ground up and then delve into LLM streaming using the Instructor library. Python Generators: An Efficient Approach to Iterables Generators in Python are a game-changer for handling large data sets and stream processing. They allow functions to yield values one at a time, pausing and resuming their state, which is a faster and more memory-efficient approach compared to traditional collections that store all elements in memory. The Basics: Yielding Values A generator function in Python uses the yield keyword. It yields values one at a time, allowing the function to pause and resume its state. def count\\\\\\_to\\\\\\_3(): yield 1 yield 2 yield 3 for num in count\\\\\\_to\\\\\\_3(): print(num) 1 2 3 Advantages Over Traditional Collections Lazy Evaluation & reduced latency: The time to get the first element (or time-to-first-token in LLM land) from a generator is significantly lower. Generators only produce one value at a time, whereas accessing the first element of a collection will require that the whole collection be created first. Memory Efficiency: Only one item is in memory at a time. Maintain State: Automatically maintains state between executions. Let's see how much faster generators are and where they really shine: import time def expensive\\\\\\_func(x): \"\"\"Simulate an expensive operation.\"\"\" time.sleep(1) return x \\\\\\*\\\\\\* 2 def calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_list(func\\\\\\_input, func): \"\"\"Calculate using a list comprehension and return the first result with its computation time.\"\"\" start\\\\\\_perf = time.perf\\\\\\_counter() result = \\\\\\[func(x) for x in func\\\\\\_input\\\\\\]\\\\\\[0\\\\\\] end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (list): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") return result def calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_generator(func\\\\\\_input, func): \"\"\"Calculate using a generator and return the first result with its computation time.\"\"\" start\\\\\\_perf = time.perf\\\\\\_counter() result = next(func(x) for x in func\\\\\\_input) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (generator): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") return result # Prepare inputs for the function numbers = \\\\\\[1, 2, 3, 4, 5\\\\\\] # Benchmarking first\\\\\\_result\\\\\\_list = calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_list(numbers, expensive\\\\\\_func) first\\\\\\_result\\\\\\_gen = calculate\\\\\\_time\\\\\\_for\\\\\\_first\\\\\\_result\\\\\\_with\\\\\\_generator(numbers, expensive\\\\\\_func) Time for first result (list): 5.02 seconds Time for first result (generator): 1.01 seconds The generator computes one expensive operation and returns the first result immediately, while the list comprehension computes the expensive operation for all elements in the list before returning the first result. Generator Expressions: A Shortcut Python also allows creating generators in a single line of code, known as generator expressions. They are syntactically similar to list comprehensions but use parentheses. squares = (x\\\\\\*x for x in range(10)) Use Cases in Real-World Applications Generators shine in scenarios like reading large files, data streaming (eg. llm token streaming), and pipeline creation for data processing. LLM Streaming If you've used ChatGPT, you'll see that the tokens are streamed out one by one, instead of the full response being shown at the end (can you imagine waiting for the full response??). This is made possible by generators. Here's how a vanilla openai generator looks: from openai import OpenAI # Set your OpenAI API key client = OpenAI( api\\\\\\_key=\"My API Key\", ) response\\\\\\_generator = client.chat.completions.create( model='gpt-3.5-turbo', messages=\\\\\\[ {'role': 'user', 'content': \"What are some good reasons to smile?\"} \\\\\\], temperature=0, stream=True ) for chunk in response\\\\\\_generator: print(chunk.choices\\\\\\[0\\\\\\].delta.content, end=\"\") This is great, but what if we want to do some structured extraction on this stream? For instance, we might want to render frontend components based on product rankings that are streamed out by an LLM. Should we wait for the entire stream to finish before extracting & validating the list of components or can we extract & validate the components in real time as they are streamed? In e-commerce, every millisecond matters so the time-to-first-render can differentiate a successful and not-so-successful e commerce store (and i know how a failing e commerce store feels :/ ). Let's see how we can use Instructor to handle extraction from this real time stream! E-commerce Product Ranking SCENARIO Imagine an e-commerce platform where we have: • a customer profile: this includes a detailed history of purchases, browsing behavior, product ratings, preferences in various categories, search history, and even responses to previous recommendations. This extensive data is crucial for generating highly personalized and relevant product suggestions. • a list of candidate products: these could be some shortlisted products we think the customer would like. Our goal is to re-rerank these candidate products for the best conversion and we'll use an LLM! STREAM PROCESSING User Data: Let's assume we have the following user profile: profile\\\\\\_data = \"\"\" Customer ID: 12345 Recent Purchases: \\\\\\[Laptop, Wireless Headphones, Smart Watch\\\\\\] Frequently Browsed Categories: \\\\\\[Electronics, Books, Fitness Equipment\\\\\\] Product Ratings: {Laptop: 5 stars, Wireless Headphones: 4 stars} Recent Search History: \\\\\\[best budget laptops 2023, latest sci-fi books, yoga mats\\\\\\] Preferred Brands: \\\\\\[Apple, AllBirds, Bench\\\\\\] Responses to Previous Recommendations: {Philips: Not Interested, Adidas: Not Interested} Loyalty Program Status: Gold Member Average Monthly Spend: $500 Preferred Shopping Times: Weekend Evenings ... \"\"\" We want to rank the following products for this user: products = \\\\\\[ {\"product\\\\\\_id\": 1, \"product\\\\\\_name\": \"Apple MacBook Air (2023) - Latest model, high performance, portable\"}, {\"product\\\\\\_id\": 2, \"product\\\\\\_name\": \"Sony WH-1000XM4 Wireless Headphones - Noise-canceling, long battery life\"}, {\"product\\\\\\_id\": 3, \"product\\\\\\_name\": \"Apple Watch Series 7 - Advanced fitness tracking, seamless integration with Apple ecosystem\"}, {\"product\\\\\\_id\": 4, \"product\\\\\\_name\": \"Kindle Oasis - Premium e-reader with adjustable warm light\"}, {\"product\\\\\\_id\": 5, \"product\\\\\\_name\": \"AllBirds Wool Runners - Comfortable, eco-friendly sneakers\"}, {\"product\\\\\\_id\": 6, \"product\\\\\\_name\": \"Manduka PRO Yoga Mat - High-quality, durable, eco-friendly\"}, {\"product\\\\\\_id\": 7, \"product\\\\\\_name\": \"Bench Hooded Jacket - Stylish, durable, suitable for outdoor activities\"}, {\"product\\\\\\_id\": 8, \"product\\\\\\_name\": \"GoPro HERO9 Black - 5K video, waterproof, for action photography\"}, {\"product\\\\\\_id\": 9, \"product\\\\\\_name\": \"Nespresso Vertuo Next Coffee Machine - Quality coffee, easy to use, compact design\"}, {\"product\\\\\\_id\": 10, \"product\\\\\\_name\": \"Project Hail Mary by Andy Weir - Latest sci-fi book from a renowned author\"} \\\\\\] Let's now define our models for structured extraction. Note: instructor will conveniently let us use Iterable to model an iterable of our class. In this case, once we define our product recommendation model, we can slap on Iterable to define what we ultimately want - a (ranked) list of product recommendations. import instructor from openai import OpenAI from typing import Iterable from pydantic import BaseModel client = instructor.patch(OpenAI(), mode=instructor.function\\\\\\_calls.Mode.JSON) class ProductRecommendation(BaseModel): product\\\\\\_id: str product\\\\\\_name: str Recommendations = Iterable\\\\\\[ProductRecommendation\\\\\\] Now let's use our instructor patch. Since we don't want to wait for all the tokens to finish, will set stream to True and process each product recommendation as it comes in: prompt = f\"Based on the following user profile:\\\\\\\\n{profile\\\\\\_data}\\\\\\\\nRank the following products from most relevant to least relevant:\\\\\\\\n\" + '\\\\\\\\n'.join(f\"{product\\\\\\['product\\\\\\_id'\\\\\\]} {product\\\\\\['product\\\\\\_name'\\\\\\]}\" for product in products) start\\\\\\_perf = time.perf\\\\\\_counter() recommendations\\\\\\_stream = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Iterable\\\\\\[ProductRecommendation\\\\\\], stream=True, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"}, {\"role\": \"user\", \"content\": prompt} \\\\\\] ) for product in recommendations\\\\\\_stream: print(product) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (generator): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") break product\\\\\\_id='1' product\\\\\\_name='Apple MacBook Air (2023)' Time for first result (generator): 4.33 seconds recommendations\\\\\\_stream is a generator! It yields the extracted products as it's processing the stream in real-time. Now let's get the same response without streaming and see how they compare. start\\\\\\_perf = time.perf\\\\\\_counter() recommendations\\\\\\_list = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", temperature=0.1, response\\\\\\_model=Iterable\\\\\\[ProductRecommendation\\\\\\], stream=False, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\"}, {\"role\": \"user\", \"content\": prompt} \\\\\\] ) print(recommendations\\\\\\_list\\\\\\[0\\\\\\]) end\\\\\\_perf = time.perf\\\\\\_counter() print(f\"Time for first result (list): {end\\\\\\_perf - start\\\\\\_perf:.2f} seconds\") product\\\\\\_id='1' product\\\\\\_name='Apple MacBook Air (2023)' Time for first result (list): 8.63 seconds Our web application now displays results faster. Even a 100ms improvement can lead to a 1% increase in revenue. FastAPI We can also take this and set up a streaming LLM API endpoint using FastAPI. Check out our docs on using FastAPI here! Key Takeaways To summarize, we looked at: • Generators in Python: A powerful feature that allows for efficient data handling with reduced latency • LLM Streaming: LLMs provide us generators to stream tokens and Instructor can let us validate and extract data from this stream. Real-time data validation ftw! Don't forget to check our GitHub for more resources and give us a star if you find the library helpful! If you have any questions or need further clarifications, feel free to reach out or dive into the Instructor library's documentation for more detailed information. Happy coding! Continue reading 2023/11/18 4 min read Verifying LLM Citations with Pydantic Ensuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification. We'll start with using a simple substring check to verify citations. Then we'll use instructor itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses. Example 1: Simple Substring Check In this example, we use the Statements class to verify if a given substring quote exists within a text chunk. If the substring is not found, an error is raised. Code Example: from typing import List, Optional from openai import OpenAI from pydantic import BaseModel, Field, ValidationError, ValidationInfo, field\\\\\\_validator, model\\\\\\_validator import instructor client = instructor.patch(OpenAI()) class Statements(BaseModel): body: str substring\\\\\\_quote: str @field\\\\\\_validator(\"substring\\\\\\_quote\") @classmethod def substring\\\\\\_quote\\\\\\_exists(cls, v: str, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) for text\\\\\\_chunk in context.values(): if v in text\\\\\\_chunk: # While we use a simple substring check in this example, we can use more complex techniques like regex or Levenshtein distance. return v raise ValueError(\"Could not find substring\\\\\\_quote \\\\\\`{v}\\\\\\` in contexts\") class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] Once the class is defined, we can use it to validate the context and raise an error if the substring is not found. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is not the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example: answer.0.substring\\\\\\_quote Value error, Could not find substring\\\\\\_quote \\\\\\`Paris is the capital of France\\\\\\` in contexts \\\\\\[type=value\\\\\\_error, input\\\\\\_value='Paris is the capital of France', input\\\\\\_type=str\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Pydantic raises a validation error when the substring\\\\\\_quote attribute does not exist in the context. This approach can be used to validate more complex data using techniques like regex or Levenshtein distance. Example 2: Using LLM for Verification This approach leverages OpenAI's LLM to validate citations. If the citation does not exist in the context, the LLM returns an error message. Code Example: class Validation(BaseModel): is\\\\\\_valid: bool error\\\\\\_messages: Optional\\\\\\[str\\\\\\] = Field(None, description=\"Error messages if any\") class Statements(BaseModel): body: str substring\\\\\\_quote: str @model\\\\\\_validator(mode=\"after\") def substring\\\\\\_quote\\\\\\_exists(self, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) resp: Validation = client.chat.completions.create( response\\\\\\_model=Validation, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Does the following citation exist in the following context?\\\\\\\\n\\\\\\\\nCitation: {self.substring\\\\\\_quote}\\\\\\\\n\\\\\\\\nContext: {context}\", } \\\\\\], model=\"gpt-3.5-turbo\", ) if resp.is\\\\\\_valid: return self raise ValueError(resp.error\\\\\\_messages) class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] Now when we use a correct citation, the LLM returns a valid response. resp = AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is the capital of France\", 3: \"Irrelevant data\", } }, ) print(resp.model\\\\\\_dump\\\\\\_json(indent=2)) Result: { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ { \"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\" } \\\\\\] } When we have citations that don't exist in the context, the LLM returns an error message. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Paris\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is not the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example: 1 validation error for AnswerWithCitaton answer.0 Value error, Citation not found in context \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'body': 'Paris', 'substr... the capital of France'}, input\\\\\\_type=dict\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Example 3: Aligning Citations and Answers In this example, we ensure that the provided answers are aligned with the given citations and context. The LLM is used to verify the alignment. We use the same Statements model as above, but we add a new model for the answer that also verifies the alignment of citations. Code Example: class AnswerWithCitaton(BaseModel): question: str answer: List\\\\\\[Statements\\\\\\] @model\\\\\\_validator(mode=\"after\") def validate\\\\\\_answer(self, info: ValidationInfo): context = info.context.get(\"text\\\\\\_chunks\", None) resp: Validation = client.chat.completions.create( response\\\\\\_model=Validation, messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Does the following answers match the question and the context?\\\\\\\\n\\\\\\\\nQuestion: {self.question}\\\\\\\\n\\\\\\\\nAnswer: {self.answer}\\\\\\\\n\\\\\\\\nContext: {context}\", } \\\\\\], model=\"gpt-3.5-turbo\", ) if resp.is\\\\\\_valid: return self raise ValueError(resp.error\\\\\\_messages) When we have a mismatch between the answer and the citation, the LLM returns an error message. try: AnswerWithCitaton.model\\\\\\_validate( { \"question\": \"What is the capital of France?\", \"answer\": \\\\\\[ {\"body\": \"Texas\", \"substring\\\\\\_quote\": \"Paris is the capital of France\"}, \\\\\\], }, context={ \"text\\\\\\_chunks\": { 1: \"Jason is a pirate\", 2: \"Paris is the capital of France\", 3: \"Irrelevant data\", } }, ) except ValidationError as e: print(e) Error Message Example: 1 validation error for AnswerWithCitaton Value error, The answer does not match the question and context \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'question': 'What is the...he capital of France'}\\\\\\]}, input\\\\\\_type=dict\\\\\\] For further information visit \\\\\\[https://errors.pydantic.dev/2.4/v/value\\\\\\_error\\\\\\](https://errors.pydantic.dev/2.4/v/value\\\\\\_error) Conclusion These examples demonstrate the potential of using Pydantic and OpenAI to enhance data accuracy through citation verification. While the LLM-based approach may not be efficient for runtime operations, it has exciting implications for generating a dataset of accurate responses. By leveraging this method during data generation, we can fine-tune a model that excels in citation accuracy. Similar to our last post on finetuning a better summarizer. If you like the content check out our GitHub as give us a star and checkout the library. Continue reading 2023/11/13 6 min read Introduction to Batch Processing using asyncio and Instructor Today, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using instructor and learn how to use asyncio.gather and asyncio.as\\\\\\_completed for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using asyncio.Semaphore. Github Example If you want to run the code examples in this article, you can find them on jxnl/instructor We will start by defining an async function that calls openai to extract data, and then examine four different ways to execute it. We will discuss the pros and cons of each approach and analyze the results of running them on a small batch. Understanding asyncio asyncio is a Python library that enables writing concurrent code using the async/await syntax. It is particularly useful for IO-bound and structured network code. If you are familiar with OpenAI's SDK, you might have encountered two classes: OpenAI() and AsyncOpenAI(). Today, we will be using the AsyncOpenAI() class, which processes data asynchronously. By utilizing these tools in web applications or batch processing, we can significantly improve performance by handling multiple requests concurrently instead of sequentially. Understanding async and await We will be using the async and await keywords to define asynchronous functions. The async keyword is used to define a function that returns a coroutine object. The await keyword is used to wait for the result of a coroutine object. If you want to understand the deeper details of asyncio, I recommend reading this article by Real Python. Understanding gather vs as\\\\\\_completed In this post we'll show two ways to run tasks concurrently: asyncio.gather and asyncio.as\\\\\\_completed. The gather method is used to run multiple tasks concurrently and return the results as a list. The as\\\\\\_completed returns a iterable is used to run multiple tasks concurrently and return the results as they complete. Another great resource on the differences between the two can be found here. Example: Batch Processing In this example, we will demonstrate how to use asyncio for batch processing tasks, specifically for extracting and processing data concurrently. The script will extract data from a list of texts and process it concurrently using asyncio. import instructor from pydantic import BaseModel from openai import AsyncOpenAI # Enables \\\\\\`response\\\\\\_model\\\\\\` in \\\\\\`create\\\\\\` method client = instructor.apatch(AsyncOpenAI()) We use instructor.apatch to patch the create method of AsyncOpenAI to accept a response\\\\\\_model argument. This is because the create method of AsyncOpenAI does not accept a response\\\\\\_model argument without this patch. class Person(BaseModel): name: str age: int async def extract\\\\\\_person(text: str) -> Person: return await client.chat.completions.create( We use await here to wait for the response from the server before we return the result. This is because create returns a coroutine object, not the result of the coroutine. model=\"gpt-3.5-turbo\", messages=\\\\\\[ {\"role\": \"user\", \"content\": text}, \\\\\\], response\\\\\\_model=Person, ) Notice that now there are async and await keywords in the function definition. This is because we're using the asyncio library to run the function concurrently. Now lets define a batch of texts to process. dataset = \\\\\\[ \"My name is John and I am 20 years old\", \"My name is Mary and I am 21 years old\", \"My name is Bob and I am 22 years old\", \"My name is Alice and I am 23 years old\", \"My name is Jane and I am 24 years old\", \"My name is Joe and I am 25 years old\", \"My name is Jill and I am 26 years old\", \\\\\\] for loop: Running tasks sequentially. persons = \\\\\\[\\\\\\] for text in dataset: person = await extract\\\\\\_person(text) persons.append(person) Even though there is an await keyword, we still have to wait for each task to finish before starting the next one. This is because we're using a for loop to iterate over the dataset. This method, which uses a for loop, will be the slowest among the four methods discussed today. asyncio.gather: Running tasks concurrently. async def gather(): tasks\\\\\\_get\\\\\\_persons = \\\\\\[extract\\\\\\_person(text) for text in dataset\\\\\\] all\\\\\\_persons = await asyncio.gather(\\\\\\*tasks\\\\\\_get\\\\\\_persons) We use await here to wait for all the tasks to finish before assigning the result to all\\\\\\_persons. This is because asyncio.gather returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.as\\\\\\_completed to achieve the same result. Using asyncio.gather allows us to return all the results at once. It is an effective way to speed up our code, but it's not the only way. Particularly, if we have a large dataset, we might not want to wait for everything to finish before starting to process the results. This is where asyncio.as\\\\\\_completed comes into play. asyncio.as\\\\\\_completed: Handling tasks as they complete. async def as\\\\\\_completed(): all\\\\\\_persons = \\\\\\[\\\\\\] tasks\\\\\\_get\\\\\\_persons = \\\\\\[extract\\\\\\_person(text) for text in dataset\\\\\\] for person in asyncio.as\\\\\\_completed(tasks\\\\\\_get\\\\\\_persons): all\\\\\\_persons.append(await person) We use await here to wait for each task to complete before appending it to the list. This is because as\\\\\\_completed returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.gather to achieve the same result. This method is a great way to handle large datasets. We can start processing the results as they come in, especially if we are streaming data back to a client. However, these methods aim to complete as many tasks as possible as quickly as possible. This can be problematic if we want to be considerate to the server we're making requests to. This is where rate limiting comes into play. While there are libraries available to assist with rate limiting, for our initial defense, we will use a semaphore to limit the number of concurrent requests we make. Ordering of results Its important to note that the order of the results will not be the same as the order of the dataset. This is because the tasks are completed in the order they finish, not the order they were started. If you need to preserve the order of the results, you can use asyncio.gather instead. Rate-Limited Gather: Using semaphores to limit concurrency. sem = asyncio.Semaphore(2) async def rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text: str, sem: Semaphore) -> Person: async with sem: We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to. return await extract\\\\\\_person(text) async def rate\\\\\\_limited\\\\\\_gather(sem: Semaphore): tasks\\\\\\_get\\\\\\_persons = \\\\\\[rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text, sem) for text in dataset\\\\\\] resp = await asyncio.gather(\\\\\\*tasks\\\\\\_get\\\\\\_persons) Rate-Limited As Completed: Using semaphores to limit concurrency. sem = asyncio.Semaphore(2) async def rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text: str, sem: Semaphore) -> Person: async with sem: We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to. return await extract\\\\\\_person(text) async def rate\\\\\\_limited\\\\\\_as\\\\\\_completed(sem: Semaphore): all\\\\\\_persons = \\\\\\[\\\\\\] tasks\\\\\\_get\\\\\\_persons = \\\\\\[rate\\\\\\_limited\\\\\\_extract\\\\\\_person(text, sem) for text in dataset\\\\\\] for person in asyncio.as\\\\\\_completed(tasks\\\\\\_get\\\\\\_persons): all\\\\\\_persons.append(await person) We use await here to wait for each task to complete before appending it to the list. This is because as\\\\\\_completed returns a coroutine object, not the result of the coroutine. Alternatively, we can use asyncio.gather to achieve the same result. Now that we have seen the code, let's examine the results of processing 7 texts. As the prompts become longer or if we use GPT-4, the differences between these methods will become more pronounced. Other Options Its important to also note that here we are using a semaphore to limit the number of concurrent requests. However, there are other ways to limit concurrency esp since we have rate limit information from the openai request. You can imagine using a library like ratelimit to limit the number of requests per second. OR catching rate limit exceptions and using tenacity to retry the request after a certain amount of time. tenacity aiolimiter Results As you can see, the for loop is the slowest, while asyncio.as\\\\\\_completed and asyncio.gather are the fastest without any rate limiting. Method Execution Time Rate Limited (Semaphore) For Loop 6.17 seconds Asyncio.gather 0.85 seconds Asyncio.as\\\\\\_completed 0.95 seconds Asyncio.gather 3.04 seconds 2 Asyncio.as\\\\\\_completed 3.26 seconds 2 Practical implications of batch processing The choice of approach depends on the task's nature and the desired balance between speed and resource utilization. Here are some guidelines to consider: Use asyncio.gather for handling multiple independent tasks quickly. Apply asyncio.as\\\\\\_completed for large datasets to process tasks as they complete. Implement rate-limiting to avoid overwhelming servers or API endpoints. If you find the content helpful or want to try out Instructor, please visit our GitHub page and give us a star! Continue reading 2023/11/05 15 min read Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor In this article, we'll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4's iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density. By the end you'll end up with a GPT 3.5 model, (fine-tuned using Instructor's great tooling), capable of producing summaries that rival the effectiveness of Chain of Density \\\\\\[Adams et al. (2023)\\\\\\]. As always, all code is readily available in our examples/chain-of-density folder in our repo for your reference. Datasets and Colab Notebook Part 1) Chain of Density Summarizing extensive texts with AI can be challenging, often relying on inconsistent techniques. Their novel method, Chain Of Density prompting, enhances AI-based text summarization, outperforming human-generated summaries. Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density. First introduced in the paper - From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. The team has found that this method is able to consistently beats similar summaries written by human annotators. Implementation Details Original Prompt We can break down the original process into smaller api calls. This allows us to introduce validation at each step to ensure that we're getting the results that we want. Original Chain of Density Prompt Improved process with Instructor Data Modelling Before we begin modelling the data, let's make sure we install all of our dependencies pip install instructor aiohttp rich INITIAL SUMMARY Let's start by walking through some of the data models that we'll be using as the response\\\\\\_model for our open ai function calls Firstly, we'll need a data model for the initial summary that we will be generating. We'll take the description of this class straight from the original prompt. It's important to note that these docstrings serve a purpose, they are directly used by the LLM when generating the outputs. A quick note on Docstrings class InitialSummary(BaseModel): \"\"\" This is an initial summary which should be long ( 4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words. \"\"\" summary: str = Field( ..., description=\"This is a summary of the article provided which is overly verbose and uses fillers. It should be roughly 80 words in length\", ) REWRITTEN SUMMARY We'll also need one additional class to help model the rewritten schema class RewrittenSummary(BaseModel): \"\"\" This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. Guidelines - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" - Missing entities can appear anywhere in the new summary An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. \"\"\" summary: str = Field( ..., description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\", ) absent: List\\\\\\[str\\\\\\] = Field( ..., default\\\\\\_factory=list, description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\", ) missing: List\\\\\\[str\\\\\\] = Field( default\\\\\\_factory=list, description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\", ) Using Pydantic Validators with Instructor For a more in-depth walkthrough on how to use Pydantic validators with the Instructor library, we recommend checking out our previous article on LLM validation - Good LLM Validation is just Good Validation Ideally, we'd like for Missing to have a length between 1 and 3, Absent to be an empty list and for our rewritten summaries to keep a minimum entity density. With Instructor, we can implement this logic using native Pydantic validators that are simply declared as part of the class itself. import nltk import spacy nlp = spacy.load(\"en\\\\\\_core\\\\\\_web\\\\\\_sm\") @field\\\\\\_validator(\"summary\") def min\\\\\\_length(cls, v: str): tokens = nltk.word\\\\\\_tokenize(v) Similar to the original paper, we utilize the NLTK word tokenizer to count the number of tokens within our generated sentences. We aim for at least 60 tokens in our generated summary so that we don't lose information. num\\\\\\_tokens = len(tokens) if num\\\\\\_tokens < 60: raise ValueError( \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\" ) return v @field\\\\\\_validator(\"missing\") def has\\\\\\_missing\\\\\\_entities(cls, missing\\\\\\_entities: List\\\\\\[str\\\\\\]): if len(missing\\\\\\_entities) == 0: raise ValueError( \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\" ) return missing\\\\\\_entities @field\\\\\\_validator(\"absent\") def has\\\\\\_no\\\\\\_absent\\\\\\_entities(cls, absent\\\\\\_entities: List\\\\\\[str\\\\\\]): absent\\\\\\_entity\\\\\\_string = \",\".join(absent\\\\\\_entities) if len(absent\\\\\\_entities) > 0: print(f\"Detected absent entities of {absent\\\\\\_entity\\\\\\_string}\") raise ValueError( f\"Do not omit the following Entities {absent\\\\\\_entity\\\\\\_string} from the new summary\" ) return absent\\\\\\_entities @field\\\\\\_validator(\"summary\") def min\\\\\\_entity\\\\\\_density(cls, v: str): tokens = nltk.word\\\\\\_tokenize(v) num\\\\\\_tokens = len(tokens) # Extract Entities doc = nlp(v) We also use the spaCy library to calculate the entity density of the generated summary. num\\\\\\_entities = len(doc.ents) density = num\\\\\\_entities / num\\\\\\_tokens if density < 0.08: We also implement a minimum entity density so that we stay within a given range. 0.08 is arbitrarily chosen in this case raise ValueError( f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\" ) return v Putting it all Together Now that we have our models and the rough flow figured out, let's implement a function to summarize a piece of text using Chain Of Density summarization. from openai import OpenAI import instructor client = instructor.patch(OpenAI()) We need to apply a patch function on the OpenAI client for us to get all of the benefits that Instructor provides. With a simple patch, we can get automatic type coercion of our outputs and automatic retries for invalid outputs out of the box! def summarize\\\\\\_article(article: str, summary\\\\\\_steps: int = 3): summary\\\\\\_chain = \\\\\\[\\\\\\] # We first generate an initial summary summary: InitialSummary = client.chat.completions.create( We first generate an initial summary. Note here that we explictly ask for a summary that has 80 words and is lengthy with overly verbose fillers in the system prompt model=\"gpt-4-0613\", response\\\\\\_model=InitialSummary, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": \"The generated summary should be about 80 words.\", }, \\\\\\], max\\\\\\_retries=2, ) prev\\\\\\_summary = None summary\\\\\\_chain.append(summary.summary) for i in range(summary\\\\\\_steps): missing\\\\\\_entity\\\\\\_message = ( \\\\\\[\\\\\\] if prev\\\\\\_summary is None else \\\\\\[ { \"role\": \"user\", \"content\": f\"Please include these Missing Entities: {','.join(prev\\\\\\_summary.missing)}\", }, \\\\\\] ) new\\\\\\_summary: RewrittenSummary = client.chat.completions.create( We slightly modify the original system prompt used in the original paper to perform a rewrite of the summary. Using Instructor, we also get validation of the generated output with our field\\\\\\_validators that we defined above model=\"gpt-4-0613\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"\"\" You are going to generate an increasingly concise,entity-dense summary of the following article. Perform the following two tasks - Identify 1-3 informative entities from the following article which is missing from the previous summary - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities Guidelines - Make every word count: re-write the previous summary to improve flow and make space for additional entities - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\". - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article. - Missing entities can appear anywhere in the new summary - Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \"\"\", }, {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"}, { \"role\": \"user\", \"content\": f\"Here is the previous summary: {summary\\\\\\_chain\\\\\\[-1\\\\\\]}\", }, \\\\\\*missing\\\\\\_entity\\\\\\_message, \\\\\\], max\\\\\\_retries=3, If you've chosen a value that is larger than 0.08, make sure to increase this value in case you need to do multiple rewrites max\\\\\\_tokens=1000, response\\\\\\_model=RewrittenSummary, ) summary\\\\\\_chain.append(new\\\\\\_summary.summary) prev\\\\\\_summary = new\\\\\\_summary return summary\\\\\\_chain This summarization function yields a result which triples the number of entities while maintaining the same number of tokens. We can also see that stylistically, the summary is a lot more natural. First Iteration This article discusses the highly-anticipated boxing match between Manny Pacquiao and Floyd Mayweather. The article revolves around Manny Pacquiao's statements about his upcoming fight and his preparations for the same. A portion of the article provides details about the financial stipulations of the match and its significance in the sporting arena. Quotes from Pacquiao illustrating his determination and his battle strategy are highlighted. The tone of the article is largely centered around creating a build-up to the upcoming mega event. Final Iteration Manny Pacquiao, the Filipino boxer, anticipates the forthcoming May 2 showdown at the MGM Grand as the fight of his life, against the undefeated American Floyd Mayweather, in a $300m bout. Despite being seen as the underdog in this high-stakes Las Vegas match, Pacquiao is confident, promising a warrior's spirit and assuring the fans who have been awaiting this encounter for a decade, that it will indeed be the biggest sporting spectacle in history worthy of their anticipation Part 2) Fine-Tuning In this section, we'll look into how to fine-tune a GPT 3.5 model so that it is able to perform at an equivalent level as a GPT-4 model. We'll then compare the performance of our model against that of GPT-4 to see how it stacks up. Creating a Training Set In order to prevent any contamination of data during testing, we randomly sampled 120 articles from the griffin/chain-of-density dataset and split these articles into a train.csv and a test.csv file which we uploaded to Hugging Face. Now, we just neeed to import the Instructions module from the Instructor package which allows you to generate a nicely formatted .jsonl file to be used for fine-tuning from typing import List from chain\\\\\\_of\\\\\\_density import summarize\\\\\\_article In this example, we're using the summarize\\\\\\_article that we defined up above. We saved it in a local file called chain\\\\\\_of\\\\\\_density.py, hence the import import csv import logging import instructor from pydantic import BaseModel from openai import OpenAI client = instructor.patch(OpenAI()) We patch the default OpenAI client so that we can use the Instructor library with it logging.basicConfig(level=logging.INFO) We also need to configure logging at the INFO level. This is very important, if this is not configured, your output will not be generated. instructions = instructor.Instructions( name=\"Chain Of Density\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"generated.jsonl\")\\\\\\], openai\\\\\\_client=client, ) class GeneratedSummary(BaseModel): \"\"\" This represents a highly concise summary that includes as many entities as possible from the original source article. An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title. Guidelines - Make every word count - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article. - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\" \"\"\" summary: str = Field( ..., description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \", ) @instructions.distil We instantiate a Instruction object which will help us handle the conversion of our function calls into a valid .jsonl file. We also define the name of the .jsonl file in the log\\\\\\_handlers parameter def distil\\\\\\_summarization(text: str) -> GeneratedSummary: summary\\\\\\_chain: List\\\\\\[str\\\\\\] = summarize\\\\\\_article(text) return GeneratedSummary(summary=summary\\\\\\_chain\\\\\\[-1\\\\\\]) We add in an instructions.distil annotation so that we automatically capture the input and output of the function we'd like to fine-tune our model to output with open(\"train.csv\", \"r\") as file: reader = csv.reader(file) next(reader) # Skip the header for article, summary in reader: # Run Distillisation to generate the values distil\\\\\\_summarization(article) Rate Limiting We recommend running this script on a small subset of the dataset first to test you've got everything configured nicely. Don't forget to add in rate limiting error handling with tenacity and set the OPENAI\\\\\\_API\\\\\\_KEY shell environment variable before running any subsequent commands Creating Fine-Tuning Jobs Once we run this script, we'll have a new file called generated.jsonl in our local repository. Now all that's left is to run the command below to start fine-tuning your first model! instructor jobs create-from-file generated.jsonl Finetuning Reference Once the job is complete, all we need to do is to then change the annotation in the function call to distil\\\\\\_summarization in our original file above to start using our new model. @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard def distil\\\\\\_summarization(text: str) -> GeneratedSummary: summary\\\\\\_chain: List\\\\\\[str\\\\\\] = summarize\\\\\\_article(text) return GeneratedSummary(summary=summary\\\\\\_chain\\\\\\[-1\\\\\\]) With that, you've now got your own fine-tuned model ready to go and serve data in production. We've seen how Instructor can make your life easier, from fine-tuning to distillation. Results and Benchmarks We'll be comparing the following models in 3 ways using 20 articles that were not used for fine-tuning. Entity Density : This is entities per token, the higher the better for density. Latency : Time to last token generated in seconds Costs : Total cost to generate outputs - we break down the cost into training and inference costs for easy reference 3.5 Finetuned (n) This is a GPT 3.5 model that we fine-tuned on n examples. Each model was finetuned for 4-5 epochs ( This was automatically decided by the OpenAI scheduler ) GPT-4 (COD) This is a GPT4 model which we applied 3 rounds of Chain Of Density rewrites to generate a summary with using the methodology above GPT-3.5 (Vanilla) This is a GPT 3.5 model that we asked to generate entity-dense summaries which were concise. Summaries were generated in a single pass targetting about 80-90 tokens. Model Mean Latency (s) Mean Entity Density 3.5 Finetuned (20) 2.1 0.15 3.5 Finetuned (50) 2.1 0.14 3.5 Finetuned (76) 2.1 0.14 GPT-3.5 (Vanilla) 16.8 0.12 GPT-4 (COD) 49.5 0.15 Finetuning Datasets Using the OpenAI Usage Dashboard, we can calculate the cost of generating 20 summaries as seen below. Model Training Cost ($) Inference Cost ($) Tokens Used Total Cost ($) GPT-3.5 (Vanilla) - 0.20 51,162 0.2 3.5 Finetuned (20) 0.7 0.20 56,573 0.8 3.5 Finetuned (50) 1.4 0.17 49,057 1.3 3.5 Finetuned (76) 1.8 0.17 51,583 2.5 GPT-4 (COD) - 12.9 409,062 12.9 Here, we can see that GPT-4 has an approximate inference cost of 0.65 per summary while our finetuned models have an inference cost of 0.0091 per summary which is ~ 72x cheaper. Interestingly, the model finetuned with the least examples seems to outperform the others. While the reason for this is unknown, a few potential reasons could be that either we didn't train for sufficient epochs ( We chose the default 5 epochs ) or that the models started learning to imitate other behaviour such as more abstract writing styles from the larger variety of samples, resulting in a decrease in entity density. Conclusions Finetuning this iterative method was 20-40x faster while improving overall performance, resulting in massive efficiency gains by finetuning and distilling capabilities into specialized models. We've seen how Instructor can make your life easier, from data modeling to distillation and finetuning. If you enjoy the content or want to try out instructor check out the github and don't forget to give us a star! Continue reading 2023/11/02 1 min read AI Engineer Keynote: Pydantic is all you need Click here to watch the full talk Last month, I ventured back onto the speaking circuit at the inaugural AI Engineer Summit, sharing insights on leveraging Pydantic for effective prompt engineering. I dove deep into what is covered in our documentation and standard blog posts, I'd genuinely appreciate any feedback on the talk – every bit helps in refining the art. So, take a moment to check out the full talk here, and let's continue pushing the boundaries of what's possible. Continue reading 2023/10/23 10 min read Good LLM Validation is Just Good Validation What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here. Validation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like Pydantic and Instructor. We validate these outputs using a validation function which conforms to the structure seen below. def validation\\\\\\_function(value): if condition(value): raise ValueError(\"Value is not valid\") return mutation(value) What is Instructor? Instructor helps to ensure you get the exact response type you're looking for when using openai's function call api. Once you've defined the Pydantic model for your desired response, Instructor handles all the complicated logic in-between - from the parsing/validation of the response to the automatic retries for invalid responses. This means that we can build in validators 'for free' and have a clear separation of concerns between the prompt and the code that calls openai. from openai import OpenAI import instructor # pip install instructor from pydantic import BaseModel # This enables response\\\\\\_model keyword # from client.chat.completions.create client = instructor.patch(OpenAI()) To simplify your work with OpenAI models and streamline the extraction of Pydantic objects from prompts, we offer a patching mechanism for the ChatCompletion class. class UserDetail(BaseModel): name: str age: int user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] max\\\\\\_retries=3 Invalid responses that fail to be validated succesfully will trigger up to as many reattempts as you define. ) assert user.name == \"Jason\" As long as you pass in a response\\\\\\_model parameter to the ChatCompletion api call, the returned object will always be a validated Pydantic object. assert user.age == 25 In this post, we'll explore how to evolve from static, rule-based validation methods to dynamic, machine learning-driven ones. You'll learn to use Pydantic and Instructor to leverage language models and dive into advanced topics like content moderation, validating chain of thought reasoning, and contextual validation. Let's examine how these approaches with a example. Imagine that you run a software company who wants to ensure you never serve hateful and racist content. This isn't an easy job since the language around these topics change very quickly and frequently. Software 1.0: Introduction to Validations in Pydantic A simple method could be to compile a list of different words that are often associated with hate speech. For simplicity, let's assume that we've found that the words Steal and Rob are good predictors of hateful speech from our database. We can modify our validation structure above to accomodate this. This will throw an error if we pass in a string like Let's rob the bank! or We should steal from the supermarkets. Pydantic offers two approaches for this validation: using the field\\\\\\_validator decorator or the Annotated hints. Using field\\\\\\_validator decorator We can use the field\\\\\\_validator decorator to define a validator for a field in Pydantic. Here's a quick example of how we might be able to do so. from pydantic import BaseModel, ValidationError, field\\\\\\_validator from pydantic.fields import Field class UserMessage(BaseModel): message: str @field\\\\\\_validator('message') def message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words(cls, v: str) -> str: for word in v.split(): We split the sentence into its individual words and iterate through each of the words. We then try to see if any of these words are in our blacklist which in this case is just rob and steal if word.lower() in {'rob','steal'}: raise ValueError(f\"\\\\\\`{word}\\\\\\` was found in the message \\\\\\`{v}\\\\\\`\") return v try: UserMessage(message=\"This is a lovely day\") UserMessage(message=\"We should go and rob a bank\") except ValidationError as e: print(e) Since the message This is a lovely day does not have any blacklisted words, no errors are thrown. However, in the given example above, the validation fails for the message We should go and rob a bank due to the presence of the word rob and the corresponding error message is displayed. 1 validation error for UserMessage message Value error, \\\\\\`rob\\\\\\` was found in the message \\\\\\`We should go and rob a bank\\\\\\` \\\\\\[type=value\\\\\\_error, input\\\\\\_value='We should go and rob a bank', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Using Annotated Alternatively, you can use the Annotated function to perform the same validation. Here's an example where we utilise the same function we started with. from pydantic import BaseModel, ValidationError from typing import Annotated from pydantic.functional\\\\\\_validators import AfterValidator def message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words(value:str): for word in value.split(): if word.lower() in {'rob','steal'}: raise ValueError(f\"\\\\\\`{word}\\\\\\` was found in the message \\\\\\`{value}\\\\\\`\") return value class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(message\\\\\\_cannot\\\\\\_have\\\\\\_blacklisted\\\\\\_words)\\\\\\] try: UserMessage(message=\"This is a lovely day\") UserMessage(message=\"We should go and rob a bank\") except ValidationError as e: print(e) This code snippet achieves the same validation result. If the user message contains any of the words in the blacklist, a ValueError is raised and the corresponding error message is displayed. 1 validation error for UserMessage message Value error, \\\\\\`rob\\\\\\` was found in the message \\\\\\`We should go and rob a bank\\\\\\` \\\\\\[type=value\\\\\\_error, input\\\\\\_value='We should go and rob a bank', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Validation is a fundamental concept in software development and remains the same when applied to AI systems. Existing programming concepts should be leveraged when possible instead of introducing new terms and standards. The underlying principles of validation remain unchanged. Suppose now that we've gotten a new message - Violence is always acceptable, as long as we silence the witness. Our original validator wouldn't throw any errors when passed this new message since it uses neither the words rob or steal. However, it's clear that it is not a message which should be published. How can we ensure that our validation logic can adapt to new challenges? Software 3.0: Validation for LLMs or powered by LLMs Building upon the understanding of simple field validators, let's delve into probabilistic validation in software 3.0, (prompt engineering). We'll introduce an LLM-powered validator called llm\\\\\\_validator that uses a statement to verify the value. We can get around this by using the inbuilt llm\\\\\\_validator class from Instructor. from instructor import llm\\\\\\_validator from pydantic import BaseModel, ValidationError from typing import Annotated from pydantic.functional\\\\\\_validators import AfterValidator class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(llm\\\\\\_validator(\"don't say objectionable things\"))\\\\\\] try: UserMessage(message=\"Violence is always acceptable, as long as we silence the witness\") except ValidationError as e: print(e) This produces the following error message as seen below 1 validation error for UserMessage message Assertion failed, The statement promotes violence, which is objectionable. \\\\\\[type=assertion\\\\\\_error, input\\\\\\_value='Violence is always accep... we silence the witness', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/assertion\\\\\\_error The error message is generated by the language model (LLM) rather than the code itself, making it helpful for re-asking the model in a later section. To better understand this approach, let's see how to build an llm\\\\\\_validator from scratch. Creating Your Own Field Level llm\\\\\\_validator Building your own llm\\\\\\_validator can be a valuable exercise to get started with Instructor and create custom validators. Before we continue, let's review the anatomy of a validator: def validation\\\\\\_function(value): if condition(value): raise ValueError(\"Value is not valid\") return value As we can see, a validator is simply a function that takes in a value and returns a value. If the value is not valid, it raises a ValueError. We can represent this using the following structure: class Validation(BaseModel): is\\\\\\_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\") error\\\\\\_message: Optional\\\\\\[str\\\\\\] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\") Using this structure, we can implement the same logic as before and utilize Instructor to generate the validation. import instructor from openai import OpenAI # Enables \\\\\\`response\\\\\\_model\\\\\\` and \\\\\\`max\\\\\\_retries\\\\\\` parameters client = instructor.patch(OpenAI()) def validator(v): statement = \"don't say objectionable things\" resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Does \\\\\\`{v}\\\\\\` follow the rules: {statement}\", }, \\\\\\], # this comes from client = instructor.patch(OpenAI()) response\\\\\\_model=Validation, The new parameter of response\\\\\\_model comes from client = instructor.patch(OpenAI()) and does not exist in the original OpenAI SDK. This allows us to pass in the Pydantic model that we want as a response. ) if not resp.is\\\\\\_valid: raise ValueError(resp.error\\\\\\_message) return v Now we can use this validator in the same way we used the llm\\\\\\_validator from Instructor. class UserMessage(BaseModel): message: Annotated\\\\\\[str, AfterValidator(validator)\\\\\\] Writing more complex validations Validating Chain of Thought A popular way of prompting large language models nowadays is known as chain of thought. This involves getting a model to generate reasons and explanations for an answer to a prompt. We can utilise Pydantic and Instructor to perform a validation to check of the reasoning is reasonable, given both the answer and the chain of thought. To do this we can't build a field validator since we need to access multiple fields in the model. Instead we can use a model validator. def validate\\\\\\_chain\\\\\\_of\\\\\\_thought(values): chain\\\\\\_of\\\\\\_thought = values\\\\\\[\"chain\\\\\\_of\\\\\\_thought\"\\\\\\] answer = values\\\\\\[\"answer\"\\\\\\] resp = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\", }, { \"role\": \"user\", \"content\": f\"Verify that \\\\\\`{answer}\\\\\\` follows the chain of thought: {chain\\\\\\_of\\\\\\_thought}\", }, \\\\\\], # this comes from client = instructor.patch(OpenAI()) response\\\\\\_model=Validation, ) if not resp.is\\\\\\_valid: raise ValueError(resp.error\\\\\\_message) return values We can then take advantage of the model\\\\\\_validator decorator to perform a validation on a subset of the model's data. We're defining a model validator here which runs before Pydantic parses the input into its respective fields. That's why we have a before keyword used in the model\\\\\\_validator class. from pydantic import BaseModel, model\\\\\\_validator class AIResponse(BaseModel): chain\\\\\\_of\\\\\\_thought: str answer: str @model\\\\\\_validator(mode='before') @classmethod def chain\\\\\\_of\\\\\\_thought\\\\\\_makes\\\\\\_sense(cls, data: Any) -> Any: # here we assume data is the dict representation of the model # since we use 'before' mode. return validate\\\\\\_chain\\\\\\_of\\\\\\_thought(data) Now, when you create a AIResponse instance, the chain\\\\\\_of\\\\\\_thought\\\\\\_makes\\\\\\_sense validator will be invoked. Here's an example: try: resp = AIResponse( chain\\\\\\_of\\\\\\_thought=\"1 + 1 = 2\", answer=\"The meaning of life is 42\" ) except ValidationError as e: print(e) If we create a AIResponse instance with an answer that does not follow the chain of thought, we will get an error. 1 validation error for AIResponse Value error, The statement 'The meaning of life is 42' does not follow the chain of thought: 1 + 1 = 2. \\\\\\[type=value\\\\\\_error, input\\\\\\_value={'chain\\\\\\_of\\\\\\_thought': '1 +... meaning of life is 42'}, input\\\\\\_type=dict\\\\\\] Validating Citations From Original Text Let's see a more concrete example. Let's say that we've asked our model a question about some text source and we want to validate that the generated answer is supported by the source. This would allow us to minimize hallucinations and prevent statements that are not backed by the original text. While we could verify this by looking up the original source manually, a more scalable approach is to use a validator to do this automatically. We can pass in additional context to our validation functions using the model\\\\\\_validate function in Pydantic so that our models have more information to work with when performing validation. This context is a normal python dictionary and can be accessed inside the info argument in our validator functions. from pydantic import ValidationInfo,BaseModel,field\\\\\\_validator class AnswerWithCitation(BaseModel): answer: str citation: str @field\\\\\\_validator('citation') @classmethod def citation\\\\\\_exists(cls, v: str, info: ValidationInfo): This info object corresponds to the value of context that we pass into the model\\\\\\_validate function as seen below. context = info.context if context: context = context.get('text\\\\\\_chunk') if v not in context: raise ValueError(f\"Citation \\\\\\`{v}\\\\\\` not found in text chunks\") return v We can then take our original example and test it against our new model try: AnswerWithCitation.model\\\\\\_validate( {\"answer\": \"Jason is a cool guy\", \"citation\": \"Jason is cool\"}, context={\"text\\\\\\_chunk\": \"Jason is just a guy\"}, This context object is just a normal python dictionary and can take in and store any arbitrary values ) except ValidationError as e: print(e) This in turn generates the following error since Jason is cool does not exist in the text Jason is just a guy. 1 validation error for AnswerWithCitation citation Value error, Citation \\\\\\`Jason is cool\\\\\\` not found in text chunks \\\\\\[type=value\\\\\\_error, input\\\\\\_value='Jason is cool', input\\\\\\_type=str\\\\\\] For further information visit https://errors.pydantic.dev/2.4/v/value\\\\\\_error Putting it all together with client = instructor.patch(OpenAI()) To pass this context from the client.chat.completions.create call, client = instructor.patch(OpenAI()) also passes the validation\\\\\\_context, which will be accessible from the info argument in the decorated validator functions. from openai import OpenAI import instructor # Enables \\\\\\`response\\\\\\_model\\\\\\` and \\\\\\`max\\\\\\_retries\\\\\\` parameters client = instructor.patch(OpenAI()) def answer\\\\\\_question(question:str, text\\\\\\_chunk: str) -> AnswerWithCitation: return client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"user\", \"content\": f\"Answer the question: {question} with the text chunk: {text\\\\\\_chunk}\", }, \\\\\\], response\\\\\\_model=AnswerWithCitation, validation\\\\\\_context={\"text\\\\\\_chunk\": text\\\\\\_chunk}, ) Error Handling and Re-Asking Validators can ensure certain properties of the outputs by throwing errors, in an AI system we can use the errors and allow language model to self correct. The by running client = instructor.patch(OpenAI()) not only do we add response\\\\\\_model and validation\\\\\\_context it also allows you to use the max\\\\\\_retries parameter to specify the number of times to try and self correct. This approach provides a layer of defense against two types of bad outputs: Pydantic Validation Errors (code or LLM-based) JSON Decoding Errors (when the model returns an incorrect response) Define the Response Model with Validators To keep things simple lets assume we have a model that returns a UserModel object. We can define the response model using Pydantic and add a field validator to ensure that the name is in uppercase. from pydantic import BaseModel, field\\\\\\_validator class UserModel(BaseModel): name: str age: int @field\\\\\\_validator(\"name\") @classmethod def validate\\\\\\_name(cls, v): if v.upper() != v: raise ValueError(\"Name must be in uppercase.\") return v This is where the max\\\\\\_retries parameter comes in. It allows the model to self correct and retry the prompt using the error message rather than the prompt. model = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"}, \\\\\\], # Powered by client = instructor.patch(OpenAI()) response\\\\\\_model=UserModel, max\\\\\\_retries=2, ) assert model.name == \"JASON\" In this example, even though there is no code explicitly transforming the name to uppercase, the model is able to correct the output. Conclusion From the simplicity of Pydantic and Instructor to the dynamic validation capabilities of LLMs, the landscape of validation is changing but without needing to introduce new contepts. It's clear that the future of validation is not just about preventing bad data but about allowing llms to understand the data and correcting it. If you enjoy the content or want to try out Instructor please check out the github and give us a star! Continue reading 2023/10/17 4 min read Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation Introduction Get ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the instructor.instructions streamlines this process, making the task you want to distil more efficient and powerful while preserving its original functionality and backwards compatibility. If you want to see the full example checkout examples/distillation Why use Instructor? Imagine you're developing a backend service that uses a mix old and new school ML practises, it may involve pipelines with multiple function calls, validations, and data processing. Sounds cumbersome, right? That's where Instructor comes in. It simplifies complex procedures, making them more efficient and easier to manage by adding a decorator to your function that will automatically generate a dataset for fine-tuning and help you swap out the function implementation. Quick Start: How to Use Instructor's Distillation Feature Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file. import logging import random from pydantic import BaseModel from instructor import Instructions # pip install instructor # Logging setup logging.basicConfig(level=logging.INFO) instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", finetune\\\\\\_format=\"messages\", # log handler is used to save the data to a file # you can imagine saving it to a database or other storage # based on your needs! log\\\\\\_handlers=\\\\\\[logging.FileHandler(\"math\\\\\\_finetunes.jsonl\")\\\\\\] ) class Multiply(BaseModel): a: int b: int result: int # Define a function with distillation # The decorator will automatically generate a dataset for fine-tuning # They must return a pydantic model to leverage function calling @instructions.distil def fn(a: int, b: int) -> Multiply: resp = a \\\\\\* b return Multiply(a=a, b=b, result=resp) # Generate some data for \\\\\\_ in range(10): a = random.randint(100, 999) b = random.randint(100, 999) print(fn(a, b)) The Intricacies of Fine-tuning Language Models Fine-tuning isn't just about writing a function like def f(a, b): return a \\\\\\* b. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this. Why Instructor and Distillation are Game Changers The library offers two main benefits: Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code. Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions. Role of Instructor in Simplifying Fine-Tuning The from instructor import Instructions feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior. Logging Output and Running a Finetune Here's how the logging output would look: { \"messages\": \\\\\\[ {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'}, {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'}, {\"role\": \"assistant\", \"function\\\\\\_call\": { \"name\": \"Multiply\", \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}' } } \\\\\\], \"functions\": \\\\\\[ {\"name\": \"Multiply\", \"description\": \"Correctly extracted \\\\\\`Multiply\\\\\\`...\"} \\\\\\] } Run a finetune like this: Don't forget to set your OpenAI Key as an environment variable All of the instructor jobs commands assume you've set an environment variable of OPENAI\\\\\\_API\\\\\\_KEY in your shell. You can set this by running the command export OPENAI\\\\\\_API\\\\\\_KEY= in your shell instructor jobs create-from-file math\\\\\\_finetunes.jsonl Next Steps and Future Plans Here's a sneak peek of what I'm planning: from instructor import Instructions, patch patch() Don't forget to run the patch() command that we provide with the Instructor package. This helps automatically serialize the content back into the \\\\\\`Pydantic\\\\\\`\\\\\\` model that we're looking for. class Multiply(BaseModel): a: int b: int result: int instructions = Instructions( name=\"three\\\\\\_digit\\\\\\_multiply\", ) @instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\") Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard def fn(a: int, b: int) -> Multiply: resp = a + b return Multiply(a=a, b=b, result=resp) With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation. Conclusion We've seen how Instructor can make your life easier, from fine-tuning to distillation. Now if you're thinking wow, I'd love a backend service to do this for continously, you're in luck! Please check out the survey at useinstructor.com and let us know who you are. If you enjoy the content or want to try out instructor please check out the github and give us a star! Continue reading 2023/09/17 7 min read RAG is more than just embedding search With the advent of large language models (LLM), retrival augmented generation (RAG) has become a hot topic. However throught the past year of helping startups integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware. What is RAG? Retrival augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized. Simple RAG that embedded the user query and makes a search. So let's kick things off by examining what I like to call the 'Dumb' RAG Model—a basic setup that's more common than you'd think. The 'Dumb' RAG Model When you ask a question like, \"what is the capital of France?\" The RAG 'dumb' model embeds the query and searches in some unopinonated search endpoint. Limited to a single method API like search(query: str) -> List\\\\\\[str\\\\\\]. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings. Why is this a problem? Query-Document Mismatch: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation! Monolithic Search Backend: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more. Limitation of text search: Restricts complex queries to a single string ({query: str}), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, asking what problems did we fix last week cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week. Limited ability to plan: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context its able to plan a suite of queries to execute to return the best results. Now let's dive into how we can make it smarter with query understanding. This is where things get interesting. Improving the RAG Model with Query Understanding Shoutouts Much of this work has been inspired by / done in collab with a few of my clients at new.computer, Metaphor Systems, and Naro, go check them out! Ultimately what you want to deploy is a system that understands how to take the query and rewrite it to improve precision and recall. Query Understanding system routes to multiple search backends. Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems. Whats instructor? Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling API. Widespread Adoption: Pydantic is a popular tool among Python developers. Simplicity: Pydantic allows model definition in Python. Framework Compatibility: Many Python frameworks already use Pydantic. Case Study 1: Metaphor Systems Take Metaphor Systems, which turns natural language queries into their custom search-optimized query. If you take a look web UI you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using a language model, and turns it into a fully specified metaphor systems query. Metaphor Systems UI If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. It's actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library class DateRange(BaseModel): start: datetime.date end: datetime.date class MetaphorQuery(BaseModel): rewritten\\\\\\_query: str published\\\\\\_daterange: DateRange domains\\\\\\_allow\\\\\\_list: List\\\\\\[str\\\\\\] async def execute(): return await metaphor.search(...) Note how we model a rewritten query, range of published dates, and a list of domains to search in. This powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works. import instructor from openai import OpenAI # Enables response\\\\\\_model in the openai client client = instructor.patch(OpenAI()) query = client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=MetaphorQuery, messages=\\\\\\[ { \"role\": \"system\", \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\" }, { \"role\": \"user\", \"content\": \"What are some recent developments in AI?\" } \\\\\\], ) Example Output { \"rewritten\\\\\\_query\": \"novel developments advancements ai artificial intelligence machine learning\", \"published\\\\\\_daterange\": { \"start\": \"2023-09-17\", \"end\": \"2021-06-17\" }, \"domains\\\\\\_allow\\\\\\_list\": \\\\\\[\"arxiv.org\"\\\\\\] } This isn't just about adding some date ranges. It's about nuanced, tailored searches, that are deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features. class DateRange(BaseModel): start: datetime.date end: datetime.date chain\\\\\\_of\\\\\\_thought: str = Field( None, description=\"Think step by step to plan what is the best time range to search in\" ) Now, let's see how this approach can help model an agent like personal assistant. Case Study 2: Personal Assistant Another great example of this multiple dispatch pattern is a personal assistant. You might ask, \"What do I have today?\", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts. class ClientSource(enum.Enum): GMAIL = \"gmail\" CALENDAR = \"calendar\" class SearchClient(BaseModel): query: str keywords: List\\\\\\[str\\\\\\] email: str source: ClientSource start\\\\\\_date: datetime.date end\\\\\\_date: datetime.date async def execute(self) -> str: if self.source == ClientSource.GMAIL: ... elif self.source == ClientSource.CALENDAR: ... class Retrival(BaseModel): queries: List\\\\\\[SearchClient\\\\\\] async def execute(self) -> str: return await asyncio.gather(\\\\\\*\\\\\\[query.execute() for query in self.queries\\\\\\]) Now we can call this with a simple query like \"What do I have today?\" and it will try to async dispatch to the correct backend. It's still important to prompt the language model well, but we'll leave that for another day. import instructor from openai import OpenAI # Enables response\\\\\\_model in the openai client client = instructor.patch(OpenAI()) retrival = client.chat.completions.create( model=\"gpt-4\", response\\\\\\_model=Retrival, messages=\\\\\\[ {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"}, {\"role\": \"user\", \"content\": \"What do I have today?\"} \\\\\\], ) Example Output { \"queries\": \\\\\\[ { \"query\": None, \"keywords\": None, \"email\": \"jason@example.com\", \"source\": \"gmail\", \"start\\\\\\_date\": \"2023-09-17\", \"end\\\\\\_date\": None }, { \"query\": None, \"keywords\": \\\\\\[\"meeting\", \"call\", \"zoom\"\\\\\\]\\\\\\]\\\\\\], \"email\": \"jason@example.com\", \"source\": \"calendar\", \"start\\\\\\_date\": \"2023-09-17\", \"end\\\\\\_date\": None } \\\\\\] } Notice that we have a list of queries that route to different search backends (email and calendar). We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well. Perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app. Can I used framework X? I get this question a lot, but it's just code. Within these dispatchs you can do whatever you want. You can use input() to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information. The sky is the limit. Both of these examples showcase how both search providors and consumers can use instructor to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend. Conclusion This isnt about fancy embedding tricks, it's just plain old information retrival and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place. What's Next? Here I want to show that \\\\\\`instructor\\\\\\`\\\\\\` isn’t just about data extraction. It’s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning — the untapped goldmine is skilled use of tools and APIs. If you enjoy the content or want to try out instructor please check out the github and give us a star! Continue reading 1 2 Back to top Previous Core Library Next 2023 Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Why use Instructor? - Instructor",
    "url": "https://jxnl.github.io/instructor/why/",
    "html": "Skip to content Instructor Why use Instructor? Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Introduction Why use Instructor? Help with Instructor Installation Contributing Table of contents Understanding the patch Step 1: Patch the client Step 2: Define the Pydantic Model Step 3: Extract Understanding Validation Self Correcting on Validation Error Why use Instructor?¶ Why use Pydantic? Our instructor.patch for the OpenAI class introduces three key enhancements: Response Mode: Specify a Pydantic model to streamline data extraction. Max Retries: Set your desired number of retry attempts for requests. Validation Context: Provide a context object for enhanced validator access. A Glimpse into Instructor's Capabilities Using Validators Learn more about validators checkout our blog post Good llm validation is just good validation With Instructor, your code becomes more efficient and readable. Here’s a quick peek: Understanding the patch¶ Lets go over the patch function. And see how we can leverage it to make use of instructor Step 1: Patch the client¶ First, import the required libraries and apply the patch function to the OpenAI module. This exposes new functionality with the response\\\\\\_model parameter. import instructor from openai import OpenAI from pydantic import BaseModel # This enables response\\\\\\_model keyword # from client.chat.completions.create client = instructor.patch(OpenAI()) Step 2: Define the Pydantic Model¶ Create a Pydantic model to define the structure of the data you want to extract. This model will map directly to the information in the prompt. from pydantic import BaseModel class UserDetail(BaseModel): name: str age: int Step 3: Extract¶ Use the client.chat.completions.create method to send a prompt and extract the data into the Pydantic object. The response\\\\\\_model parameter specifies the Pydantic model to use for extraction. Its helpful to annotate the variable with the type of the response model. which will help your IDE provide autocomplete and spell check. user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] ) assert user.name == \"Jason\" assert user.age == 25 Understanding Validation¶ Validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error. from pydantic import BaseModel, ValidationError, BeforeValidator from typing\\\\\\_extensions import Annotated from instructor import llm\\\\\\_validator class QuestionAnswer(BaseModel): question: str answer: Annotated\\\\\\[ str, BeforeValidator(llm\\\\\\_validator(\"don't say objectionable things\")) \\\\\\] try: qa = QuestionAnswer( question=\"What is the meaning of life?\", answer=\"The meaning of life is to be evil and steal\", ) except ValidationError as e: print(e) Its important to not here that the error message is generated by the LLM, not the code, so it'll be helpful for re asking the model. 1 validation error for QuestionAnswer answer Assertion failed, The statement is objectionable. (type=assertion\\\\\\_error) Self Correcting on Validation Error¶ Here, the UserDetails model is passed as the response\\\\\\_model, and max\\\\\\_retries is set to 2. import instructor from openai import OpenAI from pydantic import BaseModel, field\\\\\\_validator # Apply the patch to the OpenAI client client = instructor.patch(OpenAI()) class UserDetails(BaseModel): name: str age: int @field\\\\\\_validator(\"name\") @classmethod def validate\\\\\\_name(cls, v): if v.upper() != v: raise ValueError(\"Name must be in uppercase.\") return v model = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetails, max\\\\\\_retries=2, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"}, \\\\\\], ) assert model.name == \"JASON\" As you can see, we've baked in a self correcting mechanism into the model. This is a powerful way to make your models more robust and less brittle without include a lot of extra code or prompt. Was this page helpful? Back to top Previous Welcome To Instructor Next Help with Instructor Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Core Library - Instructor",
    "url": "https://jxnl.github.io/instructor/api/",
    "html": "Skip to content Instructor Core Library Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog API Reference Core Library Table of contents patch apatch() dump\\\\\\_message() is\\\\\\_async() patch() process\\\\\\_response() process\\\\\\_response\\\\\\_async() validators Validator llm\\\\\\_validator() Usage openai\\\\\\_moderation() citation CitationMixin Usage Result validate\\\\\\_sources() multitask MultiTask() Usage Result maybe MaybeBase Maybe() Usage Result function\\\\\\_calls Mode OpenAISchema Usage Result openai\\\\\\_schema from\\\\\\_response() from\\\\\\_response\\\\\\_async() API Reference¶ apatch(client, mode=Mode.FUNCTIONS) ¶ No longer necessary, use patch instead. Patch the client.chat.completions.create method Enables the following features: response\\\\\\_model parameter to parse the response from OpenAI's API max\\\\\\_retries parameter to retry the function if the response is not valid validation\\\\\\_context parameter to validate the response using the pydantic model strict parameter to use strict json parsing Source code in instructor/patch.py dump\\\\\\_message(message) ¶ Dumps a message to a dict, to be returned to the OpenAI API. Workaround for an issue with the OpenAI API, where the tool\\\\\\_calls field isn't allowed to be present in requests if it isn't used. Source code in instructor/patch.py is\\\\\\_async(func) ¶ Returns true if the callable is async, accounting for wrapped callables Source code in instructor/patch.py patch(client, mode=Mode.FUNCTIONS) ¶ Patch the client.chat.completions.create method Enables the following features: response\\\\\\_model parameter to parse the response from OpenAI's API max\\\\\\_retries parameter to retry the function if the response is not valid validation\\\\\\_context parameter to validate the response using the pydantic model strict parameter to use strict json parsing Source code in instructor/patch.py process\\\\\\_response(response, \\\\\\*, response\\\\\\_model, stream, validation\\\\\\_context=None, strict=None, mode=Mode.FUNCTIONS) ¶ Processes a OpenAI response with the response model, if available. It can use validation\\\\\\_context and strict to validate the response via the pydantic model Parameters: Name Type Description Default response ChatCompletion The response from OpenAI's API required response\\\\\\_model BaseModel The response model to use for parsing the response required stream bool Whether the response is a stream required validation\\\\\\_context dict The validation context to use for validating the response. Defaults to None. None strict bool Whether to use strict json parsing. Defaults to None. None Source code in instructor/patch.py process\\\\\\_response\\\\\\_async(response, \\\\\\*, response\\\\\\_model, stream, validation\\\\\\_context=None, strict=None, mode=Mode.FUNCTIONS) async ¶ Processes a OpenAI response with the response model, if available. It can use validation\\\\\\_context and strict to validate the response via the pydantic model Parameters: Name Type Description Default response ChatCompletion The response from OpenAI's API required response\\\\\\_model BaseModel The response model to use for parsing the response required stream bool Whether the response is a stream required validation\\\\\\_context dict The validation context to use for validating the response. Defaults to None. None strict bool Whether to use strict json parsing. Defaults to None. None Source code in instructor/patch.py Validator ¶ Bases: OpenAISchema Validate if an attribute is correct and if not, return a new value with an error message Source code in instructor/dsl/validators.py llm\\\\\\_validator(statement, allow\\\\\\_override=False, model='gpt-3.5-turbo', temperature=0, openai\\\\\\_client=None) ¶ Create a validator that uses the LLM to validate an attribute Usage¶ from instructor import llm\\\\\\_validator from pydantic import BaseModel, Field, field\\\\\\_validator class User(BaseModel): name: str = Annotated\\\\\\[str, llm\\\\\\_validator(\"The name must be a full name all lowercase\") age: int = Field(description=\"The age of the person\") try: user = User(name=\"Jason Liu\", age=20) except ValidationError as e: print(e) 1 validation error for User name The name is valid but not all lowercase (type=value\\\\\\_error.llm\\\\\\_validator) Note that there, the error message is written by the LLM, and the error type is value\\\\\\_error.llm\\\\\\_validator. Parameters: Name Type Description Default statement str The statement to validate required model str The LLM to use for validation (default: \"gpt-3.5-turbo-0613\") 'gpt-3.5-turbo' temperature float The temperature to use for the LLM (default: 0) 0 openai\\\\\\_client OpenAI The OpenAI client to use (default: None) None Source code in instructor/dsl/validators.py openai\\\\\\_moderation(client=None) ¶ Validates a message using OpenAI moderation model. Should only be used for monitoring inputs and outputs of OpenAI APIs Other use cases are disallowed as per: https://platform.openai.com/docs/guides/moderation/overview Example: from instructor import OpenAIModeration class Response(BaseModel): message: Annotated\\\\\\[str, AfterValidator(OpenAIModeration(openai\\\\\\_client=client))\\\\\\] Response(message=\"I hate you\") ValidationError: 1 validation error for Response message Value error, \\\\\\`I hate you.\\\\\\` was flagged for \\\\\\['harassment'\\\\\\] \\\\\\[type=value\\\\\\_error, input\\\\\\_value='I hate you.', input\\\\\\_type=str\\\\\\] client (OpenAI): The OpenAI client to use, must be sync (default: None) Source code in instructor/dsl/validators.py CitationMixin ¶ Bases: BaseModel Helpful mixing that can use validation\\\\\\_context={\"context\": context} in from\\\\\\_response to find the span of the substring\\\\\\_phrase in the context. Usage¶ from pydantic import BaseModel, Field from instructor import CitationMixin class User(BaseModel): name: str = Field(description=\"The name of the person\") age: int = Field(description=\"The age of the person\") role: str = Field(description=\"The role of the person\") context = \"Betty was a student. Jason was a student. Jason is 20 years old\" user = openai.ChatCompletion.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[ { \"role\": \"user\", \"content\": \"Extract jason from {context}\", }, response\\\\\\_model=User, validation\\\\\\_context={\"context\": context}, \\\\\\] ) for quote in user.substring\\\\\\_quotes: assert quote in context print(user.model\\\\\\_dump()) Result¶ { \"name\": \"Jason Liu\", \"age\": 20, \"role\": \"student\", \"substring\\\\\\_quotes\": \\\\\\[ \"Jason was a student\", \"Jason is 20 years old\", \\\\\\] } Source code in instructor/dsl/citation.py validate\\\\\\_sources(info) ¶ For each substring\\\\\\_phrase, find the span of the substring\\\\\\_phrase in the context. If the span is not found, remove the substring\\\\\\_phrase from the list. Source code in instructor/dsl/citation.py MultiTask(subtask\\\\\\_class, name=None, description=None) ¶ Dynamically create a MultiTask OpenAISchema that can be used to segment multiple tasks given a base class. This creates class that can be used to create a toolkit for a specific task, names and descriptions are automatically generated. However they can be overridden. Usage¶ from pydantic import BaseModel, Field from instructor import MultiTask class User(BaseModel): name: str = Field(description=\"The name of the person\") age: int = Field(description=\"The age of the person\") role: str = Field(description=\"The role of the person\") MultiUser = MultiTask(User) Result¶ class MultiUser(OpenAISchema, MultiTaskBase): tasks: List\\\\\\[User\\\\\\] = Field( default\\\\\\_factory=list, repr=False, description=\"Correctly segmented list of \\\\\\`User\\\\\\` tasks\", ) @classmethod def from\\\\\\_streaming\\\\\\_response(cls, completion) -> Generator\\\\\\[User\\\\\\]: ''' Parse the streaming response from OpenAI and yield a \\\\\\`User\\\\\\` object for each task in the response ''' json\\\\\\_chunks = cls.extract\\\\\\_json(completion) yield from cls.tasks\\\\\\_from\\\\\\_chunks(json\\\\\\_chunks) Parameters: Name Type Description Default subtask\\\\\\_class Type\\\\\\[OpenAISchema\\\\\\] The base class to use for the MultiTask required name Optional\\\\\\[str\\\\\\] The name of the MultiTask class, if None then the name of the subtask class is used as Multi{subtask\\\\\\_class.\\\\\\_\\\\\\_name\\\\\\_\\\\\\_} None description Optional\\\\\\[str\\\\\\] The description of the MultiTask class, if None then the description is set to Correct segmentation of{subtask\\\\\\_class.name}tasks None Returns: Name Type Description schema OpenAISchema A new class that can be used to segment multiple tasks Source code in instructor/dsl/multitask.py MaybeBase ¶ Bases: BaseModel Extract a result from a model, if any, otherwise set the error and message fields. Source code in instructor/dsl/maybe.py Maybe(model) ¶ Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for result, error, and message for sitatations where the data may not be present in the context. Usage¶ from pydantic import BaseModel, Field from instructor import Maybe class User(BaseModel): name: str = Field(description=\"The name of the person\") age: int = Field(description=\"The age of the person\") role: str = Field(description=\"The role of the person\") MaybeUser = Maybe(User) Result¶ class MaybeUser(BaseModel): result: Optional\\\\\\[User\\\\\\] error: bool = Field(default=False) message: Optional\\\\\\[str\\\\\\] def \\\\\\_\\\\\\_bool\\\\\\_\\\\\\_(self): return self.result is not None Parameters: Name Type Description Default model Type\\\\\\[BaseModel\\\\\\] The Pydantic model to wrap with Maybe. required Returns: Name Type Description MaybeModel Type\\\\\\[BaseModel\\\\\\] A new Pydantic model that includes fields for result, error, and message. Source code in instructor/dsl/maybe.py Mode ¶ Bases: Enum The mode to use for patching the client Source code in instructor/function\\\\\\_calls.py OpenAISchema ¶ Bases: BaseModel Augments a Pydantic model with OpenAI's schema for function calling This class augments a Pydantic model with OpenAI's schema for function calling. The schema is generated from the model's signature and docstring. The schema can be used to validate the response from OpenAI's API and extract the function call. Usage¶ from instructor import OpenAISchema class User(OpenAISchema): name: str age: int completion = openai.ChatCompletion.create( model=\"gpt-3.5-turbo\", messages=\\\\\\[{ \"content\": \"Jason is 20 years old\", \"role\": \"user\" }\\\\\\], functions=\\\\\\[User.openai\\\\\\_schema\\\\\\], function\\\\\\_call={\"name\": User.openai\\\\\\_schema\\\\\\[\"name\"\\\\\\]}, ) user = User.from\\\\\\_response(completion) print(user.model\\\\\\_dump()) Result¶ { \"name\": \"Jason Liu\", \"age\": 20, } Source code in instructor/function\\\\\\_calls.py openai\\\\\\_schema classmethod property ¶ Return the schema in the format of OpenAI's schema as jsonschema Note Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt. Returns: Name Type Description model\\\\\\_json\\\\\\_schema dict A dictionary in the format of OpenAI's schema as jsonschema from\\\\\\_response(completion, validation\\\\\\_context=None, strict=None, mode=Mode.FUNCTIONS, stream\\\\\\_multitask=False) classmethod ¶ Execute the function from the response of an openai chat completion Parameters: Name Type Description Default completion ChatCompletion The response from an openai chat completion required throw\\\\\\_error bool Whether to throw an error if the function call is not detected required validation\\\\\\_context dict The validation context to use for validating the response None strict bool Whether to use strict json parsing None mode Mode The openai completion mode FUNCTIONS stream\\\\\\_multitask bool Whether to stream a multitask response False Returns: Name Type Description cls OpenAISchema An instance of the class Source code in instructor/function\\\\\\_calls.py from\\\\\\_response\\\\\\_async(completion, validation\\\\\\_context=None, strict=None, mode=Mode.FUNCTIONS, stream\\\\\\_multitask=False) async classmethod ¶ Execute the function from the response of an openai chat completion Parameters: Name Type Description Default completion ChatCompletion The response from an openai chat completion required validation\\\\\\_context dict The validation context to use for validating the response None strict bool Whether to use strict json parsing None mode Mode The openai completion mode FUNCTIONS stream\\\\\\_multitask bool Whether to stream a multitask response False Returns: Name Type Description cls OpenAISchema An instance of the class Source code in instructor/function\\\\\\_calls.py Was this page helpful? Back to top Previous Usage Tracking Next Welcome to the Instructor Blog Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Introduction - Instructor",
    "url": "https://jxnl.github.io/instructor/cli/",
    "html": "Skip to content Instructor Introduction Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog CLI Reference Finetuning GPT-3.5 Usage Tracking Table of contents Quick Start Installation & Setup Features Support & Contribution Instructor CLI¶ Welcome to the Instructor Command-Line Interface (CLI), a tool designed to ease your experience with the OpenAI API. Whether it's tracking your API usage or fine-tuning your models, Instructor CLI is your go-to utility. Quick Start¶ First things first: make sure your OpenAI API key is set as an environment variable. The CLI will use this for authenticating your requests to OpenAI's services. You can set the API key in your terminal as follows: export OPENAI\\\\\\_API\\\\\\_KEY=\"your-api-key-here\" Installation & Setup¶ pip install instructor Features¶ API Usage Monitoring: Keep tabs on your API usage right from the terminal. Track token counts, total requests, and even calculate the costs. To learn more, consult the Usage Guide. Model Fine-Tuning: Optimize your models to meet your specific requirements using our fine-tuning app. For more details, check out the Fine-Tuning Guide. Support & Contribution¶ Need help or want to contribute? Visit our GitHub Repository Was this page helpful? Back to top Previous Open Source Next Finetuning GPT-3.5 Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Overview - Instructor",
    "url": "https://jxnl.github.io/instructor/examples/",
    "html": "Skip to content Instructor Overview Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Cookbook Text Classification Self Critique Image Extracting Tables Image to Ad Copy Moderation Citations Knowledge Graph Entity Resolution Search Queries Query Decomposition Recursive Schemas Table Extraction Action Item and Dependency Mapping Multi-File Code Generation PII Data Sanitization Open Source Table of contents Quick Links Function Calls by Example¶ Quick Links¶ How are single and multi-label classifications done using enums? How is AI self-assessment implemented with llm\\\\\\_validator? How are exact citations retrieved using regular expressions and smart prompting? How are search queries segmented through function calling and multi-task definitions? How are knowledge graphs generated from questions? How are complex queries decomposed into subqueries in a single request? How are entities extracted and resolved from documents? How are recursive schemas implemented and understood? How are tables extracted automatically from textual data? How is multi-file code generation accomplished? How is Personally Identifiable Information sanitized from documents? How are action items and dependencies generated from transcripts? How to enable OpenAI's moderation How to extract tables from images How to generate advertising copy from image inputs Explore more! Was this page helpful? Back to top Previous Type Adapter Next Text Classification Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Philosophy - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/philosophy/",
    "html": "Skip to content Instructor Philosophy Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Concepts Philosophy Models Fields Missing Patching Streaming FastAPI Caching Validators Distillation Types Union Alias Type Adapter Table of contents The Bridge to Object-Oriented Programming The zen of instructor My Goals Philosophy¶ The instructor values simplicity and flexibility in leveraging language models (LLMs). It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions. Let Pydantic do the heavy lifting. “Simplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.” — Edsger Dijkstra The Bridge to Object-Oriented Programming¶ instructor acts as a bridge converting text-based LLM interactions into a familiar object-oriented format. Its integration with Pydantic provides type hints, runtime validation, and robust IDE support; love and supported by many in the Python ecosystem. By treating LLMs as callable functions returning typed objects, instructor makes language models backwards compatible with code, making them practical for everyday use while being complex enough for advanced applications. The zen of instructor¶ Maintain the flexibility and power of Python, without unnecessary constraints. Begin with a function and a return type hint – simplicity is key. With my experience maintaining a large enterprize framework at my previous job over many years I've learned that the goal of a making a useful framework is minimizing regret, both for the author and hopefully for the user. Define a Schema class StructuredData(BaseModel): Define validators and methods on your schema. Encapsulate all your LLM logic into a function def extract(a) -> StructuredData: Define typed computations against your data with def compute(data: StructuredData): or call methods on your schema data.compute() It should be that simple. My Goals¶ The goal for the library, documentation, and blog, is to help you be a better python programmer and as a result a better AI engineer. The library is a result of my desire for simplicity. The library should help maintain simplicity in your codebase. I won't try to write prompts for you, I don't try to create indirections or abstractions that make it hard to debug in the future Please note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit me up on twitter Cheers! Was this page helpful? Back to top Previous Tips Next Models Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Tips - Instructor",
    "url": "https://jxnl.github.io/instructor/concepts/prompting/",
    "html": "Skip to content Instructor Tips Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Tips Table of contents Modular Chain of Thought Utilize Optional Attributes Handling Errors Within Function Calls Simplification with the Maybe Pattern Tips for Enumerations Reiterate Long Instructions Handle Arbitrary Properties Limiting the Length of Lists Advanced Arbitrary Properties Defining Relationships Between Entities Reusing Components with Different Contexts General Tips for Prompt Engineering¶ The overarching theme of using Instructor and Pydantic for function calling is to make the models as self-descriptive, modular, and flexible as possible, while maintaining data integrity and ease of use. Modularity: Design self-contained components for reuse. Self-Description: Use Pydantic's Field for clear field descriptions. Optionality: Use Python's Optional type for nullable fields and set sensible defaults. Standardization: Employ enumerations for fields with a fixed set of values; include a fallback option. Dynamic Data: Use key-value pairs for arbitrary properties and limit list lengths. Entity Relationships: Define explicit identifiers and relationship fields. Contextual Logic: Optionally add a \"chain of thought\" field in reusable components for extra context. Modular Chain of Thought¶ This approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT. from pydantic import BaseModel, Field class Role(BaseModel): chain\\\\\\_of\\\\\\_thought: str = Field(..., description=\"Think step by step to determine the correct title\") title: str class UserDetail(BaseModel): age: int name: str role: Role Utilize Optional Attributes¶ Use Python's Optional type and set a default value to prevent undesired defaults like empty strings. from typing import Optional class UserDetail(BaseModel): age: int name: str role: Optional\\\\\\[str\\\\\\] = Field(default=None) Handling Errors Within Function Calls¶ You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow. class UserDetail(BaseModel): age: int name: str role: Optional\\\\\\[str\\\\\\] = Field(default=None) class MaybeUser(BaseModel): result: Optional\\\\\\[UserDetail\\\\\\] = Field(default=None) error: bool = Field(default=False) message: Optional\\\\\\[str\\\\\\] def \\\\\\_\\\\\\_bool\\\\\\_\\\\\\_(self): return self.result is not None With the MaybeUser class, you can either receive a UserDetail object in result or get an error message in message. Simplification with the Maybe Pattern¶ You can further simplify this using instructor to create the Maybe pattern dynamically from any BaseModel. import instructor MaybeUser = instructor.Maybe(UserDetail) This allows you to quickly create a Maybe type for any class, streamlining the process. Tips for Enumerations¶ To prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty. from enum import Enum, auto class Role(Enum): PRINCIPAL = auto() TEACHER = auto() STUDENT = auto() OTHER = auto() class UserDetail(BaseModel): age: int name: str role: Role = Field(description=\"Correctly assign one of the predefined roles to the user.\") If you're having a hard time with Enum and alternative is to use Literal class UserDetail(BaseModel): age: int name: str role: Literal\\\\\\[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"\\\\\\] If you'd like to improve performance more you can reiterate the requirements in the field descriptions or in the docstrings. Reiterate Long Instructions¶ For complex attributes, it helps to reiterate the instructions in the field's description. class Role(BaseModel): \"\"\" Extract the role based on the following rules ... \"\"\" instructions: str = Field(..., description=\"Restate the instructions and rules to correctly determine the title.\") title: str class UserDetail(BaseModel): age: int name: str role: Role Handle Arbitrary Properties¶ When you need to extract undefined attributes, use a list of key-value pairs. from typing import List class Property(BaseModel): key: str value: str class UserDetail(BaseModel): age: int name: str properties: List\\\\\\[Property\\\\\\] = Field(..., description=\"Extract any other properties that might be relevant.\") Limiting the Length of Lists¶ When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties. class Property(BaseModel): index: str = Field(..., description=\"Monotonically increasing ID\") key: str value: str class UserDetail(BaseModel): age: int name: str properties: List\\\\\\[Property\\\\\\] = Field(..., description=\"Numbered list of arbitrary extracted properties, should be less than 6\") Using Tuples for Simple Types For simple types, tuples can be a more compact alternative to custom classes, especially when the properties don't require additional descriptions. class UserDetail(BaseModel): age: int name: str properties: List\\\\\\[Tuple\\\\\\[int, str\\\\\\]\\\\\\] = Field(..., description=\"Numbered list of arbitrary extracted properties, should be less than 6\") Advanced Arbitrary Properties¶ For multiple users, aim to use consistent key names when extracting properties. class UserDetails(BaseModel): \"\"\" Extract information for multiple users. Use consistent key names for properties across users. \"\"\" users: List\\\\\\[UserDetail\\\\\\] This refined guide should offer a cleaner and more organized approach to structure engineering in Python. Defining Relationships Between Entities¶ In cases where relationships exist between entities, it's vital to define them explicitly in the model. The following example demonstrates how to define relationships between users by incorporating an id and a friends field: class UserDetail(BaseModel): id: int = Field(..., description=\"Unique identifier for each user.\") age: int name: str friends: List\\\\\\[int\\\\\\] = Field(..., description=\"Correct and complete list of friend IDs, representing relationships between users.\") class UserRelationships(BaseModel): users: List\\\\\\[UserDetail\\\\\\] = Field(..., description=\"Collection of users, correctly capturing the relationships among them.\") Reusing Components with Different Contexts¶ You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both work\\\\\\_time and leisure\\\\\\_time. class TimeRange(BaseModel): start\\\\\\_time: int = Field(..., description=\"The start time in hours.\") end\\\\\\_time: int = Field(..., description=\"The end time in hours.\") class UserDetail(BaseModel): id: int = Field(..., description=\"Unique identifier for each user.\") age: int name: str work\\\\\\_time: TimeRange = Field(..., description=\"Time range during which the user is working.\") leisure\\\\\\_time: TimeRange = Field(..., description=\"Time range reserved for leisure activities.\") Sometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations. class TimeRange(BaseModel): chain\\\\\\_of\\\\\\_thought: str = Field(..., description=\"Step by step reasoning to get the correct time range\") start\\\\\\_time: int = Field(..., description=\"The start time in hours.\") end\\\\\\_time: int = Field(..., description=\"The end time in hours.\") Was this page helpful? Back to top Previous Contributing Next Philosophy Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Welcome To Instructor - Instructor",
    "url": "https://jxnl.github.io/instructor/?q=",
    "html": "Skip to content Instructor Welcome To Instructor Initializing search instructor 0.4.5 2.8k 223 Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Introduction Why use Instructor? Help with Instructor Installation Contributing Table of contents Usage Why use Instructor? More Examples Contributing License Instructor¶ Structured extraction in Python, powered by OpenAI's function calling api, designed for simplicity, transparency, and control. Dive into the world of Python-based structured extraction, by OpenAI's function calling API and Pydantic, the most widely used data validation library for Python. Instructor stands out for its simplicity, transparency, and user-centric design. Whether you're a seasoned developer or just starting out, you'll find Instructor's approach intuitive and steerable. Usage¶ import instructor from openai import OpenAI from pydantic import BaseModel # This enables response\\\\\\_model keyword # from client.chat.completions.create client = instructor.patch(OpenAI()) class UserDetail(BaseModel): name: str age: int user = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] ) assert isinstance(user, UserDetail) assert user.name == \"Jason\" assert user.age == 25 Using async clients For async clients you must use apatch vs patch like so: import instructor from openai import AsyncOpenAI from pydantic import BaseModel aclient = instructor.apatch(AsyncOpenAI()) class UserExtract(BaseModel): name: str age: int model = await aclient.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserExtract, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"}, \\\\\\], ) assert isinstance(model, UserExtract) Accessing the original response If you want to access anything like usage or other metadata, the original response is available on the Model.\\\\\\_raw\\\\\\_response attribute. user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] ) from openai.types.chat.chat\\\\\\_completion import ChatCompletion assert isinstance(user.\\\\\\_raw\\\\\\_response, ChatCompletion) Why use Instructor?¶ The question of using Instructor is fundamentally a question of why to use Pydantic. Powered by type hints — Instructor is powered by Pydantic, which is powered by type hints. Schema validation, prompting is controleld by type annotations; less to learn, less code ot write, and integrates with your IDE. Powered by OpenAI — Instructor is powered by OpenAI's function calling API. This means you can use the same API for both prompting and extraction. Customizable — Pydantic is highly customizable. You can define your own validators, custom error messages, and more. Ecosystem Pydantic is the most widely used data validation library for Python. It's used by FastAPI, Typer, and many other popular libraries. Battle Tested — Pydantic is downloaded over 100M times per month, and supported by a large community of contributors. Easy Integration with CLI - We offer a variety of CLI tools like instructor jobs, instructor files and instructor usage to track your OpenAI usage, fine-tuning jobs and more, just check out our CLI Documentation to find out more. More Examples¶ If you'd like to see more check out our cookbook. Installing Instructor is a breeze. Just run pip install instructor. Contributing¶ If you want to help out checkout some of the issues marked as good-first-issue or help-wanted. Found here. They could be anything from code improvements, a guest blog post, or a new cook book. License¶ This project is licensed under the terms of the MIT License. Was this page helpful? Back to top Next Why use Instructor? Copyright © 2023 Jason Liu Made with Material for MkDocs"
  },
  {
    "title": "Welcome To Instructor - Instructor",
    "url": "https://jxnl.github.io/instructor/",
    "html": "Skip to content Instructor Welcome To Instructor Type to start searching instructor Introduction Tips Concepts Cookbook CLI Reference API Reference Blog Introduction Why use Instructor? Help with Instructor Installation Contributing Table of contents Usage Why use Instructor? More Examples Contributing License Instructor¶ Structured extraction in Python, powered by OpenAI's function calling api, designed for simplicity, transparency, and control. Dive into the world of Python-based structured extraction, by OpenAI's function calling API and Pydantic, the most widely used data validation library for Python. Instructor stands out for its simplicity, transparency, and user-centric design. Whether you're a seasoned developer or just starting out, you'll find Instructor's approach intuitive and steerable. Usage¶ import instructor from openai import OpenAI from pydantic import BaseModel # This enables response\\\\\\_model keyword # from client.chat.completions.create client = instructor.patch(OpenAI()) class UserDetail(BaseModel): name: str age: int user = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] ) assert isinstance(user, UserDetail) assert user.name == \"Jason\" assert user.age == 25 Using async clients For async clients you must use apatch vs patch like so: import instructor from openai import AsyncOpenAI from pydantic import BaseModel aclient = instructor.apatch(AsyncOpenAI()) class UserExtract(BaseModel): name: str age: int model = await aclient.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserExtract, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"}, \\\\\\], ) assert isinstance(model, UserExtract) Accessing the original response If you want to access anything like usage or other metadata, the original response is available on the Model.\\\\\\_raw\\\\\\_response attribute. user: UserDetail = client.chat.completions.create( model=\"gpt-3.5-turbo\", response\\\\\\_model=UserDetail, messages=\\\\\\[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, \\\\\\] ) from openai.types.chat.chat\\\\\\_completion import ChatCompletion assert isinstance(user.\\\\\\_raw\\\\\\_response, ChatCompletion) Why use Instructor?¶ The question of using Instructor is fundamentally a question of why to use Pydantic. Powered by type hints — Instructor is powered by Pydantic, which is powered by type hints. Schema validation, prompting is controleld by type annotations; less to learn, less code ot write, and integrates with your IDE. Powered by OpenAI — Instructor is powered by OpenAI's function calling API. This means you can use the same API for both prompting and extraction. Customizable — Pydantic is highly customizable. You can define your own validators, custom error messages, and more. Ecosystem Pydantic is the most widely used data validation library for Python. It's used by FastAPI, Typer, and many other popular libraries. Battle Tested — Pydantic is downloaded over 100M times per month, and supported by a large community of contributors. Easy Integration with CLI - We offer a variety of CLI tools like instructor jobs, instructor files and instructor usage to track your OpenAI usage, fine-tuning jobs and more, just check out our CLI Documentation to find out more. More Examples¶ If you'd like to see more check out our cookbook. Installing Instructor is a breeze. Just run pip install instructor. Contributing¶ If you want to help out checkout some of the issues marked as good-first-issue or help-wanted. Found here. They could be anything from code improvements, a guest blog post, or a new cook book. License¶ This project is licensed under the terms of the MIT License. Was this page helpful? Back to top Next Why use Instructor? Copyright © 2023 Jason Liu Made with Material for MkDocs"
  }
]